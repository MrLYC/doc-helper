import re
import argparse
from pathlib import Path
from collections import deque
import tempfile
import shutil
import time
from urllib.parse import urlparse, urljoin
import logging
import urllib.parse
from dataclasses import dataclass
from typing import Tuple, Dict, Any, Optional, List
import json
import hashlib
import signal
import sys
import os

from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from PyPDF2 import PdfMerger, PdfReader

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TimeoutConfig:
    """è¶…æ—¶é…ç½®ç®¡ç†"""
    base_timeout: int  # åŸºç¡€è¶…æ—¶æ—¶é—´ï¼ˆæ¥è‡ªå‘½ä»¤è¡Œå‚æ•°ï¼‰
    
    @property
    def initial_load_timeout(self) -> int:
        """åˆå§‹é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰"""
        return max(self.base_timeout, 30) * 1000
    
    @property
    def fast_mode_timeout(self) -> int:
        """å¿«é€Ÿæ¨¡å¼è¶…æ—¶ï¼ˆç§’ï¼‰"""
        return max(self.base_timeout, 30)
    
    @property
    def content_additional_wait(self) -> int:
        """å†…å®¹åŠ è½½é¢å¤–ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        return max(5, self.base_timeout // 4)
    
    @property
    def thorough_min_timeout(self) -> int:
        """å½»åº•æ¨¡å¼æœ€å°ä¿ç•™è¶…æ—¶ï¼ˆç§’ï¼‰"""
        return max(10, self.base_timeout // 2)
    
    @property
    def retry_backoff_max(self) -> int:
        """é‡è¯•é€€é¿æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        return max(10, self.base_timeout // 6)
    
    @property
    def element_check_interval(self) -> float:
        """å…ƒç´ æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰"""
        return 5.0
    
    @property
    def fast_check_interval(self) -> float:
        """å¿«é€Ÿæ¨¡å¼æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰"""
        return 0.5
    
    @property
    def page_render_wait(self) -> float:
        """é¡µé¢æ¸²æŸ“ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        return max(2.0, self.base_timeout * 0.02)
    
    @property
    def min_pdf_size(self) -> int:
        """æœ€å°PDFæ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
        return 5000

@dataclass
class ProgressState:
    """è¿›åº¦çŠ¶æ€ç®¡ç†"""
    base_url: str
    output_pdf: str
    temp_dir: str
    progress_file: str
    visited_urls: set
    failed_urls: list
    processed_urls: list
    pdf_files: list
    queue: deque
    enqueued: set
    
    def save_to_file(self):
        """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
        state_data = {
            'base_url': self.base_url,
            'output_pdf': self.output_pdf,
            'temp_dir': self.temp_dir,
            'visited_urls': list(self.visited_urls),
            'failed_urls': self.failed_urls,
            'processed_urls': self.processed_urls,
            'pdf_files': [str(f) for f in self.pdf_files],
            'queue': list(self.queue),
            'enqueued': list(self.enqueued)
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(state_data, f, ensure_ascii=False, indent=2)
        logger.debug(f"è¿›åº¦å·²ä¿å­˜åˆ°: {self.progress_file}")
    
    @classmethod
    def load_from_file(cls, progress_file: str):
        """ä»æ–‡ä»¶åŠ è½½è¿›åº¦"""
        if not os.path.exists(progress_file):
            return None
        
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                state_data = json.load(f)
            
            # éªŒè¯ä¸´æ—¶PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            valid_pdf_files = []
            for pdf_file_str in state_data.get('pdf_files', []):
                pdf_path = Path(pdf_file_str)
                if pdf_path.exists():
                    valid_pdf_files.append(pdf_path)
                else:
                    logger.warning(f"ä¸´æ—¶PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²ä»è¿›åº¦ä¸­ç§»é™¤: {pdf_file_str}")
            
            progress = cls(
                base_url=state_data.get('base_url', ''),
                output_pdf=state_data.get('output_pdf', ''),
                temp_dir=state_data.get('temp_dir', ''),
                progress_file=progress_file,
                visited_urls=set(state_data.get('visited_urls', [])),
                failed_urls=state_data.get('failed_urls', []),
                processed_urls=state_data.get('processed_urls', []),
                pdf_files=valid_pdf_files,
                queue=deque(state_data.get('queue', [])),
                enqueued=set(state_data.get('enqueued', []))
            )
            
            logger.info(f"ä»è¿›åº¦æ–‡ä»¶æ¢å¤çŠ¶æ€: å·²å¤„ç† {len(progress.processed_urls)} ä¸ªURLï¼Œ"
                       f"é˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(progress.queue)} ä¸ªURL")
            
            return progress
            
        except Exception as e:
            logger.error(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return None

def url_to_filename(url: str) -> str:
    """å°†URLè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    # ä½¿ç”¨URLçš„å“ˆå¸Œå€¼ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ï¼Œç¡®ä¿å”¯ä¸€æ€§
    url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()[:8]
    
    # æ¸…ç†URLç”¨ä½œæ–‡ä»¶å
    safe_name = re.sub(r'[^\w\-_\.]', '_', url.replace('https://', '').replace('http://', ''))
    safe_name = safe_name[:50]  # é™åˆ¶é•¿åº¦
    
    return f"{safe_name}_{url_hash}.pdf"

def setup_signal_handlers(progress_state: ProgressState):
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        progress_state.save_to_file()
        logger.info("è¿›åº¦å·²ä¿å­˜ï¼Œç¨‹åºé€€å‡º")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·

def create_progress_file_path(output_pdf: str, base_url: str) -> str:
    """åˆ›å»ºè¿›åº¦æ–‡ä»¶è·¯å¾„"""
    output_path = Path(output_pdf)
    base_name = output_path.stem
    
    # ä½¿ç”¨base_urlçš„å“ˆå¸Œå€¼ç¡®ä¿å”¯ä¸€æ€§
    url_hash = hashlib.md5(base_url.encode('utf-8')).hexdigest()[:8]
    
    progress_file = output_path.parent / f".{base_name}_{url_hash}.progress"
    return str(progress_file)

def cleanup_temp_files(temp_dir: str, progress_file: str = None):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    cleaned_count = 0
    
    # æ¸…ç†ä¸´æ—¶PDFæ–‡ä»¶
    if temp_dir and os.path.exists(temp_dir):
        temp_path = Path(temp_dir)
        for pdf_file in temp_path.glob("*.pdf"):
            try:
                pdf_file.unlink()
                cleaned_count += 1
                logger.debug(f"åˆ é™¤ä¸´æ—¶PDF: {pdf_file}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶PDFå¤±è´¥ {pdf_file}: {e}")
        
        # å°è¯•åˆ é™¤ä¸´æ—¶ç›®å½•
        try:
            temp_path.rmdir()
            logger.debug(f"åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_path}")
        except Exception as e:
            logger.debug(f"ä¸´æ—¶ç›®å½•éç©ºæˆ–åˆ é™¤å¤±è´¥ {temp_path}: {e}")
    
    # æ¸…ç†è¿›åº¦æ–‡ä»¶
    if progress_file and os.path.exists(progress_file):
        try:
            os.unlink(progress_file)
            logger.debug(f"åˆ é™¤è¿›åº¦æ–‡ä»¶: {progress_file}")
        except Exception as e:
            logger.warning(f"åˆ é™¤è¿›åº¦æ–‡ä»¶å¤±è´¥ {progress_file}: {e}")
    
    if cleaned_count > 0:
        logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªä¸´æ—¶PDFæ–‡ä»¶")
    
    return cleaned_count

def normalize_url(url, base_url):
    """æ ‡å‡†åŒ–URLå¹¶ç§»é™¤URLç‰‡æ®µ"""
    parsed = urlparse(url)
    base_parsed = urlparse(base_url)
    
    # å¤„ç†ç›¸å¯¹URL
    if not parsed.scheme:
        url = urllib.parse.urljoin(base_url, url)
        parsed = urlparse(url)
    
    # åˆ›å»ºæ–°çš„URLå¯¹è±¡
    normalized = parsed._replace(
        scheme=base_parsed.scheme or 'https',
        path=urllib.parse.unquote(parsed.path) if parsed.path else '',
        fragment='',
        query=parsed.query
    )
    
    # ç”Ÿæˆè§„èŒƒåŒ–URLå­—ç¬¦ä¸²
    normalized_url = normalized.geturl()
    
    # å¤„ç†é‡å¤æ–œæ 
    normalized_url = re.sub(r'([^:])//+', r'\1/', normalized_url)
    
    # ç»Ÿä¸€åè®®å¤„ç†
    if normalized_url.startswith("http://"):
        normalized_url = "https://" + normalized_url[7:]
    
    return normalized_url

def resolve_selector(selector):
    """æ™ºèƒ½è§£æé€‰æ‹©å™¨"""
    if selector.startswith('/'):
        if not selector.startswith('//'):
            return f'selector=/{selector[1:]}'
        return f'selector={selector}'
    return selector

def check_element_visibility_and_content(page, selector: str) -> Tuple[bool, str, int, Dict[str, Any]]:
    """æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ã€å¯è§ä¸”æœ‰è¶³å¤Ÿå†…å®¹"""
    element = page.query_selector(resolve_selector(selector))
    if not element:
        return False, "å…ƒç´ ä¸å­˜åœ¨", 0, {}
    
    element_info = page.evaluate('''(el) => {
        const rect = el.getBoundingClientRect();
        const style = window.getComputedStyle(el);
        return {
            textLength: el.textContent ? el.textContent.trim().length : 0,
            isVisible: rect.width > 0 && rect.height > 0,
            width: rect.width,
            height: rect.height,
            display: style.display,
            visibility: style.visibility,
            opacity: parseFloat(style.opacity) || 1
        };
    }''', element)
    
    # æ£€æŸ¥å¯è§æ€§
    is_visible = (element_info['isVisible'] and 
                element_info['display'] != 'none' and 
                element_info['visibility'] != 'hidden' and 
                element_info['opacity'] > 0.1)
    
    if not is_visible:
        reason = f"å…ƒç´ ä¸å¯è§ (display:{element_info['display']}, visibility:{element_info['visibility']}, opacity:{element_info['opacity']}, size:{element_info['width']}x{element_info['height']})"
        return False, reason, element_info['textLength'], element_info
    
    return True, "å…ƒç´ å¯è§", element_info['textLength'], element_info

def wait_for_element_visible(page, selector: str, timeout_config: TimeoutConfig, 
                           strategy: str = "normal") -> bool:
    """ç­‰å¾…å…ƒç´ å¯è§çš„é€šç”¨å‡½æ•°"""
    if strategy == "fast":
        timeout = timeout_config.fast_mode_timeout
        check_interval = timeout_config.fast_check_interval
        logger.info(f"å¿«é€Ÿç­‰å¾…å…ƒç´ å¯è§ï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´ {timeout} ç§’")
    elif strategy == "thorough":
        # thoroughæ¨¡å¼ç”±è°ƒç”¨æ–¹è®¡ç®—å‰©ä½™æ—¶é—´
        timeout = timeout_config.base_timeout  
        check_interval = timeout_config.element_check_interval
        logger.info(f"å½»åº•æ¨¡å¼ï¼šæŒç»­ç­‰å¾…å…ƒç´ å¯è§ï¼Œå‰©ä½™ç­‰å¾…æ—¶é—´ {timeout:.1f} ç§’")
    else:  # normal
        timeout = timeout_config.base_timeout
        check_interval = timeout_config.element_check_interval
        logger.info(f"æ™ºèƒ½ç­‰å¾…æ¨¡å¼ï¼šæŒç»­ç­‰å¾…å…ƒç´ å¯è§ï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´ {timeout} ç§’")
    
    wait_start_time = time.time()
    consecutive_failures = 0  # è¿ç»­å¤±è´¥æ¬¡æ•°
    max_consecutive_failures = 3  # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°ï¼Œè¶…è¿‡åå¿«é€Ÿå¤±è´¥
    
    while time.time() - wait_start_time < timeout:
        is_ready, status_msg, text_length, element_info = check_element_visibility_and_content(page, selector)
        
        if is_ready:
            logger.info(f"å†…å®¹å…ƒç´ å·²æ‰¾åˆ°ä¸”å¯è§: {status_msg}")
            consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
            
            if strategy == "normal":
                # Normalæ¨¡å¼æœ‰æ›´å¤æ‚çš„å†…å®¹æ£€æŸ¥é€»è¾‘
                if text_length > 100:  # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿå†…å®¹ï¼Œç›´æ¥æˆåŠŸ
                    logger.info(f"å†…å®¹å……è¶³ ({text_length} å­—ç¬¦)ï¼Œå®Œæˆç­‰å¾…")
                    return True
                elif text_length > 0:
                    # æœ‰å°‘é‡å†…å®¹ï¼Œå†ç­‰å¾…ä¸€æ®µæ—¶é—´çœ‹æ˜¯å¦æœ‰æ›´å¤šå†…å®¹åŠ è½½
                    remaining_time = timeout - (time.time() - wait_start_time)
                    additional_wait = min(remaining_time, timeout_config.content_additional_wait)
                    
                    if additional_wait > 0:
                        logger.info(f"å†…å®¹è¾ƒå°‘ ({text_length} å­—ç¬¦)ï¼Œå†ç­‰å¾… {additional_wait:.1f} ç§’çœ‹æ˜¯å¦æœ‰æ›´å¤šå†…å®¹...")
                        time.sleep(additional_wait)
                        
                        # å†æ¬¡æ£€æŸ¥
                        is_ready_again, _, text_length_again, _ = check_element_visibility_and_content(page, selector)
                        if is_ready_again and text_length_again >= text_length:
                            logger.info(f"å†…å®¹å·²æ›´æ–°åˆ° {text_length_again} å­—ç¬¦ï¼Œæ¥å—å½“å‰çŠ¶æ€")
                        else:
                            logger.info(f"å†…å®¹æ— æ˜æ˜¾å¢åŠ ï¼Œæ¥å—å½“å‰çŠ¶æ€ ({text_length} å­—ç¬¦)")
                    
                    return True
                else:
                    logger.info("å…ƒç´ å¯è§ä½†æ— æ–‡æœ¬å†…å®¹ï¼Œç»§ç»­ç­‰å¾…...")
            else:
                # Fastå’ŒThoroughæ¨¡å¼åªè¦å…ƒç´ å¯è§å°±æˆåŠŸ
                return True
        else:
            consecutive_failures += 1
            elapsed = time.time() - wait_start_time
            remaining = timeout - elapsed
            
            # å¦‚æœæ˜¯"å…ƒç´ ä¸å­˜åœ¨"ä¸”è¿ç»­å¤±è´¥å¤šæ¬¡ï¼Œå¯èƒ½æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œå¿«é€Ÿå¤±è´¥
            if "å…ƒç´ ä¸å­˜åœ¨" in status_msg and consecutive_failures >= max_consecutive_failures:
                logger.warning(f"å…ƒç´ è¿ç»­ {consecutive_failures} æ¬¡ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯å¤–éƒ¨é“¾æ¥æˆ–æ— æ•ˆé¡µé¢ï¼Œå¿«é€Ÿå¤±è´¥")
                return False
            
            logger.info(f"å…ƒç´ çŠ¶æ€: {status_msg}, å·²ç­‰å¾… {elapsed:.1f}s, å‰©ä½™ {remaining:.1f}s, è¿ç»­å¤±è´¥: {consecutive_failures}")
        
        time.sleep(check_interval)
    
    elapsed = time.time() - wait_start_time
    logger.warning(f"{strategy}æ¨¡å¼ç­‰å¾…è¶…æ—¶ ({elapsed:.1f}s)ï¼Œå…ƒç´ ä»ä¸å¯è§")
    return False

def _setup_request_blocking(page, patterns):
    """è®¾ç½®è¯·æ±‚æ‹¦æˆªå™¨ï¼Œé˜»æ­¢é»‘åå•URL"""
    if not patterns:
        return
        
    def handle_route(route):
        request_url = route.request.url
        for pattern in patterns:
            if pattern.match(request_url):
                logger.debug(f"é˜»æ­¢é»‘åå•URL: {request_url}")
                route.abort()
                return
        route.continue_()
    
    page.route("**/*", handle_route)

def _setup_slow_request_monitoring(page):
    """è®¾ç½®æ…¢è¯·æ±‚ç›‘æ§ï¼ˆä»…ç”¨äºthoroughæ¨¡å¼ï¼‰"""
    slow_requests = {}
    
    def on_request(request):
        slow_requests[request.url] = time.time()
    
    def on_response(response):
        request_url = response.url
        if request_url in slow_requests:
            duration = time.time() - slow_requests[request_url]
            if duration > 3.0:  # è¶…è¿‡3ç§’çš„è¯·æ±‚
                logger.warning(f"åŠ è½½ç¼“æ…¢çš„èµ„æº ({duration:.1f}s): {request_url}")
            del slow_requests[request_url]
    
    def on_request_failed(request):
        if request.url in slow_requests:
            del slow_requests[request.url]
    
    page.on("request", on_request)
    page.on("response", on_response)
    page.on("requestfailed", on_request_failed)
    
    return slow_requests

def _handle_page_loading_with_retries(page, url, content_selector, timeout_config, max_retries, 
                                    verbose_mode, load_strategy, url_blacklist_patterns=None):
    """å¤„ç†é¡µé¢åŠ è½½å’Œé‡è¯•é€»è¾‘"""
    
    def _apply_load_strategy(page, content_selector, timeout_config, load_strategy, slow_requests):
        """åº”ç”¨ç‰¹å®šçš„åŠ è½½ç­–ç•¥"""
        if load_strategy == "fast":
            logger.info("å¿«é€ŸåŠ è½½æ¨¡å¼ï¼šè·³è¿‡ç½‘ç»œç©ºé—²ç­‰å¾…ï¼Œä½†æŒç»­ç­‰å¾…å…ƒç´ å¯è§")
            return wait_for_element_visible(page, content_selector, timeout_config, "fast")
        
        elif load_strategy == "thorough":
            logger.info("å½»åº•åŠ è½½æ¨¡å¼ï¼šç­‰å¾…å®Œå…¨çš„ç½‘ç»œç©ºé—²ï¼Œç„¶åæŒç»­ç­‰å¾…å…ƒç´ å¯è§")
            
            # é¦–å…ˆç­‰å¾…ç½‘ç»œç©ºé—²
            try:
                page.wait_for_load_state("networkidle", timeout=timeout_config.base_timeout*1000)
                logger.info("ç½‘ç»œå·²è¾¾åˆ°ç©ºé—²çŠ¶æ€")
            except PlaywrightTimeoutError:
                logger.warning("ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…å…ƒç´ å¯è§")
                # åœ¨thoroughæ¨¡å¼ä¸‹ï¼Œæ‰“å°è¿˜åœ¨åŠ è½½çš„æ…¢è¯·æ±‚
                if slow_requests:
                    logger.warning(f"ä»æœ‰ {len(slow_requests)} ä¸ªè¯·æ±‚æœªå®Œæˆ:")
                    for req_url in list(slow_requests.keys())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        duration = time.time() - slow_requests[req_url]
                        logger.warning(f"  - {duration:.1f}s: {req_url}")
            
            # ç„¶åç­‰å¾…å…ƒç´ å¯è§ï¼ˆä½¿ç”¨å‰©ä½™æ—¶é—´ï¼‰
            remaining_timeout = max(timeout_config.base_timeout // 2, timeout_config.thorough_min_timeout)
            timeout_config_remaining = TimeoutConfig(remaining_timeout)
            return wait_for_element_visible(page, content_selector, timeout_config_remaining, "thorough")
        
        else:  # normal strategy (æ™ºèƒ½ç­‰å¾…)
            return wait_for_element_visible(page, content_selector, timeout_config, "normal")
    
    # è®¾ç½®è¯·æ±‚æ‹¦æˆª
    _setup_request_blocking(page, url_blacklist_patterns)
    
    # è®¾ç½®æ…¢è¯·æ±‚ç›‘æ§ï¼ˆä»…åœ¨thoroughæ¨¡å¼ä¸‹ï¼‰
    slow_requests = None
    if load_strategy == "thorough":
        slow_requests = _setup_slow_request_monitoring(page)
    
    for attempt in range(max_retries):
        try:
            logger.info(f"å°è¯•åŠ è½½é¡µé¢ ({attempt+1}/{max_retries}): {url}")
            
            if verbose_mode:
                logger.info("å¯è§†åŒ–æ¨¡å¼ï¼šç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½...")
            
            # å…ˆå°è¯•å¿«é€ŸåŠ è½½åˆ° domcontentloaded çŠ¶æ€
            page.goto(url, wait_until="domcontentloaded", timeout=timeout_config.initial_load_timeout)
            logger.info("é¡µé¢DOMå·²åŠ è½½å®Œæˆ")
            
            if verbose_mode:
                # åœ¨é¡µé¢æ ‡é¢˜ä¸­æ˜¾ç¤ºå¤„ç†çŠ¶æ€
                try:
                    page.evaluate('''() => {
                        document.title = "[æ£€æŸ¥å†…å®¹...] " + (document.title || "é¡µé¢");
                    }''')
                except:
                    pass
            
            # åº”ç”¨åŠ è½½ç­–ç•¥
            if _apply_load_strategy(page, content_selector, timeout_config, load_strategy, slow_requests):
                return page.url  # è¿”å›æœ€ç»ˆURL
            elif attempt < max_retries - 1:
                time.sleep(timeout_config.element_check_interval)
            
        except PlaywrightTimeoutError as timeout_err:
            if "Timeout" in str(timeout_err) and "goto" in str(timeout_err):
                logger.warning(f"ç¬¬ {attempt+1} æ¬¡é¡µé¢åŠ è½½è¶…æ—¶: {timeout_err}")
            else:
                logger.warning(f"ç¬¬ {attempt+1} æ¬¡æ“ä½œè¶…æ—¶: {timeout_err}")
                
            if attempt == max_retries - 1:
                logger.error("æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µé¢")
                raise
                
            # æŒ‡æ•°é€€é¿é‡è¯•
            wait_time = min(2 ** attempt, timeout_config.retry_backoff_max)
            logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
        
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {str(e)}")
                raise
            logger.warning(f"ç¬¬ {attempt+1} æ¬¡é¡µé¢åŠ è½½å¼‚å¸¸: {str(e)}ï¼Œé‡è¯•ä¸­...")
            wait_time = min(2 ** attempt, timeout_config.retry_backoff_max)
            time.sleep(wait_time)
    else:
        logger.error("æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µé¢")
        raise Exception("æ‰€æœ‰é‡è¯•å‡å¤±è´¥")

def _extract_page_links(page, toc_selector, final_url, base_url):
    """æå–é¡µé¢ä¸­çš„å¯¼èˆªé“¾æ¥"""
    links = []
    try:
        logger.info(f"å¼€å§‹æå–å¯¼èˆªé“¾æ¥: {toc_selector}")
        resolved_toc = resolve_selector(toc_selector)
        
        toc_element = page.query_selector(resolved_toc)
        if not toc_element:
            logger.warning(f"å¯¼èˆªå…ƒç´ ä¸å­˜åœ¨: {resolved_toc}")
            return links
        
        a_elements = toc_element.query_selector_all("a")
        logger.info(f"æ‰¾åˆ° {len(a_elements)} ä¸ªé“¾æ¥å…ƒç´ ")
        
        for a in a_elements:
            href = a.get_attribute("href")
            if href and href.strip():
                abs_url = urljoin(final_url, href.strip())
                norm_url = normalize_url(abs_url, base_url)
                links.append(norm_url)
        
        links = list(set(links))
        logger.info(f"æå–åˆ° {len(links)} ä¸ªå”¯ä¸€é“¾æ¥")
        
    except Exception as e:
        logger.error(f"æå–å¯¼èˆªé“¾æ¥å¤±è´¥: {e}", exc_info=True)
    
    return links

def _clean_page_content(page, content_element, verbose_mode, timeout_config):
    """æ¸…ç†é¡µé¢å†…å®¹ï¼Œä¿ç•™ä¸»è¦å†…å®¹"""
    logger.info("æ¸…ç†é¡µé¢å¹¶ä¿ç•™ä¸»è¦å†…å®¹...")
    
    # ä¿å­˜åŸå§‹å†…å®¹ç”¨äºå¯¹æ¯”
    original_content = page.evaluate('''(element) => {
        return {
            textLength: element.textContent ? element.textContent.trim().length : 0,
            innerHTML: element.innerHTML.substring(0, 200) + '...'
        };
    }''', content_element)
    logger.info(f"æ¸…ç†å‰å†…å®¹é¢„è§ˆ: æ–‡æœ¬é•¿åº¦={original_content['textLength']}, HTMLç‰‡æ®µ={original_content['innerHTML']}")
    
    if verbose_mode:
        page.evaluate('''() => {
            document.title = "[æ¸…ç†é¡µé¢...] " + document.title.replace(/^\[.*?\] /, "");
        }''')
        # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹ï¼Œç¨å¾®å»¶è¿Ÿä¸€ä¸‹è®©ç”¨æˆ·çœ‹åˆ°åŸå§‹é¡µé¢
        time.sleep(timeout_config.element_check_interval)
    
    # æ–°çš„æ¸…ç†é€»è¾‘ï¼šé€çº§å‘ä¸Šæ¸…ç†DOM
    page.evaluate('''(element) => {
        // ä»å†…å®¹å…ƒç´ å¼€å§‹å‘ä¸Šæ¸…ç†
        let current = element;
        
        // å‘ä¸Šéå†ç›´åˆ°bodyå…ƒç´ 
        while (current && current !== document.body) {
            const parent = current.parentElement;
            if (!parent) break;
            
            // åˆ é™¤æ‰€æœ‰éå½“å‰å…ƒç´ çš„å…„å¼ŸèŠ‚ç‚¹
            for (let i = parent.children.length - 1; i >= 0; i--) {
                const child = parent.children[i];
                if (child !== current) {
                    child.remove();
                }
            }
            
            // ç§»åŠ¨åˆ°çˆ¶çº§å…ƒç´ 
            current = parent;
        }
        
        // æ¸…ç†bodyå…ƒç´ 
        if (current === document.body) {
            // ç§»é™¤æ‰€æœ‰è„šæœ¬
            document.querySelectorAll('script').forEach(s => s.remove());
            
            // è®¾ç½®bodyæ ·å¼
            document.body.style.margin = '0';
            document.body.style.padding = '0';
            
            // ç¡®ä¿å†…å®¹å…ƒç´ å®½åº¦100%
            element.style.width = '100%';
            element.style.boxSizing = 'border-box';
            element.style.padding = '20px';
        }
    }''', content_element)
    
    # æ£€æŸ¥æ¸…ç†åçš„å†…å®¹
    after_cleanup = page.evaluate('''(element) => {
        const rect = element.getBoundingClientRect();
        return {
            textLength: element.textContent ? element.textContent.trim().length : 0,
            hasVisibleContent: rect.width > 0 && rect.height > 0,
            width: rect.width,
            height: rect.height,
            innerHTML: element.innerHTML.substring(0, 200) + '...'
        };
    }''', content_element)
    logger.info(f"æ¸…ç†åå†…å®¹æ£€æŸ¥: æ–‡æœ¬é•¿åº¦={after_cleanup['textLength']}, å¯è§={after_cleanup['hasVisibleContent']}, å°ºå¯¸={after_cleanup['width']}x{after_cleanup['height']}")
    
    # å¦‚æœæ¸…ç†åå†…å®¹æ˜æ˜¾å‡å°‘ï¼Œå‘å‡ºè­¦å‘Š
    if after_cleanup['textLength'] < original_content['textLength'] * 0.8:
        logger.warning(f"è­¦å‘Šï¼šæ¸…ç†åå†…å®¹å¤§å¹…å‡å°‘ï¼åŸå§‹: {original_content['textLength']} -> æ¸…ç†å: {after_cleanup['textLength']}")

def _save_debug_screenshot(page, url, debug_dir):
    """ä¿å­˜è°ƒè¯•æˆªå›¾"""
    debug_path = Path(debug_dir)
    debug_path.mkdir(exist_ok=True)
    
    # æ¸…ç†URLä½œä¸ºæ–‡ä»¶å
    safe_url = re.sub(r'[^\w\-_\.]', '_', url.replace('https://', '').replace('http://', ''))[:50]
    screenshot_path = debug_path / f"{safe_url}_after_cleanup.png"
    
    try:
        page.screenshot(path=str(screenshot_path), full_page=True)
        logger.info(f"è°ƒè¯•æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
    except Exception as screenshot_err:
        logger.warning(f"ä¿å­˜æˆªå›¾å¤±è´¥: {screenshot_err}")

def _prepare_page_for_pdf(page, content_selector, verbose_mode, timeout_config, debug_mode, debug_dir, url):
    """å‡†å¤‡é¡µé¢å†…å®¹ç”¨äºPDFç”Ÿæˆ"""
    content_element = page.query_selector(resolve_selector(content_selector))
    if not content_element:
        logger.error(f"é¡µé¢ä¸­æœªæ‰¾åˆ°å†…å®¹èŠ‚ç‚¹: {content_selector}")
        return False
    
    if verbose_mode:
        page.evaluate('''() => {
            document.title = "[åˆ†æå†…å®¹...] " + document.title.replace(/^\[.*?\] /, "");
        }''')
    
    logger.info("åˆ†æå†…å®¹å…ƒç´ ...")
    
    # ä½¿ç”¨ç»Ÿä¸€çš„å…ƒç´ æ£€æŸ¥å‡½æ•°
    is_ready, status_msg, text_length, element_info = check_element_visibility_and_content(page, content_selector)
    
    if not is_ready:
        logger.error(f"å†…å®¹å…ƒç´ ä¸å¯è§ï¼Œè·³è¿‡PDFç”Ÿæˆï¼{status_msg}")
        return False
    
    logger.info(f"å†…å®¹å…ƒç´ ä¿¡æ¯: {element_info}")
    
    # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­ï¼ˆå¯èƒ½æ˜¯åŠ¨æ€åŠ è½½ï¼‰
    if text_length == 0:
        logger.warning(f"è­¦å‘Šï¼šå†…å®¹å…ƒç´ æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼å¯èƒ½æ˜¯åŠ¨æ€åŠ è½½æˆ–ç©ºé¡µé¢")
    elif text_length < 50:
        logger.warning(f"è­¦å‘Šï¼šå†…å®¹å…ƒç´ æ–‡æœ¬å¾ˆå°‘ ({text_length} å­—ç¬¦)ï¼")
    
    # æ¸…ç†é¡µé¢å†…å®¹
    _clean_page_content(page, content_element, verbose_mode, timeout_config)
    
    # è°ƒè¯•æ¨¡å¼ï¼šä¿å­˜æˆªå›¾
    if debug_mode and debug_dir:
        _save_debug_screenshot(page, url, debug_dir)
    
    return True

def _generate_pdf_from_page(page, verbose_mode, timeout_config, temp_dir: str, url: str):
    """ä»é¡µé¢ç”ŸæˆPDF"""
    logger.info("ç­‰å¾…é¡µé¢æ¸²æŸ“...")
    if verbose_mode:
        page.evaluate('''() => {
            document.title = "[å‡†å¤‡ç”ŸæˆPDF...] " + document.title.replace(/^\[.*?\] /, "");
        }''')
        time.sleep(timeout_config.element_check_interval)  # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹ç»™ç”¨æˆ·æ›´å¤šæ—¶é—´è§‚å¯Ÿ
    
    time.sleep(timeout_config.page_render_wait)  # ä½¿ç”¨é…ç½®çš„é¡µé¢æ¸²æŸ“ç­‰å¾…æ—¶é—´
    
    # åœ¨ç”ŸæˆPDFå‰åšæœ€åçš„å†…å®¹æ£€æŸ¥
    final_check = page.evaluate('''() => {
        const body = document.body;
        const rect = body.getBoundingClientRect();
        return {
            bodyTextLength: body.textContent ? body.textContent.trim().length : 0,
            bodyHeight: rect.height,
            visibleElements: Array.from(document.querySelectorAll('*')).filter(el => {
                const style = window.getComputedStyle(el);
                const rect = el.getBoundingClientRect();
                return style.display !== 'none' && 
                       style.visibility !== 'hidden' && 
                       rect.width > 0 && 
                       rect.height > 0;
            }).length,
            hasImages: document.querySelectorAll('img').length,
            hasTables: document.querySelectorAll('table').length
        };
    }''')
    logger.info(f"PDFç”Ÿæˆå‰æœ€ç»ˆæ£€æŸ¥: {final_check}")
    
    if final_check['bodyTextLength'] == 0:
        logger.error("ä¸¥é‡è­¦å‘Šï¼šé¡µé¢å†…å®¹ä¸ºç©ºï¼Œå°†ç”Ÿæˆç©ºç™½PDFï¼")
    elif final_check['bodyTextLength'] < 50:
        logger.warning(f"è­¦å‘Šï¼šé¡µé¢å†…å®¹å¾ˆå°‘ ({final_check['bodyTextLength']} å­—ç¬¦)ï¼Œå¯èƒ½ç”Ÿæˆè¿‘ä¼¼ç©ºç™½çš„PDF")
    
    # ä½¿ç”¨æŒä¹…åŒ–çš„æ–‡ä»¶å
    filename = url_to_filename(url)
    temp_file = Path(temp_dir) / filename
    logger.info(f"ç”ŸæˆPDF: {temp_file}")
    
    try:
        page.pdf(
            path=str(temp_file),
            format='A4',
            print_background=True,
            margin={'top': '1cm', 'right': '1cm', 'bottom': '1cm', 'left': '1cm'},
            scale=0.95
        )
        
        # æ£€æŸ¥ç”Ÿæˆçš„PDFæ–‡ä»¶å¤§å°
        if temp_file.exists():
            file_size = temp_file.stat().st_size
            logger.info(f"PDFæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼Œå¤§å°: {file_size} å­—èŠ‚")
            if file_size < timeout_config.min_pdf_size:  # ä½¿ç”¨é…ç½®çš„æœ€å°PDFå¤§å°
                logger.warning(f"è­¦å‘Šï¼šPDFæ–‡ä»¶å¾ˆå° ({file_size} å­—èŠ‚)ï¼Œå¯èƒ½æ˜¯ç©ºç™½é¡µé¢")
        
        return temp_file
    except Exception as pdf_err:
        logger.error(f"ç”ŸæˆPDFå¤±è´¥: {pdf_err}")
        return None

def process_page_with_failure_tracking(context, url, content_selector, toc_selector, base_url, timeout_config: TimeoutConfig, 
                max_retries, debug_mode=False, debug_dir=None, verbose_mode=False, load_strategy="normal", 
                url_blacklist_patterns=None, temp_dir=None):
    """å¤„ç†å•ä¸ªé¡µé¢å¹¶ç”ŸæˆPDFï¼ŒåŒæ—¶æå–è¯¥é¡µé¢å†…çš„é“¾æ¥ï¼ŒåŒ…å«å¤±è´¥è·Ÿè¸ª"""
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªURLï¼ˆæ ¹æ®PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
    if temp_dir:
        expected_pdf = Path(temp_dir) / url_to_filename(url)
        if expected_pdf.exists() and expected_pdf.stat().st_size > 1000:  # æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°åˆç†
            logger.info(f"å‘ç°å·²å­˜åœ¨çš„PDFæ–‡ä»¶ï¼Œè·³è¿‡å¤„ç†: {url}")
            # ä»ç„¶éœ€è¦æå–é“¾æ¥ï¼Œæ‰€ä»¥ç»§ç»­å¤„ç†ï¼Œä½†è·³è¿‡PDFç”Ÿæˆ
            pass
    
    page = context.new_page()
    pdf_path = None
    links = []
    final_url = url
    failure_reason = None
    
    try:
        logger.info(f"å‡†å¤‡å¤„ç†é¡µé¢: {url}")
        
        # å¤„ç†é¡µé¢åŠ è½½å’Œé‡è¯•é€»è¾‘
        try:
            final_url = _handle_page_loading_with_retries(
                page, url, content_selector, timeout_config, max_retries, 
                verbose_mode, load_strategy, url_blacklist_patterns
            )
        except Exception as e:
            failure_reason = f"é¡µé¢åŠ è½½å¤±è´¥: {str(e)}"
            logger.warning(f"é¡µé¢åŠ è½½å¤±è´¥ï¼Œå°†è®°å½•ä¸ºå¾…é‡è¯•: {url} - {failure_reason}")
            return None, [], url, failure_reason
        
        if final_url != url:
            logger.info(f"é‡å®šå‘: {url} -> {final_url}")
        
        # æå–é¡µé¢é“¾æ¥
        links = _extract_page_links(page, toc_selector, final_url, base_url)
        
        # å¦‚æœPDFå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if temp_dir:
            expected_pdf = Path(temp_dir) / url_to_filename(url)
            if expected_pdf.exists() and expected_pdf.stat().st_size > 1000:
                return expected_pdf, links, final_url, None
        
        # å‡†å¤‡é¡µé¢å†…å®¹ç”¨äºPDFç”Ÿæˆ
        if not _prepare_page_for_pdf(page, content_selector, verbose_mode, timeout_config, debug_mode, debug_dir, url):
            failure_reason = "å†…å®¹å…ƒç´ ä¸å¯è§æˆ–ä¸å­˜åœ¨"
            logger.warning(f"é¡µé¢å†…å®¹å‡†å¤‡å¤±è´¥ï¼Œå°†è®°å½•ä¸ºå¾…é‡è¯•: {url} - {failure_reason}")
            return None, links, final_url, failure_reason
        
        # ç”ŸæˆPDF
        if not temp_dir:
            temp_dir = tempfile.mkdtemp()
            
        pdf_path = _generate_pdf_from_page(page, verbose_mode, timeout_config, temp_dir, url)
        
        if not pdf_path:
            failure_reason = "PDFç”Ÿæˆå¤±è´¥"
            logger.warning(f"PDFç”Ÿæˆå¤±è´¥ï¼Œå°†è®°å½•ä¸ºå¾…é‡è¯•: {url} - {failure_reason}")
            return None, links, final_url, failure_reason
        
        return pdf_path, links, final_url, None
    
    except Exception as e:
        failure_reason = f"å¤„ç†é¡µé¢å¼‚å¸¸: {str(e)}"
        logger.error(f"å¤„ç†é¡µé¢å¤±è´¥: {url}\né”™è¯¯: {str(e)}", exc_info=True)
        return None, links, final_url, failure_reason
    
    finally:
        try:
            page.close()
            logger.info(f"å·²å…³é—­é¡µé¢: {url}")
        except Exception as close_err:
            logger.warning(f"å…³é—­é¡µé¢æ—¶å‡ºé”™: {str(close_err)}")

def process_page(context, url, content_selector, toc_selector, base_url, timeout_config: TimeoutConfig, 
                max_retries, debug_mode=False, debug_dir=None, verbose_mode=False, load_strategy="normal", 
                url_blacklist_patterns=None, temp_dir=None):
    """å¤„ç†å•ä¸ªé¡µé¢å¹¶ç”ŸæˆPDFï¼ŒåŒæ—¶æå–è¯¥é¡µé¢å†…çš„é“¾æ¥"""
    pdf_path, links, final_url, _ = process_page_with_failure_tracking(
        context, url, content_selector, toc_selector, base_url, timeout_config,
        max_retries, debug_mode, debug_dir, verbose_mode, load_strategy, url_blacklist_patterns, temp_dir
    )
    return pdf_path, links, final_url

def get_parent_path_pattern(base_url):
    """è·å–base_urlçš„çˆ¶ç›®å½•ä½œä¸ºé»˜è®¤URLåŒ¹é…æ¨¡å¼"""
    parsed = urlparse(base_url)
    path = parsed.path.rstrip('/')
    
    # å¦‚æœè·¯å¾„ä¸ºç©ºæˆ–è€…æ˜¯æ ¹è·¯å¾„ï¼Œä½¿ç”¨åŸŸå
    if not path or path == '/':
        return f"https?://{re.escape(parsed.netloc)}/.*"
    
    # è·å–çˆ¶ç›®å½•è·¯å¾„
    parent_path = '/'.join(path.split('/')[:-1])
    if not parent_path:
        parent_path = ''
    
    return f"https?://{re.escape(parsed.netloc)}{re.escape(parent_path)}/.*"

def compile_blacklist_patterns(blacklist_args):
    """ç¼–è¯‘URLé»‘åå•æ¨¡å¼"""
    if not blacklist_args:
        return []
    
    patterns = []
    for pattern_str in blacklist_args:
        try:
            pattern = re.compile(pattern_str, re.IGNORECASE)
            patterns.append(pattern)
            logger.info(f"æ·»åŠ URLé»‘åå•æ¨¡å¼: {pattern_str}")
        except re.error as e:
            logger.warning(f"æ— æ•ˆçš„URLé»‘åå•æ¨¡å¼ '{pattern_str}': {e}")
    
    return patterns

def _initialize_or_resume_progress(base_url_normalized, output_file, max_depth):
    """åˆå§‹åŒ–æ–°çš„è¿›åº¦çŠ¶æ€æˆ–ä»æ–‡ä»¶æ¢å¤è¿›åº¦çŠ¶æ€"""
    progress_file = create_progress_file_path(base_url_normalized, output_file)
    
    if progress_file.exists():
        logger.info(f"å‘ç°è¿›åº¦æ–‡ä»¶: {progress_file}")
        try:
            progress_state = ProgressState.load_from_file(progress_file)
            logger.info(f"æˆåŠŸæ¢å¤è¿›åº¦çŠ¶æ€:")
            logger.info(f"  - å·²è®¿é—®URL: {len(progress_state.visited_urls)} ä¸ª")
            logger.info(f"  - é˜Ÿåˆ—ä¸­URL: {len(progress_state.queue)} ä¸ª")
            logger.info(f"  - å·²ç”ŸæˆPDF: {len(progress_state.pdf_files)} ä¸ª")
            logger.info(f"  - å¤±è´¥URL: {len(progress_state.failed_urls)} ä¸ª")
            logger.info(f"  - ä¸´æ—¶ç›®å½•: {progress_state.temp_dir}")
            return progress_state, True
        except Exception as e:
            logger.warning(f"æ¢å¤è¿›åº¦çŠ¶æ€å¤±è´¥: {e}")
            logger.info("å°†åˆ›å»ºæ–°çš„è¿›åº¦çŠ¶æ€")
    
    # åˆ›å»ºæ–°çš„è¿›åº¦çŠ¶æ€
    progress_state = ProgressState(
        base_url=base_url_normalized,
        output_file=output_file,
        max_depth=max_depth,
        progress_file=progress_file
    )
    
    # åˆå§‹åŒ–é˜Ÿåˆ—
    progress_state.queue.append((base_url_normalized, 0))
    progress_state.enqueued.add(base_url_normalized)
    
    logger.info("åˆ›å»ºæ–°çš„è¿›åº¦çŠ¶æ€")
    return progress_state, False

def _crawl_pages_with_progress(context, args, base_url_normalized, url_pattern, url_blacklist_patterns, 
                              timeout_config, progress_state: ProgressState):
    """æ‰§è¡Œé¡µé¢çˆ¬å–é€»è¾‘ï¼Œæ”¯æŒè¿›åº¦æ¢å¤"""
    
    logger.info(f"å¼€å§‹/ç»§ç»­çˆ¬å–ï¼Œæœ€å¤§æ·±åº¦: {args.max_depth}")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not progress_state.temp_dir or not os.path.exists(progress_state.temp_dir):
        progress_state.temp_dir = tempfile.mkdtemp(prefix='site_to_pdf_')
        logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {progress_state.temp_dir}")
    
    processed_count = len(progress_state.visited_urls)  # å·²å¤„ç†çš„URLæ•°é‡
    
    while progress_state.queue:
        url, depth = progress_state.queue.popleft()
        processed_count += 1
        
        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
        total_discovered = len(progress_state.enqueued)
        progress_info = f"è¿›åº¦: [{processed_count}/{total_discovered}]"
        if len(progress_state.queue) > 0:
            progress_info += f" (é˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(progress_state.queue)} ä¸ª)"
        
        logger.info(f"{progress_info} å¤„ç†: {url} (æ·±åº¦: {depth})")
        
        if depth > args.max_depth:
            logger.warning(f"è¶…è¿‡æœ€å¤§æ·±åº¦é™åˆ¶({args.max_depth})ï¼Œè·³è¿‡: {url}")
            continue
            
        if url in progress_state.visited_urls:
            logger.info(f"å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {url}")
            continue
            
        try:
            pdf_path, links, final_url, failure_reason = process_page_with_failure_tracking(
                context, 
                url, 
                args.content_selector, 
                args.toc_selector,
                base_url_normalized,
                timeout_config,  # ä¼ é€’è¶…æ—¶é…ç½®å¯¹è±¡
                args.max_retries,
                args.debug,
                args.debug_dir,
                args.verbose,
                args.load_strategy,
                url_blacklist_patterns,  # ä¼ é€’URLé»‘åå•æ¨¡å¼
                progress_state.temp_dir  # ä¼ é€’ä¸´æ—¶ç›®å½•
            )
            
            progress_state.visited_urls.add(url)
            progress_state.visited_urls.add(final_url)
            
            if pdf_path and pdf_path.exists():
                progress_state.pdf_files.append(pdf_path)
                progress_state.processed_urls.append(url)
                logger.info(f"âœ… æˆåŠŸç”ŸæˆPDF: {pdf_path}")
            else:
                if failure_reason:
                    progress_state.failed_urls.append((url, failure_reason))
                    logger.warning(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ï¼Œè®°å½•å¾…é‡è¯•: {url} - {failure_reason}")
                else:
                    logger.warning(f"âŒ é¡µé¢æœªç”ŸæˆPDF: {url}")
            
            # å¤„ç†æ–°å‘ç°çš„é“¾æ¥
            new_links_count = 0
            for link in links:
                if not link:
                    continue
                    
                norm_url = normalize_url(link, base_url_normalized)
                
                if not url_pattern.match(norm_url):
                    logger.debug(f"è·³è¿‡ä¸ç¬¦åˆæ¨¡å¼çš„URL: {norm_url}")
                    continue
                
                if norm_url in progress_state.visited_urls or norm_url in progress_state.enqueued:
                    logger.debug(f"å·²å­˜åœ¨ï¼Œè·³è¿‡URL: {norm_url}")
                    continue
                
                logger.info(f"ğŸ”— æ·»åŠ æ–°URLåˆ°é˜Ÿåˆ—: {norm_url} (æ·±åº¦: {depth+1})")
                progress_state.queue.append((norm_url, depth + 1))
                progress_state.enqueued.add(norm_url)
                new_links_count += 1
            
            if new_links_count > 0:
                logger.info(f"ğŸ“Š ä»å½“å‰é¡µé¢å‘ç° {new_links_count} ä¸ªæ–°é“¾æ¥ï¼Œé˜Ÿåˆ—æ€»æ•°: {len(progress_state.queue)}")
            
            # æ¯å¤„ç†ä¸€ä¸ªURLå°±ä¿å­˜è¿›åº¦
            progress_state.save_to_file()
            
        except Exception as e:
            logger.exception(f"å¤„ç† {url} æ—¶å‘ç”Ÿé”™è¯¯")
            progress_state.failed_urls.append((url, f"å¼‚å¸¸é”™è¯¯: {str(e)}"))
            progress_state.visited_urls.add(url)
            # å³ä½¿å‡ºé”™ä¹Ÿè¦ä¿å­˜è¿›åº¦
            progress_state.save_to_file()
    
    # æœ€ç»ˆç»Ÿè®¡
    success_count = len(progress_state.processed_urls)
    failed_count = len(progress_state.failed_urls)
    total_processed = success_count + failed_count
    
    logger.info(f"\nğŸ“ˆ çˆ¬å–å®Œæˆç»Ÿè®¡:")
    logger.info(f"   æ€»å…±å¤„ç†: {total_processed} ä¸ªURL")
    logger.info(f"   æˆåŠŸ: {success_count} ä¸ª ({success_count/total_processed*100:.1f}%)")
    logger.info(f"   å¤±è´¥: {failed_count} ä¸ª ({failed_count/total_processed*100:.1f}%)")
    
    return progress_state

def _interactive_retry_failed_urls(context, failed_urls, args, base_url_normalized, timeout_config):
    """äº¤äº’å¼é‡è¯•å¤±è´¥çš„URL"""
    if not failed_urls:
        return [], []
    
    print(f"\n=== å‘ç° {len(failed_urls)} ä¸ªå¤±è´¥çš„URL ===")
    for i, (url, reason) in enumerate(failed_urls, 1):
        print(f"{i}. {url}")
        print(f"   å¤±è´¥åŸå› : {reason}")
    
    # å¦‚æœå¯ç”¨äº†è·³è¿‡å¤±è´¥é‡è¯•é€‰é¡¹ï¼Œç›´æ¥è¿”å›
    if args.skip_failed_retry:
        logger.info("å¯ç”¨äº†è·³è¿‡å¤±è´¥é‡è¯•é€‰é¡¹ï¼Œç›´æ¥å¤„ç†æˆåŠŸçš„é¡µé¢")
        return [], []
    
    while True:
        try:
            choice = input(f"\næ˜¯å¦è¦é‡è¯•å¤±è´¥çš„URLï¼Ÿ\n"
                          f"1. é‡è¯•æ‰€æœ‰å¤±è´¥çš„URL\n"
                          f"2. é€‰æ‹©æ€§é‡è¯•\n"
                          f"3. è·³è¿‡æ‰€æœ‰å¤±è´¥çš„URL\n"
                          f"è¯·é€‰æ‹© (1-3): ").strip()
            
            if choice == "3":
                logger.info("ç”¨æˆ·é€‰æ‹©è·³è¿‡æ‰€æœ‰å¤±è´¥çš„URL")
                return [], []
            elif choice == "1":
                urls_to_retry = [url for url, _ in failed_urls]
                break
            elif choice == "2":
                urls_to_retry = []
                for i, (url, reason) in enumerate(failed_urls, 1):
                    retry_choice = input(f"é‡è¯• URL {i}: {url} ? (y/n): ").strip().lower()
                    if retry_choice in ['y', 'yes', 'æ˜¯']:
                        urls_to_retry.append(url)
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")
                continue
        except (EOFError, KeyboardInterrupt):
            logger.info("ç”¨æˆ·å–æ¶ˆé‡è¯•")
            return [], []
    
    if not urls_to_retry:
        logger.info("æ²¡æœ‰é€‰æ‹©è¦é‡è¯•çš„URL")
        return [], []
    
    # è¯¢é—®é‡è¯•æ¬¡æ•°
    while True:
        try:
            retry_count = input(f"é‡è¯•æ¬¡æ•° (1-10, é»˜è®¤3): ").strip()
            if not retry_count:
                retry_count = 3
            else:
                retry_count = int(retry_count)
                if retry_count < 1 or retry_count > 10:
                    print("é‡è¯•æ¬¡æ•°å¿…é¡»åœ¨1-10ä¹‹é—´")
                    continue
            break
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            continue
        except (EOFError, KeyboardInterrupt):
            logger.info("ç”¨æˆ·å–æ¶ˆé‡è¯•")
            return [], []
    
    logger.info(f"å¼€å§‹é‡è¯• {len(urls_to_retry)} ä¸ªå¤±è´¥çš„URLï¼Œé‡è¯•æ¬¡æ•°: {retry_count}")
    
    retry_pdf_files = []
    retry_processed_urls = []
    still_failed_urls = []
    
    for i, url in enumerate(urls_to_retry, 1):
        logger.info(f"ğŸ”„ é‡è¯•è¿›åº¦: [{i}/{len(urls_to_retry)}] å¤„ç†: {url}")
        success = False
        
        for attempt in range(retry_count):
            try:
                pdf_path, _, final_url, failure_reason = process_page_with_failure_tracking(
                    context, 
                    url, 
                    args.content_selector, 
                    args.toc_selector,
                    base_url_normalized,
                    timeout_config,
                    args.max_retries,
                    args.debug,
                    args.debug_dir,
                    args.verbose,
                    args.load_strategy,
                    []  # é‡è¯•æ—¶ä¸åº”ç”¨é»‘åå•ï¼Œå¯èƒ½ä¹‹å‰è¢«è¯¯æ‹¦
                )
                
                if pdf_path and pdf_path.exists():
                    retry_pdf_files.append(pdf_path)
                    retry_processed_urls.append(url)
                    logger.info(f"âœ… é‡è¯•æˆåŠŸ: {url}")
                    success = True
                    break
                else:
                    logger.warning(f"âš ï¸ é‡è¯•ç¬¬ {attempt + 1}/{retry_count} æ¬¡å¤±è´¥: {url} - {failure_reason}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ é‡è¯•ç¬¬ {attempt + 1}/{retry_count} æ¬¡å¼‚å¸¸: {url} - {str(e)}")
        
        if not success:
            still_failed_urls.append((url, "é‡è¯•åä»ç„¶å¤±è´¥"))
            logger.error(f"âŒ é‡è¯•æ‰€æœ‰æ¬¡æ•°åä»ç„¶å¤±è´¥: {url}")
    
    # é‡è¯•ç»“æœç»Ÿè®¡
    retry_success_count = len(retry_processed_urls)
    retry_failed_count = len(still_failed_urls)
    logger.info(f"\nğŸ“Š é‡è¯•ç»“æœç»Ÿè®¡:")
    logger.info(f"   é‡è¯•æˆåŠŸ: {retry_success_count} ä¸ª")
    logger.info(f"   é‡è¯•åä»å¤±è´¥: {retry_failed_count} ä¸ª")
    
    if still_failed_urls:
        logger.warning(f"ä»æœ‰ {len(still_failed_urls)} ä¸ªURLé‡è¯•åä¾ç„¶å¤±è´¥:")
        for url, reason in still_failed_urls:
            logger.warning(f"  - {url}: {reason}")
    
    return retry_pdf_files, retry_processed_urls

def _merge_pdfs(pdf_files, processed_urls, args):
    """åˆå¹¶PDFæ–‡ä»¶"""
    if not pdf_files:
        logger.error("æœªç”Ÿæˆä»»ä½•PDFï¼Œè¯·æ£€æŸ¥å‚æ•°")
        return []
    
    logger.info(f"ğŸ“„ å‡†å¤‡åˆå¹¶ {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
    
    base_path = Path(args.output_pdf)
    stem = base_path.stem
    suffix = base_path.suffix if base_path.suffix else '.pdf'
    output_dir = base_path.parent
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    merger = PdfMerger()
    current_pages = 0
    file_index = 1
    merged_files = []

    for i, pdf_file in enumerate(pdf_files, 1):
        try:
            progress_info = f"åˆå¹¶è¿›åº¦: [{i}/{len(pdf_files)}]"
            logger.info(f"ğŸ“„ {progress_info} å¤„ç†PDFæ–‡ä»¶: {pdf_file}")
            
            if not pdf_file.exists():
                logger.warning(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_file}")
                continue
                
            with open(pdf_file, 'rb') as f:
                reader = PdfReader(f)
                num_pages = len(reader.pages)
                logger.debug(f"   æ–‡ä»¶é¡µæ•°: {num_pages}")
                
                if current_pages > 0 and current_pages + num_pages > args.max_page:
                    output_name = f"{stem}.{file_index}{suffix}"
                    output_path = output_dir / output_name
                    
                    logger.info(f"ğŸ“š å†™å…¥åˆ†å· {output_path} (é¡µæ•°: {current_pages})")
                    with open(output_path, 'wb') as out:
                        merger.write(out)
                    merged_files.append(str(output_path))
                    
                    file_index += 1
                    merger = PdfMerger()
                    current_pages = 0
                
                merger.append(str(pdf_file))
                current_pages += num_pages
                
                try:
                    pdf_file.unlink()
                    logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {pdf_file}")
                except Exception as unlink_err:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {unlink_err}")
                
        except Exception as e:
            logger.error(f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥ {pdf_file}: {e}")
    
    if current_pages > 0:
        if file_index == 1:
            output_path = base_path
        else:
            output_name = f"{stem}.{file_index}{suffix}"
            output_path = output_dir / output_name
        
        logger.info(f"ğŸ“š å†™å…¥æœ€ç»ˆPDF: {output_path} (é¡µæ•°: {current_pages})")
        with open(output_path, 'wb') as out:
            merger.write(out)
        merged_files.append(str(output_path))
    
    if merged_files:
        logger.info(f"ğŸ‰ å¤„ç†å®Œæˆ! å…±å¤„ç† {len(processed_urls)} ä¸ªé¡µé¢ï¼Œç”Ÿæˆ {len(merged_files)} ä¸ªPDFæ–‡ä»¶")
        logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {', '.join(merged_files)}")
    else:
        logger.error("æ²¡æœ‰PDFæ–‡ä»¶ç”Ÿæˆ")
    
    return merged_files

def main():
    parser = argparse.ArgumentParser(description="Webpage to PDF converter")
    parser.add_argument("--base-url", required=True, help="èµ·å§‹URL")
    parser.add_argument("--url-pattern", default=None, help="URLåŒ¹é…æ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼")
    parser.add_argument("--url-blacklist", action="append", default=[], 
                       help="URLé»‘åå•æ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¯æŒ‡å®šå¤šä¸ªï¼Œé˜»æ­¢æµè§ˆå™¨åŠ è½½åŒ¹é…çš„URL")
    parser.add_argument("--content-selector", required=True, help="å†…å®¹å®¹å™¨é€‰æ‹©å™¨")
    parser.add_argument("--toc-selector", required=True, help="é“¾æ¥æå–é€‰æ‹©å™¨")
    parser.add_argument("--output-pdf", required=True, help="è¾“å‡ºPDFè·¯å¾„")
    parser.add_argument("--max-page", type=int, default=10000, help="å•PDFæœ€å¤§é¡µæ•°")
    parser.add_argument("--timeout", type=int, default=120, help="é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--max-depth", type=int, default=10, help="æœ€å¤§çˆ¬å–æ·±åº¦")
    parser.add_argument("--max-retries", type=int, default=3, help="å¤±è´¥é‡è¯•æ¬¡æ•°")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜é¡µé¢æˆªå›¾")
    parser.add_argument("--debug-dir", default="debug_screenshots", help="è°ƒè¯•æˆªå›¾ä¿å­˜ç›®å½•")
    parser.add_argument("--verbose", action="store_true", help="æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢ï¼Œä¾¿äºè§‚å¯Ÿå¤„ç†è¿‡ç¨‹")
    parser.add_argument("--fast-load", action="store_true", help="å¿«é€ŸåŠ è½½æ¨¡å¼ï¼Œè·³è¿‡ç½‘ç»œç©ºé—²ç­‰å¾…")
    parser.add_argument("--load-strategy", choices=["fast", "normal", "thorough"], default="normal", 
                       help="é¡µé¢åŠ è½½ç­–ç•¥ï¼šfast=ä»…ç­‰å¾…DOM, normal=æ™ºèƒ½ç­‰å¾…, thorough=å®Œå…¨ç­‰å¾…ç½‘ç»œç©ºé—²")
    parser.add_argument("--skip-failed-retry", action="store_true", 
                       help="è·³è¿‡å¤±è´¥URLçš„äº¤äº’å¼é‡è¯•ï¼Œç›´æ¥å¤„ç†æˆåŠŸçš„é¡µé¢")
    parser.add_argument("--resume", action="store_true", 
                       help="è‡ªåŠ¨æ¢å¤ä¹‹å‰ä¸­æ–­çš„çˆ¬å–ä»»åŠ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
    parser.add_argument("--cleanup", action="store_true", 
                       help="æ¸…ç†æŒ‡å®šURLå’Œè¾“å‡ºæ–‡ä»¶å¯¹åº”çš„ä¸´æ—¶æ–‡ä»¶å’Œè¿›åº¦æ–‡ä»¶")
    args = parser.parse_args()
    
    # å¤„ç†æ¸…ç†å‘½ä»¤
    if args.cleanup:
        base_url_normalized = normalize_url(args.base_url)
        cleanup_temp_files(base_url_normalized, args.output_pdf)
        logger.info("æ¸…ç†å®Œæˆ")
        return
    
    logger.info(f"å¼€å§‹æ‰§è¡ŒPDFçˆ¬è™«ç¨‹åºï¼Œè¶…æ—¶è®¾ç½®: {args.timeout}ç§’")
    
    # åˆ›å»ºè¶…æ—¶é…ç½®å¯¹è±¡
    timeout_config = TimeoutConfig(args.timeout)
    logger.info(f"è¶…æ—¶é…ç½® - åŸºç¡€: {timeout_config.base_timeout}s, å¿«é€Ÿæ¨¡å¼: {timeout_config.fast_mode_timeout}s, "
               f"åˆå§‹åŠ è½½: {timeout_config.initial_load_timeout}ms, é¡µé¢æ¸²æŸ“: {timeout_config.page_render_wait}s")
    
    base_url_normalized = normalize_url(args.base_url, args.base_url)
    logger.info(f"æ ‡å‡†åŒ–åŸºå‡†URL: {base_url_normalized}")
    
    # ç¼–è¯‘URLé»‘åå•æ¨¡å¼
    url_blacklist_patterns = compile_blacklist_patterns(args.url_blacklist)
    if url_blacklist_patterns:
        logger.info(f"é…ç½®äº† {len(url_blacklist_patterns)} ä¸ªURLé»‘åå•æ¨¡å¼")
    
    temp_dir = tempfile.TemporaryDirectory()
    logger.info(f"ä¸´æ—¶ç›®å½•åˆ›å»º: {temp_dir.name}")
    
    # ä¿®æ”¹é»˜è®¤URLæ¨¡å¼ï¼šä½¿ç”¨çˆ¶ç›®å½•è€ŒéåŸŸå
    if args.url_pattern:
        url_pattern = re.compile(args.url_pattern)
        logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰URLåŒ¹é…æ¨¡å¼: {url_pattern.pattern}")
    else:
        default_pattern = get_parent_path_pattern(base_url_normalized)
        url_pattern = re.compile(default_pattern)
        logger.info(f"ä½¿ç”¨é»˜è®¤URLåŒ¹é…æ¨¡å¼ï¼ˆåŸºäºçˆ¶ç›®å½•ï¼‰: {url_pattern.pattern}")
    
    with sync_playwright() as p:
        # æ ¹æ®verboseå‚æ•°å†³å®šæ˜¯å¦æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢
        headless_mode = not args.verbose
        if args.verbose:
            logger.info("å¯ç”¨å¯è§†åŒ–æ¨¡å¼ - æµè§ˆå™¨ç•Œé¢å°†æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹")
        else:
            logger.info("ä½¿ç”¨æ— å¤´æ¨¡å¼ - æµè§ˆå™¨åœ¨åå°è¿è¡Œ")
            
        browser = p.chromium.launch(
            headless=headless_mode, 
            args=[
                '--disable-gpu',
                '--disable-dev-shm-usage',
                '--disable-setuid-sandbox',
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled'
            ] if headless_mode else [
                '--disable-blink-features=AutomationControlled'
            ]  # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹å‡å°‘å¯åŠ¨å‚æ•°ï¼Œé¿å…å½±å“æ˜¾ç¤º
        )
        context = browser.new_context(
            viewport={'width': 1366, 'height': 768},
            ignore_https_errors=True,
            java_script_enabled=True,
            bypass_csp=True
        )
        
        context.set_default_timeout(args.timeout * 1000)
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œæ”¯æŒä¸­æ–­æ¢å¤
        setup_signal_handlers()
        
        # åˆå§‹åŒ–æˆ–æ¢å¤è¿›åº¦çŠ¶æ€
        progress_state, is_resumed = _initialize_or_resume_progress(
            base_url_normalized, args.output_pdf, args.max_depth
        )
        
        if is_resumed and not args.resume:
            response = input("å‘ç°æœªå®Œæˆçš„çˆ¬å–ä»»åŠ¡ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]: ").strip().lower()
            if response not in ['y', 'yes']:
                logger.info("ç”¨æˆ·é€‰æ‹©ä¸ç»§ç»­ï¼Œé€€å‡º")
                browser.close()
                return
        
        try:
            # æ‰§è¡Œçˆ¬å–ï¼ˆæ”¯æŒè¿›åº¦æ¢å¤ï¼‰
            progress_state = _crawl_pages_with_progress(
                context, args, base_url_normalized, url_pattern, 
                url_blacklist_patterns, timeout_config, progress_state
            )
            
            # å¦‚æœæœ‰å¤±è´¥çš„URLï¼Œè¯¢é—®æ˜¯å¦é‡è¯•
            if progress_state.failed_urls and not args.skip_failed_retry:
                retry_pdf_files, retry_processed_urls = _interactive_retry_failed_urls(
                    context, progress_state.failed_urls, args, base_url_normalized, timeout_config
                )
                
                # åˆå¹¶é‡è¯•æˆåŠŸçš„æ–‡ä»¶
                progress_state.pdf_files.extend(retry_pdf_files)
                progress_state.processed_urls.extend(retry_processed_urls)
            
            logger.info(f"çˆ¬å–å®Œæˆï¼Œå…³é—­æµè§ˆå™¨...")
            browser.close()
            
            # åˆå¹¶PDFæ–‡ä»¶
            _merge_pdfs(progress_state.pdf_files, progress_state.processed_urls, args)
            
            # æˆåŠŸå®Œæˆåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if progress_state.temp_dir and os.path.exists(progress_state.temp_dir):
                logger.info("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
                shutil.rmtree(progress_state.temp_dir)
            
            # åˆ é™¤è¿›åº¦æ–‡ä»¶
            if progress_state.progress_file and progress_state.progress_file.exists():
                progress_state.progress_file.unlink()
                logger.info("åˆ é™¤è¿›åº¦æ–‡ä»¶")
        
        except KeyboardInterrupt:
            logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            logger.info(f"è¿›åº¦å·²ä¿å­˜åˆ°: {progress_state.progress_file}")
            logger.info(f"ä¸´æ—¶æ–‡ä»¶ä½äº: {progress_state.temp_dir}")
            logger.info("ä¸‹æ¬¡è¿è¡Œæ—¶å¯ä½¿ç”¨ --resume å‚æ•°ç»§ç»­")
            browser.close()
            return
        except Exception as e:
            logger.exception("ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
            browser.close()
            raise

if __name__ == "__main__":
    main()
