import argparse
import hashlib
import json
import logging
import os
import re
import shutil
import signal
import sys
import tempfile
import time
import urllib.parse
from collections import deque
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any
from urllib.parse import urljoin, urlparse

from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
from playwright.sync_api import sync_playwright
from PyPDF2 import PdfMerger, PdfReader

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


@dataclass
class PageTask:
    """é¡µé¢ä»»åŠ¡ä¿¡æ¯"""

    url: str
    depth: int
    page: Any = None
    loaded: bool = False
    error: str | None = None


@dataclass
class ParallelPageState:
    """å¹¶è¡Œé¡µé¢çŠ¶æ€ç®¡ç†"""
    url: str
    depth: int
    page: Any = None
    is_loading: bool = False
    is_loaded: bool = False
    load_error: str | None = None
    final_url: str | None = None


class TrueParallelProcessor:
    """çœŸæ­£çš„å¹¶è¡Œå¤„ç†å™¨ - åŒæ—¶æ‰“å¼€å¤šä¸ªæ ‡ç­¾é¡µé¢„åŠ è½½"""

    def __init__(self, context, parallel_count: int):
        """
        åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨
        
        Args:
            context: Playwrightæµè§ˆå™¨ä¸Šä¸‹æ–‡
            parallel_count: å¹¶è¡Œåº¦ï¼ŒåŒæ—¶æ‰“å¼€çš„æ ‡ç­¾é¡µæ•°é‡
        """
        self.context = context
        self.parallel_count = parallel_count
        self.page_states: list[ParallelPageState | None] = [None] * parallel_count
        logger.info(f"åˆ›å»ºçœŸæ­£å¹¶è¡Œå¤„ç†å™¨ï¼Œå¹¶è¡Œåº¦: {parallel_count}")

    def _start_page_loading(self, slot_index: int, url: str, depth: int, args, timeout_config, url_blacklist_patterns):
        """åœ¨æŒ‡å®šæ§½ä½å¼€å§‹åŠ è½½é¡µé¢"""
        try:
            # åˆ›å»ºæ–°é¡µé¢
            page = self.context.new_page()
            
            # åˆ›å»ºé¡µé¢çŠ¶æ€
            page_state = ParallelPageState(
                url=url,
                depth=depth,
                page=page,
                is_loading=True,
                is_loaded=False
            )
            self.page_states[slot_index] = page_state
            
            logger.info(f"ğŸš€ æ§½ä½[{slot_index}] å¼€å§‹é¢„åŠ è½½: {url}")
            
            # å¼‚æ­¥å¼€å§‹é¡µé¢åŠ è½½ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
            # è¿™é‡Œåªæ˜¯å‘èµ·å¯¼èˆªè¯·æ±‚ï¼Œä¸ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            page.goto(url, wait_until="domcontentloaded", timeout=timeout_config.initial_load_timeout)
            logger.info(f"ğŸ“¡ æ§½ä½[{slot_index}] DOMå·²åŠ è½½: {url}")
            
            return True
            
        except Exception as e:
            logger.warning(f"âŒ æ§½ä½[{slot_index}] é¢„åŠ è½½å¤±è´¥: {url} - {e!s}")
            if 'page_state' in locals() and page_state.page:
                try:
                    page_state.page.close()
                except:
                    pass
            self.page_states[slot_index] = None
            return False

    def _complete_page_loading(self, slot_index: int, args, timeout_config, url_blacklist_patterns):
        """å®ŒæˆæŒ‡å®šæ§½ä½çš„é¡µé¢åŠ è½½"""
        page_state = self.page_states[slot_index]
        if not page_state or not page_state.page:
            return False
            
        try:
            logger.info(f"â³ æ§½ä½[{slot_index}] å®Œæˆé¡µé¢åŠ è½½: {page_state.url}")
            
            # è®¾ç½®è¯·æ±‚æ‹¦æˆª
            _setup_request_blocking(page_state.page, url_blacklist_patterns)
            
            # è®¾ç½®æ…¢è¯·æ±‚ç›‘æ§
            _setup_slow_request_monitoring(page_state.page, timeout_config)
            
            # å®Œæˆé¡µé¢åŠ è½½
            final_url = _handle_page_loading_with_retries(
                page_state.page,
                page_state.url,
                args.content_selector,
                timeout_config,
                args.max_retries,
                args.verbose,
                args.load_strategy,
                url_blacklist_patterns,
            )
            
            page_state.is_loading = False
            page_state.is_loaded = True
            page_state.final_url = final_url
            logger.info(f"âœ… æ§½ä½[{slot_index}] åŠ è½½å®Œæˆ: {page_state.url}")
            return True
            
        except Exception as e:
            page_state.is_loading = False
            page_state.load_error = str(e)
            logger.warning(f"âŒ æ§½ä½[{slot_index}] åŠ è½½å¤±è´¥: {page_state.url} - {e!s}")
            return False

    def _process_page_content(self, slot_index: int, args, base_url_normalized, timeout_config, progress_state):
        """å¤„ç†é¡µé¢å†…å®¹å¹¶ç”ŸæˆPDF"""
        page_state = self.page_states[slot_index]
        if not page_state or not page_state.page or not page_state.is_loaded:
            return None, []
            
        try:
            logger.info(f"ğŸ“„ æ§½ä½[{slot_index}] å¼€å§‹å†…å®¹å¤„ç†: {page_state.url}")
            
            # æå–é¡µé¢é“¾æ¥
            links = _extract_page_links(
                page_state.page, 
                args.toc_selector, 
                page_state.final_url or page_state.url, 
                base_url_normalized
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰PDFæ–‡ä»¶
            existing_pdf = _check_existing_pdf(progress_state.temp_dir, page_state.url)
            if existing_pdf:
                logger.info(f"ğŸ“‹ æ§½ä½[{slot_index}] å‘ç°å·²å­˜åœ¨PDF: {page_state.url}")
                return existing_pdf, links
            
            # ç”ŸæˆPDF
            pdf_path = _generate_pdf_with_validation(
                page_state.page,
                args.content_selector,
                args.verbose,
                timeout_config,
                args.debug,
                args.debug_dir,
                progress_state.temp_dir,
                page_state.url,
            )
            
            logger.info(f"âœ… æ§½ä½[{slot_index}] å†…å®¹å¤„ç†å®Œæˆ: {page_state.url}")
            return pdf_path, links
            
        except Exception as e:
            logger.error(f"âŒ æ§½ä½[{slot_index}] å†…å®¹å¤„ç†å¤±è´¥: {page_state.url} - {e!s}")
            return None, []

    def _close_page_slot(self, slot_index: int):
        """å…³é—­æŒ‡å®šæ§½ä½çš„é¡µé¢"""
        page_state = self.page_states[slot_index]
        if page_state and page_state.page:
            try:
                # æ·»åŠ è¶…æ—¶æœºåˆ¶ï¼Œé˜²æ­¢é¡µé¢å…³é—­æ—¶å¡ä½
                page_state.page.close()
                logger.debug(f"ğŸ”„ æ§½ä½[{slot_index}] é¡µé¢å·²å…³é—­: {page_state.url}")
            except Exception as e:
                logger.debug(f"å…³é—­æ§½ä½[{slot_index}]é¡µé¢æ—¶å‡ºé”™: {e}")
        self.page_states[slot_index] = None

    def close_all(self):
        """å…³é—­æ‰€æœ‰é¡µé¢"""
        logger.info("ğŸ”„ æ­£åœ¨å…³é—­æ‰€æœ‰å¹¶è¡Œé¡µé¢...")
        for i in range(self.parallel_count):
            try:
                self._close_page_slot(i)
            except Exception as e:
                logger.warning(f"å…³é—­æ§½ä½[{i}]æ—¶å‡ºé”™: {e}")
                # å¼ºåˆ¶æ¸…ç©ºçŠ¶æ€ï¼Œå³ä½¿å…³é—­å¤±è´¥
                self.page_states[i] = None
        logger.info("å¹¶è¡Œé¡µé¢å¤„ç†å™¨å·²å…³é—­")


@dataclass
class TimeoutConfig:
    """è¶…æ—¶é…ç½®ç®¡ç†"""

    base_timeout: int  # åŸºç¡€è¶…æ—¶æ—¶é—´ï¼ˆæ¥è‡ªå‘½ä»¤è¡Œå‚æ•°ï¼‰

    @property
    def initial_load_timeout(self) -> int:
        """åˆå§‹é¡µé¢åŠ è½½è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰"""
        return max(self.base_timeout, 30) * 1000

    @property
    def fast_mode_timeout(self) -> int:
        """å¿«é€Ÿæ¨¡å¼è¶…æ—¶ï¼ˆç§’ï¼‰"""
        return max(self.base_timeout, 30)

    @property
    def content_additional_wait(self) -> int:
        """å†…å®¹åŠ è½½é¢å¤–ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        return max(5, self.base_timeout // 4)

    @property
    def thorough_min_timeout(self) -> int:
        """å½»åº•æ¨¡å¼æœ€å°ä¿ç•™è¶…æ—¶ï¼ˆç§’ï¼‰"""
        return max(10, self.base_timeout // 2)

    @property
    def retry_backoff_max(self) -> int:
        """é‡è¯•é€€é¿æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        return max(10, self.base_timeout // 6)

    @property
    def element_check_interval(self) -> float:
        """å…ƒç´ æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰"""
        return 5.0

    @property
    def fast_check_interval(self) -> float:
        """å¿«é€Ÿæ¨¡å¼æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰"""
        return 0.5

    @property
    def page_render_wait(self) -> float:
        """é¡µé¢æ¸²æŸ“ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰"""
        return max(2.0, self.base_timeout * 0.02)

    @property
    def min_pdf_size(self) -> int:
        """æœ€å°PDFæ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
        return 5000

    @property
    def slow_request_threshold(self) -> float:
        """æ…¢è¯·æ±‚é˜ˆå€¼ï¼ˆè¶…æ—¶æ—¶é—´çš„1/10ï¼‰"""
        return self.base_timeout / 10.0


@dataclass
class DomainFailureTracker:
    """åŸŸåå¤±è´¥è·Ÿè¸ªå™¨ï¼Œç”¨äºè‡ªåŠ¨é»‘åå•åŠŸèƒ½"""

    failure_counts: dict = field(default_factory=dict)  # {domain: failure_count}
    auto_threshold: int = 10
    auto_blacklist_patterns: list = field(default_factory=list)

    def record_failure(self, url: str):
        """è®°å½•URLå¤±è´¥ï¼Œæå–åŸŸåå¹¶å¢åŠ å¤±è´¥è®¡æ•°"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            if not domain:
                return False

            # å¢åŠ å¤±è´¥è®¡æ•°
            self.failure_counts[domain] = self.failure_counts.get(domain, 0) + 1

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è‡ªåŠ¨é»‘åå•é˜ˆå€¼
            if self.failure_counts[domain] >= self.auto_threshold:
                # åˆ›å»ºåŸŸåé»‘åå•æ¨¡å¼
                domain_pattern = f"https?://{re.escape(domain)}/.*"

                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é»‘åå•ä¸­
                pattern_exists = any(pattern.pattern == domain_pattern for pattern in self.auto_blacklist_patterns)

                if not pattern_exists:
                    try:
                        compiled_pattern = re.compile(domain_pattern, re.IGNORECASE)
                        self.auto_blacklist_patterns.append(compiled_pattern)
                        logger.warning(f"ğŸš« åŸŸå {domain} å¤±è´¥ {self.failure_counts[domain]} æ¬¡ï¼Œè‡ªåŠ¨åŠ å…¥é»‘åå•")
                        return True
                    except re.error as e:
                        logger.warning(f"åˆ›å»ºè‡ªåŠ¨é»‘åå•æ¨¡å¼å¤±è´¥: {e}")

            return False

        except Exception as e:
            logger.debug(f"è®°å½•åŸŸåå¤±è´¥æ—¶å‡ºé”™: {e}")
            return False

    def get_all_patterns(self, manual_patterns: list[Any] | None = None):
        """è·å–æ‰€æœ‰é»‘åå•æ¨¡å¼ï¼ˆæ‰‹åŠ¨+è‡ªåŠ¨ï¼‰"""
        all_patterns = []

        # æ·»åŠ æ‰‹åŠ¨é»‘åå•
        if manual_patterns:
            all_patterns.extend(manual_patterns)

        # æ·»åŠ è‡ªåŠ¨é»‘åå•
        all_patterns.extend(self.auto_blacklist_patterns)

        return all_patterns

    def get_failure_summary(self):
        """è·å–å¤±è´¥ç»Ÿè®¡æ‘˜è¦"""
        if not self.failure_counts:
            return "æ— åŸŸåå¤±è´¥è®°å½•"

        # æŒ‰å¤±è´¥æ¬¡æ•°æ’åº
        sorted_failures = sorted(
            self.failure_counts.items(),
            key=lambda x: x[1],
            reverse=True,
        )

        summary_lines = [f"åŸŸåå¤±è´¥ç»Ÿè®¡ (é˜ˆå€¼: {self.auto_threshold}):"]
        for domain, count in sorted_failures[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            status = "ğŸš«å·²æ‹‰é»‘" if count >= self.auto_threshold else "âš ï¸è­¦å‘Š"
            summary_lines.append(f"  {status} {domain}: {count} æ¬¡")

        if len(sorted_failures) > 10:
            summary_lines.append(f"  ... è¿˜æœ‰ {len(sorted_failures) - 10} ä¸ªåŸŸå")

        return "\n".join(summary_lines)


@dataclass
class ProgressState:
    """è¿›åº¦çŠ¶æ€ç®¡ç†"""

    base_url: str
    output_pdf: str
    temp_dir: str
    progress_file: str
    visited_urls: set
    failed_urls: list
    processed_urls: list
    pdf_files: list
    queue: deque
    enqueued: set

    def save_to_file(self):
        """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
        state_data = {
            "base_url": self.base_url,
            "output_pdf": self.output_pdf,
            "temp_dir": self.temp_dir,
            "visited_urls": list(self.visited_urls),
            "failed_urls": self.failed_urls,
            "processed_urls": self.processed_urls,
            "pdf_files": [str(f) for f in self.pdf_files],
            "queue": list(self.queue),
            "enqueued": list(self.enqueued),
        }

        with open(self.progress_file, "w", encoding="utf-8") as f:
            json.dump(state_data, f, ensure_ascii=False, indent=2)
        logger.debug(f"è¿›åº¦å·²ä¿å­˜åˆ°: {self.progress_file}")

    @classmethod
    def load_from_file(cls, progress_file: str):
        """ä»æ–‡ä»¶åŠ è½½è¿›åº¦"""
        if not os.path.exists(progress_file):
            return None

        try:
            with open(progress_file, encoding="utf-8") as f:
                state_data = json.load(f)

            # éªŒè¯ä¸´æ—¶PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            valid_pdf_files = []
            for pdf_file_str in state_data.get("pdf_files", []):
                pdf_path = Path(pdf_file_str)
                if pdf_path.exists():
                    valid_pdf_files.append(pdf_path)
                else:
                    logger.warning(f"ä¸´æ—¶PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²ä»è¿›åº¦ä¸­ç§»é™¤: {pdf_file_str}")

            progress = cls(
                base_url=state_data.get("base_url", ""),
                output_pdf=state_data.get("output_pdf", ""),
                temp_dir=state_data.get("temp_dir", ""),
                progress_file=progress_file,
                visited_urls=set(state_data.get("visited_urls", [])),
                failed_urls=state_data.get("failed_urls", []),
                processed_urls=state_data.get("processed_urls", []),
                pdf_files=valid_pdf_files,
                queue=deque(state_data.get("queue", [])),
                enqueued=set(state_data.get("enqueued", [])),
            )

            logger.info(
                f"ä»è¿›åº¦æ–‡ä»¶æ¢å¤çŠ¶æ€: å·²å¤„ç† {len(progress.processed_urls)} ä¸ªURLï¼Œ"
                f"é˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(progress.queue)} ä¸ªURL"
            )

            return progress

        except Exception as e:
            logger.error(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return None


def url_to_filename(url: str) -> str:
    """å°†URLè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    # ä½¿ç”¨URLçš„å“ˆå¸Œå€¼ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ï¼Œç¡®ä¿å”¯ä¸€æ€§
    url_hash = hashlib.md5(url.encode("utf-8")).hexdigest()[:8]

    # æ¸…ç†URLç”¨ä½œæ–‡ä»¶å
    safe_name = re.sub(r"[^\w\-_\.]", "_", url.replace("https://", "").replace("http://", ""))
    safe_name = safe_name[:50]  # é™åˆ¶é•¿åº¦

    return f"{safe_name}_{url_hash}.pdf"


def setup_signal_handlers(progress_state: ProgressState):
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…é€€å‡º"""

    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        progress_state.save_to_file()
        logger.info("è¿›åº¦å·²ä¿å­˜ï¼Œç¨‹åºé€€å‡º")
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·


def create_progress_file_path(cache_dir: Path, base_url: str) -> str:
    """åˆ›å»ºè¿›åº¦æ–‡ä»¶è·¯å¾„"""
    # ä½¿ç”¨base_urlçš„å“ˆå¸Œå€¼ç¡®ä¿å”¯ä¸€æ€§
    url_hash = hashlib.md5(base_url.encode("utf-8")).hexdigest()[:8]

    progress_file = cache_dir / f"progress_{url_hash}.json"
    return str(progress_file)


def calculate_cache_id(
    base_url: str, content_selector: str, toc_selector: str, max_depth: int, url_pattern: str | None = None
) -> str:
    """æ ¹æ®å…³é”®å‚æ•°è®¡ç®—ç¼“å­˜ID"""
    # å°†å…³é”®å‚æ•°ç»„åˆæˆå­—ç¬¦ä¸²
    key_params = f"{base_url}|{content_selector}|{toc_selector}|{max_depth}|{url_pattern or ''}"

    # è®¡ç®—MD5å“ˆå¸Œ
    cache_id = hashlib.md5(key_params.encode("utf-8")).hexdigest()[:12]
    return cache_id


def get_cache_directory(cache_id: str) -> Path:
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    # åœ¨ç³»ç»Ÿä¸´æ—¶ç›®å½•ä¸‹åˆ›å»ºä¸“ç”¨çš„ç¼“å­˜ç›®å½•
    base_cache_dir = Path(tempfile.gettempdir()) / "site_to_pdf_cache"
    cache_dir = base_cache_dir / cache_id

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    cache_dir.mkdir(parents=True, exist_ok=True)

    return cache_dir


def cleanup_cache_directory(cache_dir: Path):
    """æ¸…ç†ç¼“å­˜ç›®å½•"""
    if cache_dir.exists():
        try:
            shutil.rmtree(cache_dir)
            logger.info(f"å·²æ¸…ç†ç¼“å­˜ç›®å½•: {cache_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ç¼“å­˜ç›®å½•å¤±è´¥: {e}")
    else:
        logger.debug(f"ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")


def cleanup_temp_files(temp_dir: str, progress_file: str | None = None):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    cleaned_count = 0

    # æ¸…ç†ä¸´æ—¶PDFæ–‡ä»¶
    if temp_dir and os.path.exists(temp_dir):
        temp_path = Path(temp_dir)
        for pdf_file in temp_path.glob("*.pdf"):
            try:
                pdf_file.unlink()
                cleaned_count += 1
                logger.debug(f"åˆ é™¤ä¸´æ—¶PDF: {pdf_file}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶PDFå¤±è´¥ {pdf_file}: {e}")

        # å°è¯•åˆ é™¤ä¸´æ—¶ç›®å½•
        try:
            temp_path.rmdir()
            logger.debug(f"åˆ é™¤ä¸´æ—¶ç›®å½•: {temp_path}")
        except Exception as e:
            logger.debug(f"ä¸´æ—¶ç›®å½•éç©ºæˆ–åˆ é™¤å¤±è´¥ {temp_path}: {e}")

    # æ¸…ç†è¿›åº¦æ–‡ä»¶
    if progress_file and os.path.exists(progress_file):
        try:
            os.unlink(progress_file)
            logger.debug(f"åˆ é™¤è¿›åº¦æ–‡ä»¶: {progress_file}")
        except Exception as e:
            logger.warning(f"åˆ é™¤è¿›åº¦æ–‡ä»¶å¤±è´¥ {progress_file}: {e}")

    if cleaned_count > 0:
        logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªä¸´æ—¶PDFæ–‡ä»¶")

    return cleaned_count


def normalize_url(url, base_url):
    """æ ‡å‡†åŒ–URLå¹¶ç§»é™¤URLç‰‡æ®µ"""
    parsed = urlparse(url)
    base_parsed = urlparse(base_url)

    # å¤„ç†ç›¸å¯¹URL
    if not parsed.scheme:
        url = urllib.parse.urljoin(base_url, url)
        parsed = urlparse(url)

    # åˆ›å»ºæ–°çš„URLå¯¹è±¡
    normalized = parsed._replace(
        scheme=base_parsed.scheme or "https",
        path=urllib.parse.unquote(parsed.path) if parsed.path else "",
        fragment="",
        query=parsed.query,
    )

    # ç”Ÿæˆè§„èŒƒåŒ–URLå­—ç¬¦ä¸²
    normalized_url = normalized.geturl()

    # å¤„ç†é‡å¤æ–œæ 
    normalized_url = re.sub(r"([^:])//+", r"\1/", normalized_url)

    # ç»Ÿä¸€åè®®å¤„ç†
    if normalized_url.startswith("http://"):
        normalized_url = "https://" + normalized_url[7:]

    return normalized_url


def resolve_selector(selector):
    """æ™ºèƒ½è§£æé€‰æ‹©å™¨"""
    if selector.startswith("/"):
        if not selector.startswith("//"):
            return f"selector=/{selector[1:]}"
        return f"selector={selector}"
    return selector


def check_element_visibility_and_content(page, selector: str) -> tuple[bool, str, int, dict[str, Any]]:
    """æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ã€å¯è§ä¸”æœ‰è¶³å¤Ÿå†…å®¹"""
    element = page.query_selector(resolve_selector(selector))
    if not element:
        return False, "å…ƒç´ ä¸å­˜åœ¨", 0, {}

    element_info = page.evaluate(
        """(el) => {
        const rect = el.getBoundingClientRect();
        const style = window.getComputedStyle(el);
        return {
            textLength: el.textContent ? el.textContent.trim().length : 0,
            isVisible: rect.width > 0 && rect.height > 0,
            width: rect.width,
            height: rect.height,
            display: style.display,
            visibility: style.visibility,
            opacity: parseFloat(style.opacity) || 1
        };
    }""",
        element,
    )

    # æ£€æŸ¥å¯è§æ€§
    is_visible = (
        element_info["isVisible"]
        and element_info["display"] != "none"
        and element_info["visibility"] != "hidden"
        and element_info["opacity"] > 0.1
    )

    if not is_visible:
        reason = f"å…ƒç´ ä¸å¯è§ (display:{element_info['display']}, visibility:{element_info['visibility']}, opacity:{element_info['opacity']}, size:{element_info['width']}x{element_info['height']})"
        return False, reason, element_info["textLength"], element_info

    return True, "å…ƒç´ å¯è§", element_info["textLength"], element_info


def _get_wait_config(strategy: str, timeout_config):
    """æ ¹æ®ç­–ç•¥è·å–ç­‰å¾…é…ç½®"""
    if strategy == "fast":
        return timeout_config.fast_mode_timeout, timeout_config.fast_check_interval
    if strategy == "thorough":
        return timeout_config.base_timeout, timeout_config.element_check_interval
    # normal
    return timeout_config.base_timeout, timeout_config.element_check_interval


def _log_wait_strategy(strategy: str, timeout: float):
    """è®°å½•ç­‰å¾…ç­–ç•¥ä¿¡æ¯"""
    if strategy == "fast":
        logger.info(f"å¿«é€Ÿç­‰å¾…å…ƒç´ å¯è§ï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´ {timeout} ç§’")
    elif strategy == "thorough":
        logger.info(f"å½»åº•æ¨¡å¼ï¼šæŒç»­ç­‰å¾…å…ƒç´ å¯è§ï¼Œå‰©ä½™ç­‰å¾…æ—¶é—´ {timeout:.1f} ç§’")
    else:  # normal
        logger.info(f"æ™ºèƒ½ç­‰å¾…æ¨¡å¼ï¼šæŒç»­ç­‰å¾…å…ƒç´ å¯è§ï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´ {timeout} ç§’")


def _handle_normal_strategy_content(page, selector, text_length, timeout_config, wait_start_time, timeout):
    """å¤„ç†normalç­–ç•¥çš„å†…å®¹æ£€æŸ¥é€»è¾‘"""
    if text_length > 100:  # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿå†…å®¹ï¼Œç›´æ¥æˆåŠŸ
        logger.info(f"å†…å®¹å……è¶³ ({text_length} å­—ç¬¦)ï¼Œå®Œæˆç­‰å¾…")
        return True
    if text_length > 0:
        # æœ‰å°‘é‡å†…å®¹ï¼Œå†ç­‰å¾…ä¸€æ®µæ—¶é—´çœ‹æ˜¯å¦æœ‰æ›´å¤šå†…å®¹åŠ è½½
        remaining_time = timeout - (time.time() - wait_start_time)
        additional_wait = min(remaining_time, timeout_config.content_additional_wait)

        if additional_wait > 0:
            logger.info(f"å†…å®¹è¾ƒå°‘ ({text_length} å­—ç¬¦)ï¼Œå†ç­‰å¾… {additional_wait:.1f} ç§’çœ‹æ˜¯å¦æœ‰æ›´å¤šå†…å®¹...")
            time.sleep(additional_wait)

            # å†æ¬¡æ£€æŸ¥
            is_ready_again, _, text_length_again, _ = check_element_visibility_and_content(page, selector)
            if is_ready_again and text_length_again >= text_length:
                logger.info(f"å†…å®¹å·²æ›´æ–°åˆ° {text_length_again} å­—ç¬¦ï¼Œæ¥å—å½“å‰çŠ¶æ€")
            else:
                logger.info(f"å†…å®¹æ— æ˜æ˜¾å¢åŠ ï¼Œæ¥å—å½“å‰çŠ¶æ€ ({text_length} å­—ç¬¦)")

        return True
    logger.info("å…ƒç´ å¯è§ä½†æ— æ–‡æœ¬å†…å®¹ï¼Œç»§ç»­ç­‰å¾…...")
    return False


def _check_consecutive_failures(status_msg, consecutive_failures, max_consecutive_failures):
    """æ£€æŸ¥è¿ç»­å¤±è´¥æ˜¯å¦éœ€è¦å¿«é€Ÿå¤±è´¥"""
    if "å…ƒç´ ä¸å­˜åœ¨" in status_msg and consecutive_failures >= max_consecutive_failures:
        logger.warning(f"å…ƒç´ è¿ç»­ {consecutive_failures} æ¬¡ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯å¤–éƒ¨é“¾æ¥æˆ–æ— æ•ˆé¡µé¢ï¼Œå¿«é€Ÿå¤±è´¥")
        return True
    return False


def wait_for_element_visible(page, selector: str, timeout_config: TimeoutConfig, strategy: str = "normal") -> bool:
    """ç­‰å¾…å…ƒç´ å¯è§çš„é€šç”¨å‡½æ•°"""
    timeout, check_interval = _get_wait_config(strategy, timeout_config)
    _log_wait_strategy(strategy, timeout)

    wait_start_time = time.time()
    consecutive_failures = 0  # è¿ç»­å¤±è´¥æ¬¡æ•°
    max_consecutive_failures = 3  # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°ï¼Œè¶…è¿‡åå¿«é€Ÿå¤±è´¥

    while time.time() - wait_start_time < timeout:
        is_ready, status_msg, text_length, element_info = check_element_visibility_and_content(page, selector)

        if is_ready:
            logger.info(f"å†…å®¹å…ƒç´ å·²æ‰¾åˆ°ä¸”å¯è§: {status_msg}")
            consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°

            if strategy == "normal":
                return _handle_normal_strategy_content(
                    page, selector, text_length, timeout_config, wait_start_time, timeout
                )
            # Fastå’ŒThoroughæ¨¡å¼åªè¦å…ƒç´ å¯è§å°±æˆåŠŸ
            return True
        consecutive_failures += 1
        elapsed = time.time() - wait_start_time
        remaining = timeout - elapsed

        # å¦‚æœæ˜¯"å…ƒç´ ä¸å­˜åœ¨"ä¸”è¿ç»­å¤±è´¥å¤šæ¬¡ï¼Œå¯èƒ½æ˜¯å¤–éƒ¨é“¾æ¥ï¼Œå¿«é€Ÿå¤±è´¥
        if _check_consecutive_failures(status_msg, consecutive_failures, max_consecutive_failures):
            return False

        logger.info(
            f"å…ƒç´ çŠ¶æ€: {status_msg}, å·²ç­‰å¾… {elapsed:.1f}s, å‰©ä½™ {remaining:.1f}s, è¿ç»­å¤±è´¥: {consecutive_failures}"
        )

        time.sleep(check_interval)

    elapsed = time.time() - wait_start_time
    logger.warning(f"{strategy}æ¨¡å¼ç­‰å¾…è¶…æ—¶ ({elapsed:.1f}s)ï¼Œå…ƒç´ ä»ä¸å¯è§")
    return False


def _setup_request_blocking(page, patterns):
    """è®¾ç½®è¯·æ±‚æ‹¦æˆªå™¨ï¼Œé˜»æ­¢é»‘åå•URL"""
    if not patterns:
        return

    def handle_route(route):
        request_url = route.request.url
        for pattern in patterns:
            if pattern.match(request_url):
                logger.debug(f"é˜»æ­¢é»‘åå•URL: {request_url}")
                route.abort()
                return
        route.continue_()

    page.route("**/*", handle_route)


def _setup_slow_request_monitoring(page, timeout_config: TimeoutConfig):
    """è®¾ç½®æ…¢è¯·æ±‚ç›‘æ§ï¼Œæ‰“å°è¯·æ±‚æ—¶é—´æ…¢è¯·æ±‚"""
    import threading
    
    slow_requests = {}
    # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„é”æ¥ä¿æŠ¤å…±äº«æ•°æ®
    slow_requests_lock = threading.Lock()
    warned_slow_failed_urls = set()
    warned_slow_response_urls = set()
    warned_lock = threading.Lock()

    # ä½¿ç”¨é…ç½®çš„æ…¢è¯·æ±‚é˜ˆå€¼
    slow_threshold = timeout_config.slow_request_threshold
    logger.info(f"å¯ç”¨è¯·æ±‚ç›‘æ§ï¼Œæ…¢è¯·æ±‚é˜ˆå€¼: {slow_threshold:.1f}ç§’")

    def on_request(request):
        with slow_requests_lock:
            slow_requests[request.url] = time.time()

    def on_response(response):
        request_url = response.url
        duration = None
        
        with slow_requests_lock:
            if request_url in slow_requests:
                duration = time.time() - slow_requests[request_url]
                del slow_requests[request_url]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è­¦å‘Šï¼ˆåœ¨é”å¤–è¿›è¡Œï¼Œé¿å…æ­»é”ï¼‰
        if duration is not None and duration > slow_threshold:
            with warned_lock:
                if request_url not in warned_slow_response_urls:
                    logger.warning(f"â° è¯·æ±‚è¿‡ä¹… ({duration:.1f}s > {slow_threshold:.1f}s): {request_url}")
                    warned_slow_response_urls.add(request_url)

    def on_request_failed(request):
        request_url = request.url
        duration = None
        
        with slow_requests_lock:
            if request_url in slow_requests:
                duration = time.time() - slow_requests[request_url]
                del slow_requests[request_url]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è­¦å‘Šï¼ˆåœ¨é”å¤–è¿›è¡Œï¼Œé¿å…æ­»é”ï¼‰
        if duration is not None and duration > slow_threshold:
            with warned_lock:
                if request_url not in warned_slow_failed_urls:
                    logger.warning(f"â° è¯·æ±‚å¤±è´¥å‰è€—æ—¶è¿‡ä¹… ({duration:.1f}s > {slow_threshold:.1f}s): {request_url}")
                    warned_slow_failed_urls.add(request_url)

    page.on("request", on_request)
    page.on("response", on_response)
    page.on("requestfailed", on_request_failed)

    return slow_requests


def _apply_fast_load_strategy(page, content_selector, timeout_config):
    """åº”ç”¨å¿«é€ŸåŠ è½½ç­–ç•¥"""
    logger.info("å¿«é€ŸåŠ è½½æ¨¡å¼ï¼šè·³è¿‡ç½‘ç»œç©ºé—²ç­‰å¾…ï¼Œä½†æŒç»­ç­‰å¾…å…ƒç´ å¯è§")
    return wait_for_element_visible(page, content_selector, timeout_config, "fast")


def _apply_thorough_load_strategy(page, content_selector, timeout_config, slow_requests):
    """åº”ç”¨å½»åº•åŠ è½½ç­–ç•¥"""
    logger.info("å½»åº•åŠ è½½æ¨¡å¼ï¼šç­‰å¾…å®Œå…¨çš„ç½‘ç»œç©ºé—²ï¼Œç„¶åæŒç»­ç­‰å¾…å…ƒç´ å¯è§")

    # é¦–å…ˆç­‰å¾…ç½‘ç»œç©ºé—²
    try:
        page.wait_for_load_state("networkidle", timeout=timeout_config.base_timeout * 1000)
        logger.info("ç½‘ç»œå·²è¾¾åˆ°ç©ºé—²çŠ¶æ€")
    except PlaywrightTimeoutError:
        logger.warning("ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…å…ƒç´ å¯è§")
        # åœ¨thoroughæ¨¡å¼ä¸‹ï¼Œæ‰“å°è¿˜åœ¨åŠ è½½çš„æ…¢è¯·æ±‚
        _log_ongoing_slow_requests(slow_requests)

    # ç„¶åç­‰å¾…å…ƒç´ å¯è§ï¼ˆä½¿ç”¨å‰©ä½™æ—¶é—´ï¼‰
    remaining_timeout = max(timeout_config.base_timeout // 2, timeout_config.thorough_min_timeout)
    timeout_config_remaining = TimeoutConfig(remaining_timeout)
    return wait_for_element_visible(page, content_selector, timeout_config_remaining, "thorough")


def _apply_normal_load_strategy(page, content_selector, timeout_config):
    """åº”ç”¨æ­£å¸¸åŠ è½½ç­–ç•¥"""
    return wait_for_element_visible(page, content_selector, timeout_config, "normal")


def _log_ongoing_slow_requests(slow_requests):
    """è®°å½•æ­£åœ¨è¿›è¡Œçš„æ…¢è¯·æ±‚"""
    if not slow_requests:
        return

    current_time = time.time()
    ongoing_requests = []
    for req_url, start_time in slow_requests.items():
        duration = current_time - start_time
        ongoing_requests.append((req_url, duration))

    if ongoing_requests:
        # æŒ‰æŒç»­æ—¶é—´æ’åºï¼Œæ˜¾ç¤ºæœ€æ…¢çš„å‰5ä¸ª
        ongoing_requests.sort(key=lambda x: x[1], reverse=True)
        logger.warning(f"ä»æœ‰ {len(ongoing_requests)} ä¸ªè¯·æ±‚æœªå®Œæˆ:")
        for req_url, duration in ongoing_requests[:5]:
            logger.warning(f"  - {duration:.1f}s: {req_url}")


def _apply_load_strategy(page, content_selector, timeout_config, load_strategy, slow_requests):
    """åº”ç”¨ç‰¹å®šçš„åŠ è½½ç­–ç•¥"""
    if load_strategy == "fast":
        return _apply_fast_load_strategy(page, content_selector, timeout_config)
    if load_strategy == "thorough":
        return _apply_thorough_load_strategy(page, content_selector, timeout_config, slow_requests)
    # normal strategy (æ™ºèƒ½ç­‰å¾…)
    return _apply_normal_load_strategy(page, content_selector, timeout_config)


def _perform_single_load_attempt(
    page, url, content_selector, timeout_config, load_strategy, verbose_mode, slow_requests, attempt, max_retries
):
    """æ‰§è¡Œå•æ¬¡é¡µé¢åŠ è½½å°è¯•"""
    logger.info(f"å°è¯•åŠ è½½é¡µé¢ ({attempt+1}/{max_retries}): {url}")

    if verbose_mode:
        logger.info("å¯è§†åŒ–æ¨¡å¼ï¼šç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½...")

    # å…ˆå°è¯•å¿«é€ŸåŠ è½½åˆ° domcontentloaded çŠ¶æ€
    page.goto(url, wait_until="domcontentloaded", timeout=timeout_config.initial_load_timeout)
    logger.info("é¡µé¢DOMå·²åŠ è½½å®Œæˆ")

    if verbose_mode:
        # åœ¨é¡µé¢æ ‡é¢˜ä¸­æ˜¾ç¤ºå¤„ç†çŠ¶æ€
        try:
            page.evaluate(
                """() => {
                document.title = "[æ£€æŸ¥å†…å®¹...] " + (document.title || "é¡µé¢");
            }"""
            )
        except:
            pass

    # åº”ç”¨åŠ è½½ç­–ç•¥
    if _apply_load_strategy(page, content_selector, timeout_config, load_strategy, slow_requests):
        return page.url  # è¿”å›æœ€ç»ˆURL
    if attempt < max_retries - 1:
        time.sleep(timeout_config.element_check_interval)

    return None


def _handle_load_retry(attempt, max_retries, timeout_config, error):
    """å¤„ç†åŠ è½½é‡è¯•é€»è¾‘"""
    if attempt == max_retries - 1:
        logger.error("æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µé¢")
        raise error

    # æŒ‡æ•°é€€é¿é‡è¯•
    wait_time = min(2**attempt, timeout_config.retry_backoff_max)
    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
    time.sleep(wait_time)


def _handle_page_loading_with_retries(
    page, url, content_selector, timeout_config, max_retries, verbose_mode, load_strategy, url_blacklist_patterns=None
):
    """å¤„ç†é¡µé¢åŠ è½½å’Œé‡è¯•é€»è¾‘"""
    # è®¾ç½®è¯·æ±‚æ‹¦æˆª
    _setup_request_blocking(page, url_blacklist_patterns)

    # ä¸ºæ‰€æœ‰ç­–ç•¥å¯ç”¨æ…¢è¯·æ±‚ç›‘æ§
    slow_requests = _setup_slow_request_monitoring(page, timeout_config)

    for attempt in range(max_retries):
        try:
            result = _perform_single_load_attempt(
                page,
                url,
                content_selector,
                timeout_config,
                load_strategy,
                verbose_mode,
                slow_requests,
                attempt,
                max_retries,
            )

            if result:
                return result

        except PlaywrightTimeoutError as timeout_err:
            if "Timeout" in str(timeout_err) and "goto" in str(timeout_err):
                logger.warning(f"ç¬¬ {attempt+1} æ¬¡é¡µé¢åŠ è½½è¶…æ—¶: {timeout_err}")
            else:
                logger.warning(f"ç¬¬ {attempt+1} æ¬¡æ“ä½œè¶…æ—¶: {timeout_err}")

            _handle_load_retry(attempt, max_retries, timeout_config, timeout_err)

        except Exception as e:
            logger.warning(f"ç¬¬ {attempt+1} æ¬¡é¡µé¢åŠ è½½å¼‚å¸¸: {e!s}ï¼Œé‡è¯•ä¸­...")
            _handle_load_retry(attempt, max_retries, timeout_config, e)

    logger.error("æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µé¢")
    raise Exception("æ‰€æœ‰é‡è¯•å‡å¤±è´¥")


def _extract_page_links(page, toc_selectors, final_url, base_url):
    """æå–é¡µé¢ä¸­çš„å¯¼èˆªé“¾æ¥ï¼Œæ”¯æŒå¤šä¸ªç›®å½•é€‰æ‹©å™¨"""
    all_links = []
    
    # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    if isinstance(toc_selectors, str):
        toc_selectors = [toc_selectors]
    
    logger.info(f"å¼€å§‹æå–å¯¼èˆªé“¾æ¥ï¼Œå°è¯• {len(toc_selectors)} ä¸ªç›®å½•é€‰æ‹©å™¨")
    
    for i, toc_selector in enumerate(toc_selectors, 1):
        try:
            logger.info(f"å°è¯•ç›®å½•é€‰æ‹©å™¨ {i}/{len(toc_selectors)}: {toc_selector}")
            resolved_toc = resolve_selector(toc_selector)

            toc_element = page.query_selector(resolved_toc)
            if not toc_element:
                logger.debug(f"ç›®å½•é€‰æ‹©å™¨ {i} æœªæ‰¾åˆ°å…ƒç´ : {resolved_toc}")
                continue

            links_from_selector = []
            
            # æ£€æŸ¥é€‰ä¸­çš„å…ƒç´ æœ¬èº«æ˜¯å¦æ˜¯ a æ ‡ç­¾
            if toc_element.tag_name.lower() == 'a':
                href = toc_element.get_attribute("href")
                if href and href.strip():
                    abs_url = urljoin(final_url, href.strip())
                    norm_url = normalize_url(abs_url, base_url)
                    links_from_selector.append(norm_url)
                    logger.info(f"ç›®å½•é€‰æ‹©å™¨ {i} æœ¬èº«æ˜¯ a æ ‡ç­¾ï¼Œæå–åˆ° 1 ä¸ªé“¾æ¥")
            else:
                # åœ¨é€‰ä¸­çš„å…ƒç´ å†…æŸ¥æ‰¾ a æ ‡ç­¾
                a_elements = toc_element.query_selector_all("a")
                logger.info(f"ç›®å½•é€‰æ‹©å™¨ {i} æ‰¾åˆ° {len(a_elements)} ä¸ªé“¾æ¥å…ƒç´ ")

                for a in a_elements:
                    href = a.get_attribute("href")
                    if href and href.strip():
                        abs_url = urljoin(final_url, href.strip())
                        norm_url = normalize_url(abs_url, base_url)
                        links_from_selector.append(norm_url)

            unique_links_from_selector = list(set(links_from_selector))
            logger.info(f"ç›®å½•é€‰æ‹©å™¨ {i} æå–åˆ° {len(unique_links_from_selector)} ä¸ªå”¯ä¸€é“¾æ¥")
            all_links.extend(unique_links_from_selector)

        except Exception as e:
            logger.warning(f"ç›®å½•é€‰æ‹©å™¨ {i} æå–é“¾æ¥å¤±è´¥: {e}")
            continue

    # å»é‡æ‰€æœ‰é“¾æ¥
    unique_links = list(set(all_links))
    logger.info(f"æ€»å…±ä»æ‰€æœ‰ç›®å½•é€‰æ‹©å™¨æå–åˆ° {len(unique_links)} ä¸ªå”¯ä¸€é“¾æ¥")

    return unique_links


def _clean_page_content(page, content_element, verbose_mode, timeout_config):
    """æ¸…ç†é¡µé¢å†…å®¹ï¼Œä¿ç•™ä¸»è¦å†…å®¹"""
    logger.info("æ¸…ç†é¡µé¢å¹¶ä¿ç•™ä¸»è¦å†…å®¹...")

    # ä¿å­˜åŸå§‹å†…å®¹ç”¨äºå¯¹æ¯”
    original_content = page.evaluate(
        """(element) => {
        return {
            textLength: element.textContent ? element.textContent.trim().length : 0,
            innerHTML: element.innerHTML.substring(0, 200) + '...'
        };
    }""",
        content_element,
    )
    logger.info(f"æ¸…ç†å‰å†…å®¹é¢„è§ˆ: æ–‡æœ¬é•¿åº¦={original_content['textLength']}, HTMLç‰‡æ®µ={original_content['innerHTML']}")

    if verbose_mode:
        page.evaluate(
            r"""() => {
            document.title = "[æ¸…ç†é¡µé¢...] " + document.title.replace(/^\[.*?\] /, "");
        }"""
        )
        # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹ï¼Œç¨å¾®å»¶è¿Ÿä¸€ä¸‹è®©ç”¨æˆ·çœ‹åˆ°åŸå§‹é¡µé¢
        time.sleep(timeout_config.element_check_interval)

    # æ–°çš„æ¸…ç†é€»è¾‘ï¼šé€çº§å‘ä¸Šæ¸…ç†DOM
    page.evaluate(
        """(element) => {
        // ä»å†…å®¹å…ƒç´ å¼€å§‹å‘ä¸Šæ¸…ç†
        let current = element;
        
        // å‘ä¸Šéå†ç›´åˆ°bodyå…ƒç´ 
        while (current && current !== document.body) {
            const parent = current.parentElement;
            if (!parent) break;
            
            // åˆ é™¤æ‰€æœ‰éå½“å‰å…ƒç´ çš„å…„å¼ŸèŠ‚ç‚¹
            for (let i = parent.children.length - 1; i >= 0; i--) {
                const child = parent.children[i];
                if (child !== current) {
                    child.remove();
                }
            }
            
            // ç§»åŠ¨åˆ°çˆ¶çº§å…ƒç´ 
            current = parent;
        }
        
        // æ¸…ç†bodyå…ƒç´ 
        if (current === document.body) {
            // ç§»é™¤æ‰€æœ‰è„šæœ¬
            document.querySelectorAll('script').forEach(s => s.remove());
            
            // è®¾ç½®bodyæ ·å¼
            document.body.style.margin = '0';
            document.body.style.padding = '0';
            
            // ç¡®ä¿å†…å®¹å…ƒç´ å®½åº¦100%
            element.style.width = '100%';
            element.style.boxSizing = 'border-box';
            element.style.padding = '20px';
        }
    }""",
        content_element,
    )

    # æ£€æŸ¥æ¸…ç†åçš„å†…å®¹
    after_cleanup = page.evaluate(
        """(element) => {
        const rect = element.getBoundingClientRect();
        return {
            textLength: element.textContent ? element.textContent.trim().length : 0,
            hasVisibleContent: rect.width > 0 && rect.height > 0,
            width: rect.width,
            height: rect.height,
            innerHTML: element.innerHTML.substring(0, 200) + '...'
        };
    }""",
        content_element,
    )
    logger.info(
        f"æ¸…ç†åå†…å®¹æ£€æŸ¥: æ–‡æœ¬é•¿åº¦={after_cleanup['textLength']}, å¯è§={after_cleanup['hasVisibleContent']}, å°ºå¯¸={after_cleanup['width']}x{after_cleanup['height']}"
    )

    # å¦‚æœæ¸…ç†åå†…å®¹æ˜æ˜¾å‡å°‘ï¼Œå‘å‡ºè­¦å‘Š
    if after_cleanup["textLength"] < original_content["textLength"] * 0.8:
        logger.warning(
            f"è­¦å‘Šï¼šæ¸…ç†åå†…å®¹å¤§å¹…å‡å°‘ï¼åŸå§‹: {original_content['textLength']} -> æ¸…ç†å: {after_cleanup['textLength']}"
        )


def _save_debug_screenshot(page, url, debug_dir):
    """ä¿å­˜è°ƒè¯•æˆªå›¾"""
    debug_path = Path(debug_dir)
    debug_path.mkdir(exist_ok=True)

    # æ¸…ç†URLä½œä¸ºæ–‡ä»¶å
    safe_url = re.sub(r"[^\w\-_\.]", "_", url.replace("https://", "").replace("http://", ""))[:50]
    screenshot_path = debug_path / f"{safe_url}_after_cleanup.png"

    try:
        page.screenshot(path=str(screenshot_path), full_page=True)
        logger.info(f"è°ƒè¯•æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
    except Exception as screenshot_err:
        logger.warning(f"ä¿å­˜æˆªå›¾å¤±è´¥: {screenshot_err}")


def _prepare_page_for_pdf(page, content_selector, verbose_mode, timeout_config, debug_mode, debug_dir, url):
    """å‡†å¤‡é¡µé¢å†…å®¹ç”¨äºPDFç”Ÿæˆ"""
    content_element = page.query_selector(resolve_selector(content_selector))
    if not content_element:
        logger.error(f"é¡µé¢ä¸­æœªæ‰¾åˆ°å†…å®¹èŠ‚ç‚¹: {content_selector}")
        return False

    if verbose_mode:
        page.evaluate(
            r"""() => {
            document.title = "[åˆ†æå†…å®¹...] " + document.title.replace(/^\[.*?\] /, "");
        }"""
        )

    logger.info("åˆ†æå†…å®¹å…ƒç´ ...")

    # ä½¿ç”¨ç»Ÿä¸€çš„å…ƒç´ æ£€æŸ¥å‡½æ•°
    is_ready, status_msg, text_length, element_info = check_element_visibility_and_content(page, content_selector)

    if not is_ready:
        logger.error(f"å†…å®¹å…ƒç´ ä¸å¯è§ï¼Œè·³è¿‡PDFç”Ÿæˆï¼{status_msg}")
        return False

    logger.info(f"å†…å®¹å…ƒç´ ä¿¡æ¯: {element_info}")

    # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­ï¼ˆå¯èƒ½æ˜¯åŠ¨æ€åŠ è½½ï¼‰
    if text_length == 0:
        logger.warning("è­¦å‘Šï¼šå†…å®¹å…ƒç´ æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼å¯èƒ½æ˜¯åŠ¨æ€åŠ è½½æˆ–ç©ºé¡µé¢")
    elif text_length < 50:
        logger.warning(f"è­¦å‘Šï¼šå†…å®¹å…ƒç´ æ–‡æœ¬å¾ˆå°‘ ({text_length} å­—ç¬¦)ï¼")

    # æ¸…ç†é¡µé¢å†…å®¹
    _clean_page_content(page, content_element, verbose_mode, timeout_config)

    # è°ƒè¯•æ¨¡å¼ï¼šä¿å­˜æˆªå›¾
    if debug_mode and debug_dir:
        _save_debug_screenshot(page, url, debug_dir)

    return True


def _generate_pdf_from_page(page, verbose_mode, timeout_config, temp_dir: str, url: str):
    """ä»é¡µé¢ç”ŸæˆPDF"""
    logger.info("ç­‰å¾…é¡µé¢æ¸²æŸ“...")
    if verbose_mode:
        page.evaluate(
            r"""() => {
            document.title = "[å‡†å¤‡ç”ŸæˆPDF...] " + document.title.replace(/^\[.*?\] /, "");
        }"""
        )
        time.sleep(timeout_config.element_check_interval)  # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹ç»™ç”¨æˆ·æ›´å¤šæ—¶é—´è§‚å¯Ÿ

    time.sleep(timeout_config.page_render_wait)  # ä½¿ç”¨é…ç½®çš„é¡µé¢æ¸²æŸ“ç­‰å¾…æ—¶é—´

    # åœ¨ç”ŸæˆPDFå‰åšæœ€åçš„å†…å®¹æ£€æŸ¥
    final_check = page.evaluate(
        """() => {
        const body = document.body;
        const rect = body.getBoundingClientRect();
        return {
            bodyTextLength: body.textContent ? body.textContent.trim().length : 0,
            bodyHeight: rect.height,
            visibleElements: Array.from(document.querySelectorAll('*')).filter(el => {
                const style = window.getComputedStyle(el);
                const rect = el.getBoundingClientRect();
                return style.display !== 'none' && 
                       style.visibility !== 'hidden' && 
                       rect.width > 0 && 
                       rect.height > 0;
            }).length,
            hasImages: document.querySelectorAll('img').length,
            hasTables: document.querySelectorAll('table').length
        };
    }"""
    )
    logger.info(f"PDFç”Ÿæˆå‰æœ€ç»ˆæ£€æŸ¥: {final_check}")

    if final_check["bodyTextLength"] == 0:
        logger.error("ä¸¥é‡è­¦å‘Šï¼šé¡µé¢å†…å®¹ä¸ºç©ºï¼Œå°†ç”Ÿæˆç©ºç™½PDFï¼")
    elif final_check["bodyTextLength"] < 50:
        logger.warning(f"è­¦å‘Šï¼šé¡µé¢å†…å®¹å¾ˆå°‘ ({final_check['bodyTextLength']} å­—ç¬¦)ï¼Œå¯èƒ½ç”Ÿæˆè¿‘ä¼¼ç©ºç™½çš„PDF")

    # ä½¿ç”¨æŒä¹…åŒ–çš„æ–‡ä»¶å
    filename = url_to_filename(url)
    temp_file = Path(temp_dir) / filename
    logger.info(f"ç”ŸæˆPDF: {temp_file}")

    try:
        page.pdf(
            path=str(temp_file),
            format="A4",
            margin={"top": "1cm", "right": "1cm", "bottom": "1cm", "left": "1cm"},
            scale=0.99,
        )

        # æ£€æŸ¥ç”Ÿæˆçš„PDFæ–‡ä»¶å¤§å°
        if temp_file.exists():
            file_size = temp_file.stat().st_size
            logger.info(f"PDFæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼Œå¤§å°: {file_size} å­—èŠ‚")
            if file_size < timeout_config.min_pdf_size:  # ä½¿ç”¨é…ç½®çš„æœ€å°PDFå¤§å°
                logger.warning(f"è­¦å‘Šï¼šPDFæ–‡ä»¶å¾ˆå° ({file_size} å­—èŠ‚)ï¼Œå¯èƒ½æ˜¯ç©ºç™½é¡µé¢")

        return temp_file
    except Exception as pdf_err:
        logger.error(f"ç”ŸæˆPDFå¤±è´¥: {pdf_err}")
        return None


def _check_existing_pdf(temp_dir, url):
    """æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨PDFæ–‡ä»¶"""
    if not temp_dir:
        return None

    expected_pdf = Path(temp_dir) / url_to_filename(url)
    if expected_pdf.exists() and expected_pdf.stat().st_size > 1000:
        logger.info(f"å‘ç°å·²å­˜åœ¨çš„PDFæ–‡ä»¶ï¼Œè·³è¿‡å¤„ç†: {url}")
        return expected_pdf
    return None


def _handle_page_loading(
    page, url, content_selector, timeout_config, max_retries, verbose_mode, load_strategy, url_blacklist_patterns
):
    """å¤„ç†é¡µé¢åŠ è½½é€»è¾‘"""
    try:
        return _handle_page_loading_with_retries(
            page,
            url,
            content_selector,
            timeout_config,
            max_retries,
            verbose_mode,
            load_strategy,
            url_blacklist_patterns,
        )
    except Exception as e:
        raise Exception(f"é¡µé¢åŠ è½½å¤±è´¥: {e!s}")


def _generate_pdf_with_validation(
    page, content_selector, verbose_mode, timeout_config, debug_mode, debug_dir, temp_dir, url
):
    """ç”ŸæˆPDFå¹¶è¿›è¡ŒéªŒè¯"""
    if not temp_dir:
        raise ValueError("temp_dirå‚æ•°æ˜¯å¿…éœ€çš„")

    # å‡†å¤‡é¡µé¢å†…å®¹ç”¨äºPDFç”Ÿæˆ
    if not _prepare_page_for_pdf(page, content_selector, verbose_mode, timeout_config, debug_mode, debug_dir, url):
        raise Exception("å†…å®¹å…ƒç´ ä¸å¯è§æˆ–ä¸å­˜åœ¨")

    # ç”ŸæˆPDF
    pdf_path = _generate_pdf_from_page(page, verbose_mode, timeout_config, temp_dir, url)
    if not pdf_path:
        raise Exception("PDFç”Ÿæˆå¤±è´¥")

    return pdf_path


def process_page_with_failure_tracking(
    page,
    url,
    content_selector,
    toc_selectors,
    base_url,
    timeout_config: TimeoutConfig,
    max_retries,
    debug_mode=False,
    debug_dir=None,
    verbose_mode=False,
    load_strategy="normal",
    url_blacklist_patterns=None,
    temp_dir=None,
):
    """å¤„ç†å•ä¸ªé¡µé¢å¹¶ç”ŸæˆPDFï¼ŒåŒæ—¶æå–è¯¥é¡µé¢å†…çš„é“¾æ¥ï¼ŒåŒ…å«å¤±è´¥è·Ÿè¸ª"""
    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªURLï¼ˆæ ¹æ®PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
    existing_pdf = _check_existing_pdf(temp_dir, url)
    if existing_pdf:
        # ä»ç„¶éœ€è¦æå–é“¾æ¥ï¼Œæ‰€ä»¥ç»§ç»­å¤„ç†ï¼Œä½†è·³è¿‡PDFç”Ÿæˆ
        pass

    pdf_path = None
    links = []
    final_url = url
    failure_reason = None

    try:
        logger.info(f"å‡†å¤‡å¤„ç†é¡µé¢: {url}")

        # å¤„ç†é¡µé¢åŠ è½½å’Œé‡è¯•é€»è¾‘
        try:
            final_url = _handle_page_loading(
                page,
                url,
                content_selector,
                timeout_config,
                max_retries,
                verbose_mode,
                load_strategy,
                url_blacklist_patterns,
            )
        except Exception as e:
            failure_reason = str(e)
            logger.warning(f"é¡µé¢åŠ è½½å¤±è´¥ï¼Œå°†è®°å½•ä¸ºå¾…é‡è¯•: {url} - {failure_reason}")
            return None, [], url, failure_reason

        if final_url != url:
            logger.info(f"é‡å®šå‘: {url} -> {final_url}")

        # æå–é¡µé¢é“¾æ¥
        links = _extract_page_links(page, toc_selectors, final_url, base_url)

        # å¦‚æœPDFå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if existing_pdf:
            return existing_pdf, links, final_url, None

        # ç”ŸæˆPDF
        try:
            pdf_path = _generate_pdf_with_validation(
                page,
                content_selector,
                verbose_mode,
                timeout_config,
                debug_mode,
                debug_dir,
                temp_dir,
                url,
            )
            return pdf_path, links, final_url, None
        except Exception as e:
            failure_reason = str(e)
            logger.warning(f"PDFç”Ÿæˆå¤±è´¥ï¼Œå°†è®°å½•ä¸ºå¾…é‡è¯•: {url} - {failure_reason}")
            return None, links, final_url, failure_reason

    except Exception as e:
        failure_reason = f"å¤„ç†é¡µé¢å¼‚å¸¸: {e!s}"
        logger.error(f"å¤„ç†é¡µé¢å¤±è´¥: {url}\né”™è¯¯: {e!s}", exc_info=True)
        return None, links, final_url, failure_reason


def process_page(
    page,
    url,
    content_selector,
    toc_selectors,
    base_url,
    timeout_config: TimeoutConfig,
    max_retries,
    debug_mode=False,
    debug_dir=None,
    verbose_mode=False,
    load_strategy="normal",
    url_blacklist_patterns=None,
    temp_dir=None,
):
    """å¤„ç†å•ä¸ªé¡µé¢å¹¶ç”ŸæˆPDFï¼ŒåŒæ—¶æå–è¯¥é¡µé¢å†…çš„é“¾æ¥"""
    pdf_path, links, final_url, _ = process_page_with_failure_tracking(
        page,
        url,
        content_selector,
        toc_selectors,
        base_url,
        timeout_config,
        max_retries,
        debug_mode,
        debug_dir,
        verbose_mode,
        load_strategy,
        url_blacklist_patterns,
        temp_dir,
    )
    return pdf_path, links, final_url


def get_parent_path_pattern(base_url):
    """è·å–base_urlçš„çˆ¶ç›®å½•ä½œä¸ºé»˜è®¤URLåŒ¹é…æ¨¡å¼"""
    parsed = urlparse(base_url)
    path = parsed.path.rstrip("/")

    # å¦‚æœè·¯å¾„ä¸ºç©ºæˆ–è€…æ˜¯æ ¹è·¯å¾„ï¼Œä½¿ç”¨åŸŸå
    if not path or path == "/":
        return f"https?://{re.escape(parsed.netloc)}/.*"

    # è·å–çˆ¶ç›®å½•è·¯å¾„
    parent_path = "/".join(path.split("/")[:-1])
    if not parent_path:
        parent_path = ""

    return f"https?://{re.escape(parsed.netloc)}{re.escape(parent_path)}/.*"


def compile_blacklist_patterns(blacklist_args):
    """ç¼–è¯‘URLé»‘åå•æ¨¡å¼"""
    if not blacklist_args:
        return []

    patterns = []
    for pattern_str in blacklist_args:
        try:
            pattern = re.compile(pattern_str, re.IGNORECASE)
            patterns.append(pattern)
            logger.info(f"æ·»åŠ URLé»‘åå•æ¨¡å¼: {pattern_str}")
        except re.error as e:
            logger.warning(f"æ— æ•ˆçš„URLé»‘åå•æ¨¡å¼ '{pattern_str}': {e}")

    return patterns


def _initialize_or_resume_progress(base_url_normalized, output_file, max_depth, cache_dir, use_cache=True):
    """åˆå§‹åŒ–æ–°çš„è¿›åº¦çŠ¶æ€æˆ–ä»æ–‡ä»¶æ¢å¤è¿›åº¦çŠ¶æ€"""
    progress_file_path = create_progress_file_path(cache_dir, base_url_normalized)
    progress_file = Path(progress_file_path)

    if use_cache and progress_file.exists():
        logger.info(f"å‘ç°è¿›åº¦æ–‡ä»¶: {progress_file}")
        try:
            progress_state = ProgressState.load_from_file(str(progress_file))
            logger.info("æˆåŠŸæ¢å¤è¿›åº¦çŠ¶æ€:")
            logger.info(f"  - å·²è®¿é—®URL: {len(progress_state.visited_urls)} ä¸ª")
            logger.info(f"  - é˜Ÿåˆ—ä¸­URL: {len(progress_state.queue)} ä¸ª")
            logger.info(f"  - å·²ç”ŸæˆPDF: {len(progress_state.pdf_files)} ä¸ª")
            logger.info(f"  - å¤±è´¥URL: {len(progress_state.failed_urls)} ä¸ª")
            logger.info(f"  - ä¸´æ—¶ç›®å½•: {progress_state.temp_dir}")
            return progress_state, True
        except Exception as e:
            logger.warning(f"æ¢å¤è¿›åº¦çŠ¶æ€å¤±è´¥: {e}")
            logger.info("å°†åˆ›å»ºæ–°çš„è¿›åº¦çŠ¶æ€")

    # åˆ›å»ºæ–°çš„è¿›åº¦çŠ¶æ€
    progress_state = ProgressState(
        base_url=base_url_normalized,
        output_pdf=output_file,
        temp_dir=str(cache_dir),  # ä½¿ç”¨ç¼“å­˜ç›®å½•ä½œä¸ºä¸´æ—¶ç›®å½•
        progress_file=str(progress_file),
        visited_urls=set(),
        failed_urls=[],
        processed_urls=[],
        pdf_files=[],
        queue=deque(),
        enqueued=set(),
    )

    # åˆå§‹åŒ–é˜Ÿåˆ—
    progress_state.queue.append((base_url_normalized, 0))
    progress_state.enqueued.add(base_url_normalized)

    logger.info("åˆ›å»ºæ–°çš„è¿›åº¦çŠ¶æ€")
    return progress_state, False


def _crawl_pages_with_progress(
    context,
    args,
    base_url_normalized,
    url_pattern,
    url_blacklist_patterns,
    timeout_config,
    progress_state: ProgressState,
    domain_failure_tracker,
):
    """æ‰§è¡Œé¡µé¢çˆ¬å–é€»è¾‘ï¼Œæ”¯æŒè¿›åº¦æ¢å¤å’Œæµæ°´çº¿å¹¶è¡Œå¤„ç†"""
    logger.info(f"å¼€å§‹/ç»§ç»­çˆ¬å–ï¼Œæœ€å¤§æ·±åº¦: {args.max_depth}")

    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨ï¼ˆä½¿ç”¨å·²è®¾ç½®çš„ç¼“å­˜ç›®å½•ï¼‰
    if progress_state.temp_dir and not os.path.exists(progress_state.temp_dir):
        os.makedirs(progress_state.temp_dir, exist_ok=True)
        logger.info(f"ä½¿ç”¨ç¼“å­˜ç›®å½•: {progress_state.temp_dir}")

    # æ ¹æ®å¹¶è¡Œé¡µé¢æ•°é‡é€‰æ‹©å¤„ç†æ–¹å¼
    if args.parallel_pages > 1:
        return _crawl_pages_parallel(
            context,
            args,
            base_url_normalized,
            url_pattern,
            url_blacklist_patterns,
            timeout_config,
            progress_state,
            domain_failure_tracker,
        )
    return _crawl_pages_serial(
        context,
        args,
        base_url_normalized,
        url_pattern,
        url_blacklist_patterns,
        timeout_config,
        progress_state,
        domain_failure_tracker,
    )


def _crawl_pages_serial(
    context,
    args,
    base_url_normalized,
    url_pattern,
    url_blacklist_patterns,
    timeout_config,
    progress_state: ProgressState,
    domain_failure_tracker,
):
    """ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼ˆå…¼å®¹åŸæœ‰é€»è¾‘ï¼‰"""
    logger.info("å¯ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼ï¼Œåˆ›å»ºæŒä¹…é¡µé¢ç”¨äºé‡ç”¨")

    # åˆ›å»ºä¸€ä¸ªæŒä¹…çš„é¡µé¢ï¼Œé‡ç”¨ä»¥æé«˜æ€§èƒ½
    page = context.new_page()

    try:
        processed_count = len(progress_state.visited_urls)  # å·²å¤„ç†çš„URLæ•°é‡

        while progress_state.queue:
            url, depth = progress_state.queue.popleft()
            processed_count += 1

            # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            total_discovered = len(progress_state.enqueued)
            progress_info = f"è¿›åº¦: [{processed_count}/{total_discovered}]"
            if len(progress_state.queue) > 0:
                progress_info += f" (é˜Ÿåˆ—ä¸­è¿˜æœ‰ {len(progress_state.queue)} ä¸ª)"

            logger.info(f"{progress_info} å¤„ç†: {url} (æ·±åº¦: {depth})")

            if depth > args.max_depth:
                logger.warning(f"è¶…è¿‡æœ€å¤§æ·±åº¦é™åˆ¶({args.max_depth})ï¼Œè·³è¿‡: {url}")
                continue

            if url in progress_state.visited_urls:
                logger.info(f"å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {url}")
                continue

            try:
                pdf_path, links, final_url, failure_reason = process_page_with_failure_tracking(
                    page,  # ä¼ é€’é¡µé¢
                    url,
                    args.content_selector,
                    args.toc_selector,
                    base_url_normalized,
                    timeout_config,  # ä¼ é€’è¶…æ—¶é…ç½®å¯¹è±¡
                    args.max_retries,
                    args.debug,
                    args.debug_dir,
                    args.verbose,
                    args.load_strategy,
                    url_blacklist_patterns,  # ä¼ é€’URLé»‘åå•æ¨¡å¼
                    progress_state.temp_dir,  # ä¼ é€’ä¸´æ—¶ç›®å½•
                )

                _handle_page_result(
                    progress_state,
                    url,
                    final_url,
                    pdf_path,
                    links,
                    failure_reason,
                    url_pattern,
                    base_url_normalized,
                    depth,
                    args.max_depth,
                )

            except Exception as e:
                logger.exception(f"å¤„ç† {url} æ—¶å‘ç”Ÿé”™è¯¯")
                progress_state.failed_urls.append((url, f"å¼‚å¸¸é”™è¯¯: {e!s}"))
                progress_state.visited_urls.add(url)

            # æ¯å¤„ç†ä¸€ä¸ªURLå°±ä¿å­˜è¿›åº¦
            progress_state.save_to_file()

    finally:
        # ç¡®ä¿é¡µé¢è¢«æ­£ç¡®å…³é—­
        try:
            page.close()
            logger.info("å·²å…³é—­é‡ç”¨çš„é¡µé¢")
        except Exception as close_err:
            logger.warning(f"å…³é—­é‡ç”¨é¡µé¢æ—¶å‡ºé”™: {close_err!s}")

    # æœ€ç»ˆç»Ÿè®¡
    success_count = len(progress_state.processed_urls)
    failed_count = len(progress_state.failed_urls)
    total_processed = success_count + failed_count

    if total_processed > 0:
        logger.info("\nğŸ“ˆ ä¸²è¡Œå¤„ç†å®Œæˆç»Ÿè®¡:")
        logger.info(f"   æ€»å…±å¤„ç†: {total_processed} ä¸ªURL")
        logger.info(f"   æˆåŠŸ: {success_count} ä¸ª ({success_count/total_processed*100:.1f}%)")
        logger.info(f"   å¤±è´¥: {failed_count} ä¸ª ({failed_count/total_processed*100:.1f}%)")

    return progress_state


def _check_qos_trigger(loading_tasks, qos_failure_tracker):
    """æ£€æŸ¥æ˜¯å¦è§¦å‘QoSç­‰å¾…æ¡ä»¶"""
    # æ£€æŸ¥å½“å‰æ´»è·ƒä»»åŠ¡ä¸­æœ‰å¤šå°‘å·²ç»å¤±è´¥è¿‡
    failed_tasks_in_current_batch = 0

    for task_id in loading_tasks:
        if task_id in qos_failure_tracker:
            failed_tasks_in_current_batch += 1

    # å¦‚æœå½“å‰æ‰¹æ¬¡ä¸­è¶…è¿‡ä¸€åŠçš„ä»»åŠ¡éƒ½å¤±è´¥è¿‡ï¼Œè®¤ä¸ºè§¦å‘äº†æµæ§
    total_active_tasks = len(loading_tasks)
    if total_active_tasks >= 2 and failed_tasks_in_current_batch >= total_active_tasks // 2:
        return True

    return False


def _perform_qos_wait(qos_wait_seconds):
    """æ‰§è¡ŒQoSç­‰å¾…"""
    logger.warning("ğŸš¨ æ£€æµ‹åˆ°å¯èƒ½çš„ç½‘ç«™æµæ§ï¼Œè¿›å…¥QoSç­‰å¾…æ¨¡å¼")
    logger.info(f"â° ç­‰å¾… {qos_wait_seconds} ç§’ï¼ˆ{qos_wait_seconds//60:.1f} åˆ†é’Ÿï¼‰ä»¥é¿å…æµæ§...")

    # åˆ†æ®µæ˜¾ç¤ºç­‰å¾…è¿›åº¦
    wait_interval = min(30, qos_wait_seconds // 10)  # æ¯30ç§’æˆ–æ€»æ—¶é—´çš„1/10æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
    elapsed = 0

    while elapsed < qos_wait_seconds:
        remaining = qos_wait_seconds - elapsed
        if remaining <= wait_interval:
            time.sleep(remaining)
            break
        time.sleep(wait_interval)
        elapsed += wait_interval
        progress_percent = (elapsed / qos_wait_seconds) * 100
        logger.info(f"â° QoSç­‰å¾…è¿›åº¦: {progress_percent:.1f}% ({elapsed}/{qos_wait_seconds}ç§’)")

    logger.info("âœ… QoSç­‰å¾…å®Œæˆï¼Œæ¢å¤æ­£å¸¸å¤„ç†")


def _track_task_failure(task_id, qos_failure_tracker):
    """è®°å½•ä»»åŠ¡å¤±è´¥ï¼Œç”¨äºQoSæ£€æµ‹"""
    qos_failure_tracker.add(task_id)
    logger.debug(f"è®°å½•ä»»åŠ¡ #{task_id} å¤±è´¥ï¼Œå½“å‰å¤±è´¥ä»»åŠ¡æ•°: {len(qos_failure_tracker)}")


def _process_completed_task_with_qos(
    pipeline_pool,
    loading_tasks,
    completed_task_id,
    progress_state,
    args,
    base_url_normalized,
    url_pattern,
    timeout_config,
    qos_failure_tracker,
    domain_failure_tracker,
):
    """å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡ï¼ŒåŒ…å«QoSå¤±è´¥è·Ÿè¸ª"""
    if completed_task_id not in loading_tasks:
        return False

    url, depth = loading_tasks[completed_task_id]
    page, final_url, error = pipeline_pool.get_loaded_page(completed_task_id, timeout=0.1)

    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
    processed_count = len(progress_state.visited_urls) + 1
    total_discovered = len(progress_state.enqueued)
    remaining_in_queue = len(progress_state.queue)
    active_loading = len(loading_tasks) - 1  # å‡å»å½“å‰æ­£åœ¨å¤„ç†çš„
    progress_info = f"æµæ°´çº¿è¿›åº¦: [{processed_count}/{total_discovered}]"
    if remaining_in_queue > 0 or active_loading > 0:
        progress_info += f" (é˜Ÿåˆ—: {remaining_in_queue}, é¢„åŠ è½½ä¸­: {active_loading})"

    logger.info(f"{progress_info} å¤„ç†: {url} (æ·±åº¦: {depth})")

    task_failed = False

    if depth > args.max_depth:
        logger.warning(f"è¶…è¿‡æœ€å¤§æ·±åº¦é™åˆ¶({args.max_depth})ï¼Œè·³è¿‡: {url}")
    elif url in progress_state.visited_urls:
        logger.info(f"å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {url}")
    elif page is not None:
        # é¡µé¢åŠ è½½æˆåŠŸï¼Œè¿›è¡Œå†…å®¹å¤„ç†
        try:
            pdf_path, links = _process_loaded_page(
                page,
                url,
                final_url or url,
                args,
                base_url_normalized,
                timeout_config,
                progress_state.temp_dir,
            )

            _handle_page_result(
                progress_state,
                url,
                final_url or url,
                pdf_path,
                links,
                None,
                url_pattern,
                base_url_normalized,
                depth,
                args.max_depth,
            )

        except Exception as e:
            logger.exception(f"å¤„ç†å·²åŠ è½½é¡µé¢ {url} æ—¶å‘ç”Ÿé”™è¯¯")
            progress_state.failed_urls.append((url, f"å¤„ç†å¼‚å¸¸: {e!s}"))
            progress_state.visited_urls.add(url)
            task_failed = True
    else:
        # é¡µé¢åŠ è½½å¤±è´¥
        failure_reason = error or "é¡µé¢åŠ è½½å¤±è´¥"
        logger.warning(f"é¡µé¢åŠ è½½å¤±è´¥: {url} - {failure_reason}")
        progress_state.failed_urls.append((url, failure_reason))
        progress_state.visited_urls.add(url)
        task_failed = True

        # è®°å½•åŸŸåå¤±è´¥ç”¨äºè‡ªåŠ¨é»‘åå•
        added_to_blacklist = domain_failure_tracker.record_failure(url)
        if added_to_blacklist:
            logger.info(
                f"ğŸ”„ è‡ªåŠ¨é»‘åå•å·²æ›´æ–°ï¼Œå½“å‰å…±æœ‰ {len(domain_failure_tracker.auto_blacklist_patterns)} ä¸ªè‡ªåŠ¨é»‘åå•åŸŸå"
            )

    # è®°å½•ä»»åŠ¡å¤±è´¥ç”¨äºQoSæ£€æµ‹
    if task_failed:
        _track_task_failure(completed_task_id, qos_failure_tracker)

    return task_failed


def _start_initial_loading_tasks(pipeline_pool, progress_state, args, timeout_config, url_blacklist_patterns):
    """å¯åŠ¨åˆå§‹é¡µé¢é¢„åŠ è½½ä»»åŠ¡"""
    loading_tasks = {}  # {task_id: (url, depth)}
    next_task_id = 0

    initial_batch_size = min(args.parallel_pages, len(progress_state.queue))
    for _ in range(initial_batch_size):
        if progress_state.queue:
            url, depth = progress_state.queue.popleft()
            if url not in progress_state.visited_urls and depth <= args.max_depth:
                task_id = pipeline_pool.start_loading(
                    url,
                    depth,
                    args.content_selector,
                    timeout_config,
                    url_blacklist_patterns,
                    args.load_strategy,
                    args.max_retries,
                    args.verbose,
                )
                loading_tasks[task_id] = (url, depth)
                next_task_id = max(next_task_id, task_id + 1)

    logger.info(f"ğŸš€ å·²å¯åŠ¨ {len(loading_tasks)} ä¸ªåˆå§‹é¢„åŠ è½½ä»»åŠ¡")
    return loading_tasks


def _find_completed_task(pipeline_pool, loading_tasks):
    """æŸ¥æ‰¾å·²å®Œæˆçš„ä»»åŠ¡"""
    # é€‰æ‹©ä¸€ä¸ªå·²å®ŒæˆåŠ è½½çš„ä»»åŠ¡è¿›è¡Œå¤„ç†
    completed_task_id = None
    for task_id in loading_tasks:
        # å°è¯•è·å–å·²åŠ è½½çš„é¡µé¢ï¼ˆä¸ç­‰å¾…ï¼‰
        page, final_url, error = pipeline_pool.get_loaded_page(task_id, timeout=0.1)
        if page is not None or error is not None:
            completed_task_id = task_id
            break

    if completed_task_id is None:
        # å¦‚æœæ²¡æœ‰ä»»åŠ¡å®Œæˆï¼Œç­‰å¾…æœ€æ—©çš„ä»»åŠ¡
        earliest_task_id = min(loading_tasks.keys())
        page, final_url, error = pipeline_pool.get_loaded_page(earliest_task_id, timeout=30)
        completed_task_id = earliest_task_id

    return completed_task_id


def _process_completed_task(
    pipeline_pool,
    loading_tasks,
    completed_task_id,
    progress_state,
    args,
    base_url_normalized,
    url_pattern,
    timeout_config,
):
    """å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡"""
    if completed_task_id not in loading_tasks:
        return

    url, depth = loading_tasks[completed_task_id]
    page, final_url, error = pipeline_pool.get_loaded_page(completed_task_id, timeout=0.1)

    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
    processed_count = len(progress_state.visited_urls) + 1
    total_discovered = len(progress_state.enqueued)
    remaining_in_queue = len(progress_state.queue)
    active_loading = len(loading_tasks) - 1  # å‡å»å½“å‰æ­£åœ¨å¤„ç†çš„
    progress_info = f"æµæ°´çº¿è¿›åº¦: [{processed_count}/{total_discovered}]"
    if remaining_in_queue > 0 or active_loading > 0:
        progress_info += f" (é˜Ÿåˆ—: {remaining_in_queue}, é¢„åŠ è½½ä¸­: {active_loading})"

    logger.info(f"{progress_info} å¤„ç†: {url} (æ·±åº¦: {depth})")

    if depth > args.max_depth:
        logger.warning(f"è¶…è¿‡æœ€å¤§æ·±åº¦é™åˆ¶({args.max_depth})ï¼Œè·³è¿‡: {url}")
    elif url in progress_state.visited_urls:
        logger.info(f"å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {url}")
    elif page is not None:
        # é¡µé¢åŠ è½½æˆåŠŸï¼Œè¿›è¡Œå†…å®¹å¤„ç†
        try:
            pdf_path, links = _process_loaded_page(
                page,
                url,
                final_url or url,
                args,
                base_url_normalized,
                timeout_config,
                progress_state.temp_dir,
            )

            _handle_page_result(
                progress_state,
                url,
                final_url or url,
                pdf_path,
                links,
                None,
                url_pattern,
                base_url_normalized,
                depth,
                args.max_depth,
            )

        except Exception as e:
            logger.exception(f"å¤„ç†å·²åŠ è½½é¡µé¢ {url} æ—¶å‘ç”Ÿé”™è¯¯")
            progress_state.failed_urls.append((url, f"å¤„ç†å¼‚å¸¸: {e!s}"))
            progress_state.visited_urls.add(url)
    else:
        # é¡µé¢åŠ è½½å¤±è´¥
        failure_reason = error or "é¡µé¢åŠ è½½å¤±è´¥"
        logger.warning(f"é¡µé¢åŠ è½½å¤±è´¥: {url} - {failure_reason}")
        progress_state.failed_urls.append((url, failure_reason))
        progress_state.visited_urls.add(url)


def _start_new_loading_task(pipeline_pool, loading_tasks, progress_state, args, timeout_config, url_blacklist_patterns):
    """å¯åŠ¨æ–°çš„é¢„åŠ è½½ä»»åŠ¡"""
    if not progress_state.queue:
        return

    next_url, next_depth = progress_state.queue.popleft()
    if next_url not in progress_state.visited_urls and next_depth <= args.max_depth:
        task_id = pipeline_pool.start_loading(
            next_url,
            next_depth,
            args.content_selector,
            timeout_config,
            url_blacklist_patterns,
            args.load_strategy,
            args.max_retries,
            args.verbose,
        )
        loading_tasks[task_id] = (next_url, next_depth)
        logger.info(f"ğŸš€ å¯åŠ¨æ–°çš„é¢„åŠ è½½ä»»åŠ¡ #{task_id}: {next_url}")


def _crawl_pages_parallel(
    context,
    args,
    base_url_normalized,
    url_pattern,
    url_blacklist_patterns,
    timeout_config,
    progress_state: ProgressState,
    domain_failure_tracker,
):
    """çœŸæ­£çš„å¹¶è¡Œå¤„ç†æ¨¡å¼ - åŒæ—¶æ‰“å¼€å¤šä¸ªæ ‡ç­¾é¡µé¢„åŠ è½½"""
    logger.info(f"å¯ç”¨çœŸæ­£å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼Œå¹¶è¡Œåº¦: {args.parallel_pages}")
    
    # åˆ›å»ºå¹¶è¡Œå¤„ç†å™¨
    processor = TrueParallelProcessor(context, args.parallel_pages)
    
    try:
        # åˆå§‹åŒ–ï¼šä¸ºæ¯ä¸ªæ§½ä½åˆ†é…URLå¹¶å¼€å§‹é¢„åŠ è½½
        logger.info("ğŸš€ åˆå§‹åŒ–å¹¶è¡Œæ§½ä½...")
        for slot_index in range(args.parallel_pages):
            if progress_state.queue:
                url, depth = progress_state.queue.popleft()
                if url not in progress_state.visited_urls and depth <= args.max_depth:
                    processor._start_page_loading(
                        slot_index, url, depth, args, timeout_config, 
                        domain_failure_tracker.get_all_patterns(url_blacklist_patterns)
                    )
        
        # ä¸»å¤„ç†å¾ªç¯
        current_slot = 0  # å½“å‰å¤„ç†çš„æ§½ä½
        processed_count = len(progress_state.visited_urls)
        
        while any(state is not None for state in processor.page_states) or progress_state.queue:
            page_state = processor.page_states[current_slot]
            
            if page_state is None:
                # å½“å‰æ§½ä½ç©ºé—²ï¼Œå°è¯•åŠ è½½æ–°URL
                if progress_state.queue:
                    url, depth = progress_state.queue.popleft()
                    if url not in progress_state.visited_urls and depth <= args.max_depth:
                        processor._start_page_loading(
                            current_slot, url, depth, args, timeout_config,
                            domain_failure_tracker.get_all_patterns(url_blacklist_patterns)
                        )
                # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ§½ä½
                current_slot = (current_slot + 1) % args.parallel_pages
                continue
            
            # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
            processed_count += 1
            total_discovered = len(progress_state.enqueued)
            active_slots = sum(1 for state in processor.page_states if state is not None)
            remaining_queue = len(progress_state.queue)
            
            progress_info = f"å¹¶è¡Œè¿›åº¦: [{processed_count}/{total_discovered}]"
            if active_slots > 0 or remaining_queue > 0:
                progress_info += f" (æ´»è·ƒæ§½ä½: {active_slots}, é˜Ÿåˆ—: {remaining_queue})"
            
            logger.info(f"{progress_info} å¤„ç†æ§½ä½[{current_slot}]: {page_state.url} (æ·±åº¦: {page_state.depth})")
            
            # æ£€æŸ¥æ·±åº¦å’Œè®¿é—®çŠ¶æ€
            if page_state.depth > args.max_depth:
                logger.warning(f"æ§½ä½[{current_slot}] è¶…è¿‡æœ€å¤§æ·±åº¦é™åˆ¶({args.max_depth})ï¼Œè·³è¿‡: {page_state.url}")
                processor._close_page_slot(current_slot)
                current_slot = (current_slot + 1) % args.parallel_pages
                continue
                
            if page_state.url in progress_state.visited_urls:
                logger.info(f"æ§½ä½[{current_slot}] å·²è®¿é—®è¿‡ï¼Œè·³è¿‡: {page_state.url}")
                processor._close_page_slot(current_slot)
                current_slot = (current_slot + 1) % args.parallel_pages
                continue
            
            try:
                # å®Œæˆé¡µé¢åŠ è½½
                if page_state.is_loading:
                    success = processor._complete_page_loading(
                        current_slot, args, timeout_config,
                        domain_failure_tracker.get_all_patterns(url_blacklist_patterns)
                    )
                    if not success:
                        # åŠ è½½å¤±è´¥ï¼Œè®°å½•å¹¶ç»§ç»­
                        failure_reason = page_state.load_error or "é¡µé¢åŠ è½½å¤±è´¥"
                        progress_state.failed_urls.append((page_state.url, failure_reason))
                        progress_state.visited_urls.add(page_state.url)
                        domain_failure_tracker.record_failure(page_state.url)
                        processor._close_page_slot(current_slot)
                        current_slot = (current_slot + 1) % args.parallel_pages
                        continue
                
                # å¤„ç†é¡µé¢å†…å®¹
                pdf_path, links = processor._process_page_content(
                    current_slot, args, base_url_normalized, timeout_config, progress_state
                )
                
                # æ›´æ–°è¿›åº¦çŠ¶æ€
                _handle_page_result(
                    progress_state,
                    page_state.url,
                    page_state.final_url or page_state.url,
                    pdf_path,
                    links,
                    None,  # æ²¡æœ‰å¤±è´¥åŸå› 
                    url_pattern,
                    base_url_normalized,
                    page_state.depth,
                    args.max_depth,
                )
                
            except Exception as e:
                logger.exception(f"æ§½ä½[{current_slot}] å¤„ç† {page_state.url} æ—¶å‘ç”Ÿé”™è¯¯")
                progress_state.failed_urls.append((page_state.url, f"å¼‚å¸¸é”™è¯¯: {e!s}"))
                progress_state.visited_urls.add(page_state.url)
            
            # å…³é—­å½“å‰æ§½ä½ï¼Œå‡†å¤‡åŠ è½½æ–°URL
            processor._close_page_slot(current_slot)
            
            # å°è¯•ä¸ºå½“å‰æ§½ä½åŠ è½½æ–°URL
            if progress_state.queue:
                url, depth = progress_state.queue.popleft()
                if url not in progress_state.visited_urls and depth <= args.max_depth:
                    processor._start_page_loading(
                        current_slot, url, depth, args, timeout_config,
                        domain_failure_tracker.get_all_patterns(url_blacklist_patterns)
                    )
            
            # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ§½ä½
            current_slot = (current_slot + 1) % args.parallel_pages
            
            # ä¿å­˜è¿›åº¦
            progress_state.save_to_file()
    
    finally:
        # ç¡®ä¿å¤„ç†å™¨è¢«æ­£ç¡®å…³é—­
        processor.close_all()
    
    # æœ€ç»ˆç»Ÿè®¡
    success_count = len(progress_state.processed_urls)
    failed_count = len(progress_state.failed_urls)
    total_processed = success_count + failed_count
    
    if total_processed > 0:
        logger.info("\nğŸ“ˆ å¹¶è¡Œå¤„ç†å®Œæˆç»Ÿè®¡:")
        logger.info(f"   æ€»å…±å¤„ç†: {total_processed} ä¸ªURL")
        logger.info(f"   æˆåŠŸ: {success_count} ä¸ª ({success_count/total_processed*100:.1f}%)")
        logger.info(f"   å¤±è´¥: {failed_count} ä¸ª ({failed_count/total_processed*100:.1f}%)")
    
    return progress_state


def _process_loaded_page(page, original_url, final_url, args, base_url_normalized, timeout_config, temp_dir):
    """å¤„ç†å·²åŠ è½½çš„é¡µé¢ï¼Œç”ŸæˆPDFå¹¶æå–é“¾æ¥"""
    # æå–é¡µé¢é“¾æ¥
    links = _extract_page_links(page, args.toc_selector, final_url, base_url_normalized)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰PDFæ–‡ä»¶
    if temp_dir:
        expected_pdf = Path(temp_dir) / url_to_filename(original_url)
        if expected_pdf.exists() and expected_pdf.stat().st_size > 1000:
            logger.info(f"å‘ç°å·²å­˜åœ¨çš„PDFæ–‡ä»¶ï¼Œè·³è¿‡ç”Ÿæˆ: {original_url}")
            return expected_pdf, links

    # å‡†å¤‡é¡µé¢å†…å®¹ç”¨äºPDFç”Ÿæˆ
    if not _prepare_page_for_pdf(
        page, args.content_selector, args.verbose, timeout_config, args.debug, args.debug_dir, original_url
    ):
        return None, links

    # ç”ŸæˆPDF
    pdf_path = _generate_pdf_from_page(page, args.verbose, timeout_config, temp_dir, original_url)

    return pdf_path, links


def _handle_page_result(
    progress_state, url, final_url, pdf_path, links, failure_reason, url_pattern, base_url_normalized, depth, max_depth
):
    """å¤„ç†é¡µé¢å¤„ç†ç»“æœï¼Œæ›´æ–°è¿›åº¦çŠ¶æ€"""
    progress_state.visited_urls.add(url)
    progress_state.visited_urls.add(final_url)

    if pdf_path and pdf_path.exists():
        progress_state.pdf_files.append(pdf_path)
        progress_state.processed_urls.append(url)
        logger.info(f"âœ… æˆåŠŸç”ŸæˆPDF: {pdf_path}")
    elif failure_reason:
        progress_state.failed_urls.append((url, failure_reason))
        logger.warning(f"âŒ é¡µé¢å¤„ç†å¤±è´¥ï¼Œè®°å½•å¾…é‡è¯•: {url} - {failure_reason}")
    else:
        logger.warning(f"âŒ é¡µé¢æœªç”ŸæˆPDF: {url}")

    # å¤„ç†æ–°å‘ç°çš„é“¾æ¥
    new_links_count = 0
    for link in links:
        if not link:
            continue

        norm_url = normalize_url(link, base_url_normalized)

        if not url_pattern.match(norm_url):
            logger.debug(f"è·³è¿‡ä¸ç¬¦åˆæ¨¡å¼çš„URL: {norm_url}")
            continue

        if norm_url in progress_state.visited_urls or norm_url in progress_state.enqueued:
            logger.debug(f"å·²å­˜åœ¨ï¼Œè·³è¿‡URL: {norm_url}")
            continue

        logger.info(f"ğŸ”— æ·»åŠ æ–°URLåˆ°é˜Ÿåˆ—: {norm_url} (æ·±åº¦: {depth+1})")
        progress_state.queue.append((norm_url, depth + 1))
        progress_state.enqueued.add(norm_url)
        new_links_count += 1

    if new_links_count > 0:
        logger.info(f"ğŸ“Š ä»å½“å‰é¡µé¢å‘ç° {new_links_count} ä¸ªæ–°é“¾æ¥ï¼Œé˜Ÿåˆ—æ€»æ•°: {len(progress_state.queue)}")


def _prompt_user_choice(failed_urls):
    """æç¤ºç”¨æˆ·é€‰æ‹©é‡è¯•æ–¹å¼"""
    print(f"\n=== å‘ç° {len(failed_urls)} ä¸ªå¤±è´¥çš„URL ===")
    for i, (url, reason) in enumerate(failed_urls, 1):
        print(f"{i}. {url}")
        print(f"   å¤±è´¥åŸå› : {reason}")

    while True:
        try:
            choice = input(
                "\næ˜¯å¦è¦é‡è¯•å¤±è´¥çš„URLï¼Ÿ\n"
                "1. é‡è¯•æ‰€æœ‰å¤±è´¥çš„URL\n"
                "2. é€‰æ‹©æ€§é‡è¯•\n"
                "3. è·³è¿‡æ‰€æœ‰å¤±è´¥çš„URL\n"
                "è¯·é€‰æ‹© (1-3): "
            ).strip()

            if choice in ["1", "2", "3"]:
                return choice
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1ã€2 æˆ– 3")
            continue
        except (EOFError, KeyboardInterrupt):
            logger.info("ç”¨æˆ·å–æ¶ˆé‡è¯•")
            return "3"


def _get_urls_to_retry(choice, failed_urls):
    """æ ¹æ®ç”¨æˆ·é€‰æ‹©è·å–è¦é‡è¯•çš„URLåˆ—è¡¨"""
    if choice == "3":
        logger.info("ç”¨æˆ·é€‰æ‹©è·³è¿‡æ‰€æœ‰å¤±è´¥çš„URL")
        return []
    if choice == "1":
        return [url for url, _ in failed_urls]
    if choice == "2":
        urls_to_retry = []
        for i, (url, reason) in enumerate(failed_urls, 1):
            retry_choice = input(f"é‡è¯• URL {i}: {url} ? (y/n): ").strip().lower()
            if retry_choice in ["y", "yes", "æ˜¯"]:
                urls_to_retry.append(url)
        return urls_to_retry
    return []


def _get_retry_count():
    """è·å–é‡è¯•æ¬¡æ•°"""
    while True:
        try:
            retry_count = input("é‡è¯•æ¬¡æ•° (1-10, é»˜è®¤3): ").strip()
            if not retry_count:
                return 3
            retry_count = int(retry_count)
            if retry_count < 1 or retry_count > 10:
                print("é‡è¯•æ¬¡æ•°å¿…é¡»åœ¨1-10ä¹‹é—´")
                continue
            return retry_count
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            continue
        except (EOFError, KeyboardInterrupt):
            logger.info("ç”¨æˆ·å–æ¶ˆé‡è¯•")
            return 0


def _retry_single_url(retry_page, url, args, base_url_normalized, timeout_config, url_blacklist_patterns, retry_count):
    """é‡è¯•å•ä¸ªURL"""
    for attempt in range(retry_count):
        try:
            pdf_path, _, final_url, failure_reason = process_page_with_failure_tracking(
                retry_page,
                url,
                args.content_selector,
                args.toc_selector,
                base_url_normalized,
                timeout_config,
                args.max_retries,
                args.debug,
                args.debug_dir,
                args.verbose,
                args.load_strategy,
                url_blacklist_patterns,
            )

            if pdf_path and pdf_path.exists():
                logger.info(f"âœ… é‡è¯•æˆåŠŸ: {url}")
                return pdf_path, url, True
            logger.warning(f"âš ï¸ é‡è¯•ç¬¬ {attempt + 1}/{retry_count} æ¬¡å¤±è´¥: {url} - {failure_reason}")

        except Exception as e:
            logger.warning(f"âš ï¸ é‡è¯•ç¬¬ {attempt + 1}/{retry_count} æ¬¡å¼‚å¸¸: {url} - {e!s}")

    logger.error(f"âŒ é‡è¯•æ‰€æœ‰æ¬¡æ•°åä»ç„¶å¤±è´¥: {url}")
    return None, url, False


def _interactive_retry_failed_urls(
    context, failed_urls, args, base_url_normalized, timeout_config, url_blacklist_patterns, domain_failure_tracker
):
    """äº¤äº’å¼é‡è¯•å¤±è´¥çš„URL"""
    if not failed_urls:
        return [], []

    # å¦‚æœå¯ç”¨äº†è·³è¿‡å¤±è´¥é‡è¯•é€‰é¡¹ï¼Œç›´æ¥è¿”å›
    if args.skip_failed_retry:
        logger.info("å¯ç”¨äº†è·³è¿‡å¤±è´¥é‡è¯•é€‰é¡¹ï¼Œç›´æ¥å¤„ç†æˆåŠŸçš„é¡µé¢")
        return [], []

    # è·å–ç”¨æˆ·é€‰æ‹©
    choice = _prompt_user_choice(failed_urls)
    urls_to_retry = _get_urls_to_retry(choice, failed_urls)

    if not urls_to_retry:
        logger.info("æ²¡æœ‰é€‰æ‹©è¦é‡è¯•çš„URL")
        return [], []

    # è·å–é‡è¯•æ¬¡æ•°
    retry_count = _get_retry_count()
    if retry_count == 0:
        return [], []

    logger.info(f"å¼€å§‹é‡è¯• {len(urls_to_retry)} ä¸ªå¤±è´¥çš„URLï¼Œé‡è¯•æ¬¡æ•°: {retry_count}")

    # é‡è¯•æ—¶æ€»æ˜¯ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼Œé¿å…å¤æ‚æ€§
    retry_page = context.new_page()
    logger.info("ä¸ºé‡è¯•åˆ›å»ºä¸“ç”¨é¡µé¢ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")

    try:
        retry_pdf_files = []
        retry_processed_urls = []
        still_failed_urls = []

        for i, url in enumerate(urls_to_retry, 1):
            logger.info(f"ğŸ”„ é‡è¯•è¿›åº¦: [{i}/{len(urls_to_retry)}] å¤„ç†: {url}")

            pdf_path, processed_url, success = _retry_single_url(
                retry_page,
                url,
                args,
                base_url_normalized,
                timeout_config,
                url_blacklist_patterns,
                retry_count,
            )

            if success:
                retry_pdf_files.append(pdf_path)
                retry_processed_urls.append(processed_url)
            else:
                still_failed_urls.append((url, "é‡è¯•åä»ç„¶å¤±è´¥"))

    finally:
        # ç¡®ä¿é‡è¯•é¡µé¢è¢«æ­£ç¡®å…³é—­
        try:
            retry_page.close()
            logger.info("å·²å…³é—­é‡è¯•ä¸“ç”¨é¡µé¢")
        except Exception as close_err:
            logger.warning(f"å…³é—­é‡è¯•é¡µé¢æ—¶å‡ºé”™: {close_err!s}")

    # é‡è¯•ç»“æœç»Ÿè®¡
    retry_success_count = len(retry_processed_urls)
    retry_failed_count = len(still_failed_urls)
    logger.info("\nğŸ“Š é‡è¯•ç»“æœç»Ÿè®¡:")
    logger.info(f"   é‡è¯•æˆåŠŸ: {retry_success_count} ä¸ª")
    logger.info(f"   é‡è¯•åä»å¤±è´¥: {retry_failed_count} ä¸ª")

    if still_failed_urls:
        logger.warning(f"ä»æœ‰ {len(still_failed_urls)} ä¸ªURLé‡è¯•åä¾ç„¶å¤±è´¥:")
        for url, reason in still_failed_urls:
            logger.warning(f"  - {url}: {reason}")

    return retry_pdf_files, retry_processed_urls


def _merge_pdfs(pdf_files, processed_urls, args):
    """åˆå¹¶PDFæ–‡ä»¶"""
    if not pdf_files:
        logger.error("æœªç”Ÿæˆä»»ä½•PDFï¼Œè¯·æ£€æŸ¥å‚æ•°")
        return []

    logger.info(f"ğŸ“„ å‡†å¤‡åˆå¹¶ {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

    base_path = Path(args.output_pdf)
    stem = base_path.stem
    suffix = base_path.suffix if base_path.suffix else ".pdf"
    output_dir = base_path.parent

    output_dir.mkdir(parents=True, exist_ok=True)

    merger = PdfMerger()
    current_pages = 0
    file_index = 1
    merged_files = []

    for i, pdf_file in enumerate(pdf_files, 1):
        try:
            progress_info = f"åˆå¹¶è¿›åº¦: [{i}/{len(pdf_files)}]"
            logger.info(f"ğŸ“„ {progress_info} å¤„ç†PDFæ–‡ä»¶: {pdf_file}")

            if not pdf_file.exists():
                logger.warning(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_file}")
                continue

            with open(pdf_file, "rb") as f:
                reader = PdfReader(f)
                num_pages = len(reader.pages)
                logger.debug(f"   æ–‡ä»¶é¡µæ•°: {num_pages}")

                if current_pages > 0 and current_pages + num_pages > args.max_page:
                    output_name = f"{stem}.{file_index}{suffix}"
                    output_path = output_dir / output_name

                    logger.info(f"ğŸ“š å†™å…¥åˆ†å· {output_path} (é¡µæ•°: {current_pages})")
                    with open(output_path, "wb") as out:
                        merger.write(out)
                    merged_files.append(str(output_path))

                    file_index += 1
                    merger = PdfMerger()
                    current_pages = 0

                merger.append(str(pdf_file))
                current_pages += num_pages

                try:
                    pdf_file.unlink()
                    logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {pdf_file}")
                except Exception as unlink_err:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {unlink_err}")

        except Exception as e:
            logger.error(f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥ {pdf_file}: {e}")

    if current_pages > 0:
        if file_index == 1:
            output_path = base_path
        else:
            output_name = f"{stem}.{file_index}{suffix}"
            output_path = output_dir / output_name

        logger.info(f"ğŸ“š å†™å…¥æœ€ç»ˆPDF: {output_path} (é¡µæ•°: {current_pages})")
        with open(output_path, "wb") as out:
            merger.write(out)
        merged_files.append(str(output_path))

    if merged_files:
        logger.info(f"ğŸ‰ å¤„ç†å®Œæˆ! å…±å¤„ç† {len(processed_urls)} ä¸ªé¡µé¢ï¼Œç”Ÿæˆ {len(merged_files)} ä¸ªPDFæ–‡ä»¶")
        logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {', '.join(merged_files)}")
    else:
        logger.error("æ²¡æœ‰PDFæ–‡ä»¶ç”Ÿæˆ")

    return merged_files


def _create_argument_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description="Webpage to PDF converter")

    # å¿…å¡«å‚æ•° - æ·»åŠ çŸ­å‚æ•°
    parser.add_argument("-u", "--base-url", required=True, help="èµ·å§‹URL")
    parser.add_argument("-c", "--content-selector", required=True, help="å†…å®¹å®¹å™¨é€‰æ‹©å™¨")
    parser.add_argument("-t", "--toc-selector", action="append", required=True, help="é“¾æ¥æå–é€‰æ‹©å™¨ï¼Œå¯æŒ‡å®šå¤šä¸ª")
    parser.add_argument("-o", "--output-pdf", required=True, help="è¾“å‡ºPDFè·¯å¾„")

    # URLè¿‡æ»¤ç›¸å…³å‚æ•°
    parser.add_argument("--url-pattern", default=None, help="URLåŒ¹é…æ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼")
    parser.add_argument(
        "-b", "--url-blacklist",
        action="append",
        default=[
            "https://analytics.twitter.com/",
            "https://connect.facebook.net/",
            "https://t.co/",
            "https://www.google-analytics.com/"
        ],
        help="URLé»‘åå•æ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼ï¼Œå¯æŒ‡å®šå¤šä¸ªï¼Œé˜»æ­¢æµè§ˆå™¨åŠ è½½åŒ¹é…çš„URL",
    )
    parser.add_argument(
        "-B", "--url-blacklist-auto-threshold",
        type=int,
        default=5,
        help="è‡ªåŠ¨é»‘åå•é˜ˆå€¼ï¼Œå½“æŸä¸ªåŸŸåå‡ºç°æŒ‡å®šæ¬¡æ•°çš„è¯·æ±‚å¼‚å¸¸æ—¶ï¼Œè‡ªåŠ¨åŠ å…¥é»‘åå•",
    )

    # åŸºæœ¬é…ç½®å‚æ•°
    parser.add_argument("--max-page", type=int, default=10000, help="å•PDFæœ€å¤§é¡µæ•°")
    parser.add_argument("--timeout", type=int, default=120, help="é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--max-depth", type=int, default=10, help="æœ€å¤§çˆ¬å–æ·±åº¦")
    parser.add_argument("--max-retries", type=int, default=3, help="å¤±è´¥é‡è¯•æ¬¡æ•°")

    # è°ƒè¯•å’Œæ˜¾ç¤ºå‚æ•°
    parser.add_argument("-d", "--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜é¡µé¢æˆªå›¾")
    parser.add_argument("--debug-dir", default="debug_screenshots", help="è°ƒè¯•æˆªå›¾ä¿å­˜ç›®å½•")
    parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢ï¼Œä¾¿äºè§‚å¯Ÿå¤„ç†è¿‡ç¨‹")

    # åŠ è½½ç­–ç•¥å‚æ•°
    parser.add_argument("--fast-load", action="store_true", help="å¿«é€ŸåŠ è½½æ¨¡å¼ï¼Œè·³è¿‡ç½‘ç»œç©ºé—²ç­‰å¾…")
    parser.add_argument(
        "--load-strategy",
        choices=["fast", "normal", "thorough"],
        default="thorough",
        help="é¡µé¢åŠ è½½ç­–ç•¥ï¼šfast=ä»…ç­‰å¾…DOM, normal=æ™ºèƒ½ç­‰å¾…, thorough=å®Œå…¨ç­‰å¾…ç½‘ç»œç©ºé—²",
    )

    # é‡è¯•å’Œæµæ§å‚æ•°
    parser.add_argument("--skip-failed-retry", action="store_true", help="è·³è¿‡å¤±è´¥URLçš„äº¤äº’å¼é‡è¯•ï¼Œç›´æ¥å¤„ç†æˆåŠŸçš„é¡µé¢")
    parser.add_argument(
        "--parallel-pages",
        type=int,
        default=2,
        help="å¹¶è¡Œé¡µé¢æ•°é‡ï¼ŒåŒæ—¶æ‰“å¼€å¤šä¸ªæ ‡ç­¾é¡µé¢„åŠ è½½æé«˜å¤„ç†é€Ÿåº¦ã€‚1=ä¸²è¡Œå¤„ç†ï¼Œ2+=çœŸæ­£å¹¶è¡Œå¤„ç†",
    )
    parser.add_argument(
        "--qos-wait",
        type=int,
        default=600,
        help="QoSç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå½“æ£€æµ‹åˆ°å¤šä¸ªå¹¶è¡Œä»»åŠ¡éƒ½å¤±è´¥æ—¶ï¼Œç­‰å¾…æŒ‡å®šæ—¶é—´ä»¥é¿å…è§¦å‘ç½‘ç«™æµæ§ï¼Œé»˜è®¤600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰",
    )

    # ç¼“å­˜ç®¡ç†å‚æ•°
    parser.add_argument("--restart", action="store_true", help="é‡æ–°å¼€å§‹çˆ¬å–ï¼Œåˆ é™¤ä¹‹å‰çš„ç¼“å­˜å’Œè¿›åº¦æ–‡ä»¶")
    parser.add_argument("--cleanup", action="store_true", help="æ¸…ç†æŒ‡å®šURLå’Œè¾“å‡ºæ–‡ä»¶å¯¹åº”çš„ä¸´æ—¶æ–‡ä»¶å’Œè¿›åº¦æ–‡ä»¶")

    return parser


def _handle_cleanup_command(args):
    """å¤„ç†æ¸…ç†å‘½ä»¤"""
    base_url_normalized = normalize_url(args.base_url)
    cache_id = calculate_cache_id(
        base_url_normalized,
        args.content_selector,
        args.toc_selector,
        args.max_depth,
        args.url_pattern,
    )
    cache_dir = get_cache_directory(cache_id)
    cleanup_cache_directory(cache_dir)


def _initialize_configuration(args):
    """åˆå§‹åŒ–ç¨‹åºé…ç½®"""
    logger.info(f"å¼€å§‹æ‰§è¡ŒPDFçˆ¬è™«ç¨‹åºï¼Œè¶…æ—¶è®¾ç½®: {args.timeout}ç§’")

    # åˆ›å»ºè¶…æ—¶é…ç½®å¯¹è±¡
    timeout_config = TimeoutConfig(args.timeout)
    logger.info(
        f"è¶…æ—¶é…ç½® - åŸºç¡€: {timeout_config.base_timeout}s, å¿«é€Ÿæ¨¡å¼: {timeout_config.fast_mode_timeout}s, "
        f"åˆå§‹åŠ è½½: {timeout_config.initial_load_timeout}ms, é¡µé¢æ¸²æŸ“: {timeout_config.page_render_wait}s"
    )

    base_url_normalized = normalize_url(args.base_url, args.base_url)
    logger.info(f"æ ‡å‡†åŒ–åŸºå‡†URL: {base_url_normalized}")

    # åˆ›å»ºåŸŸåå¤±è´¥è·Ÿè¸ªå™¨
    domain_failure_tracker = DomainFailureTracker(
        failure_counts={},
        auto_threshold=args.url_blacklist_auto_threshold,
        auto_blacklist_patterns=[],
    )
    logger.info(f"è‡ªåŠ¨é»‘åå•é˜ˆå€¼: {args.url_blacklist_auto_threshold} æ¬¡")

    # ç¼–è¯‘URLé»‘åå•æ¨¡å¼
    url_blacklist_patterns = compile_blacklist_patterns(args.url_blacklist)
    if url_blacklist_patterns:
        logger.info(f"é…ç½®äº† {len(url_blacklist_patterns)} ä¸ªæ‰‹åŠ¨URLé»‘åå•æ¨¡å¼")

    # ä¿®æ”¹é»˜è®¤URLæ¨¡å¼ï¼šä½¿ç”¨çˆ¶ç›®å½•è€ŒéåŸŸå
    if args.url_pattern:
        url_pattern = re.compile(args.url_pattern)
        logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰URLåŒ¹é…æ¨¡å¼: {url_pattern.pattern}")
    else:
        default_pattern = get_parent_path_pattern(base_url_normalized)
        url_pattern = re.compile(default_pattern)
        logger.info(f"ä½¿ç”¨é»˜è®¤URLåŒ¹é…æ¨¡å¼ï¼ˆåŸºäºçˆ¶ç›®å½•ï¼‰: {url_pattern.pattern}")

    return timeout_config, base_url_normalized, url_blacklist_patterns, url_pattern, domain_failure_tracker


def _setup_browser_context(p, args):
    """è®¾ç½®æµè§ˆå™¨å’Œä¸Šä¸‹æ–‡"""
    headless_mode = not args.verbose
    if args.verbose:
        logger.info("å¯ç”¨å¯è§†åŒ–æ¨¡å¼ - æµè§ˆå™¨ç•Œé¢å°†æ˜¾ç¤ºå¤„ç†è¿‡ç¨‹")
    else:
        logger.info("ä½¿ç”¨æ— å¤´æ¨¡å¼ - æµè§ˆå™¨åœ¨åå°è¿è¡Œ")

    browser = p.chromium.launch(
        headless=headless_mode,
        args=(
            [
                "--disable-gpu",
                "--disable-dev-shm-usage",
                "--disable-setuid-sandbox",
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled",
            ]
            if headless_mode
            else [
                "--disable-blink-features=AutomationControlled",
            ]
        ),  # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹å‡å°‘å¯åŠ¨å‚æ•°ï¼Œé¿å…å½±å“æ˜¾ç¤º
    )
    context = browser.new_context(
        viewport={"width": 1366, "height": 768},
        ignore_https_errors=True,
        java_script_enabled=True,
        bypass_csp=True,
    )

    context.set_default_timeout(args.timeout * 1000)
    return browser, context


def _setup_cache_and_progress(args, base_url_normalized):
    """è®¾ç½®ç¼“å­˜å’Œè¿›åº¦çŠ¶æ€"""
    cache_id = calculate_cache_id(
        base_url_normalized,
        args.content_selector,
        args.toc_selector,
        args.max_depth,
        args.url_pattern,
    )
    cache_dir = get_cache_directory(cache_id)
    
    # å¦‚æœæŒ‡å®šäº† --restartï¼Œå…ˆæ¸…ç†ç¼“å­˜
    if args.restart:
        logger.info("æ£€æµ‹åˆ° --restart å‚æ•°ï¼Œæ¸…ç†ä¹‹å‰çš„ç¼“å­˜å’Œè¿›åº¦...")
        cleanup_cache_directory(cache_dir)
        logger.info("ç¼“å­˜æ¸…ç†å®Œæˆï¼Œå°†é‡æ–°å¼€å§‹çˆ¬å–")
    
    use_cache = True  # æ€»æ˜¯ä½¿ç”¨ç¼“å­˜ï¼Œä½†å¦‚æœæŒ‡å®šäº† restart åˆ™å…ˆæ¸…ç†

    logger.info(f"ç¼“å­˜ç›®å½•: {cache_dir}")
    logger.info(f"ç¼“å­˜æ¨¡å¼: {'é‡æ–°å¼€å§‹' if args.restart else 'å¯ç”¨'}")

    # åˆå§‹åŒ–æˆ–æ¢å¤è¿›åº¦çŠ¶æ€
    progress_state, is_resumed = _initialize_or_resume_progress(
        base_url_normalized,
        args.output_pdf,
        args.max_depth,
        cache_dir,
        use_cache and not args.restart,  # å¦‚æœæ˜¯é‡æ–°å¼€å§‹ï¼Œä¸æ¢å¤è¿›åº¦
    )

    # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œæ”¯æŒä¸­æ–­æ¢å¤
    setup_signal_handlers(progress_state)

    if is_resumed and not args.restart:
        logger.info("å‘ç°æœªå®Œæˆçš„çˆ¬å–ä»»åŠ¡ï¼Œè‡ªåŠ¨ç»§ç»­æ‰§è¡Œ...")
    elif args.restart:
        logger.info("é‡æ–°å¼€å§‹çˆ¬å–ä»»åŠ¡...")
    else:
        logger.info("å¼€å§‹æ–°çš„çˆ¬å–ä»»åŠ¡...")

    return cache_dir, use_cache, progress_state


def _execute_crawling_workflow(
    context,
    args,
    base_url_normalized,
    url_pattern,
    url_blacklist_patterns,
    timeout_config,
    progress_state,
    domain_failure_tracker,
):
    """æ‰§è¡Œçˆ¬å–å·¥ä½œæµ"""
    # æ‰§è¡Œçˆ¬å–ï¼ˆæ”¯æŒè¿›åº¦æ¢å¤ï¼‰
    progress_state = _crawl_pages_with_progress(
        context,
        args,
        base_url_normalized,
        url_pattern,
        url_blacklist_patterns,
        timeout_config,
        progress_state,
        domain_failure_tracker,
    )

    # å¦‚æœæœ‰å¤±è´¥çš„URLï¼Œè¯¢é—®æ˜¯å¦é‡è¯•
    if progress_state.failed_urls and not args.skip_failed_retry:
        retry_pdf_files, retry_processed_urls = _interactive_retry_failed_urls(
            context,
            progress_state.failed_urls,
            args,
            base_url_normalized,
            timeout_config,
            url_blacklist_patterns,
            domain_failure_tracker,
        )

        # åˆå¹¶é‡è¯•æˆåŠŸçš„æ–‡ä»¶
        progress_state.pdf_files.extend(retry_pdf_files)
        progress_state.processed_urls.extend(retry_processed_urls)

    return progress_state


def main():
    parser = _create_argument_parser()
    args = parser.parse_args()

    # å¤„ç†æ¸…ç†å‘½ä»¤
    if args.cleanup:
        _handle_cleanup_command(args)
        return

    # åˆå§‹åŒ–é…ç½®
    timeout_config, base_url_normalized, url_blacklist_patterns, url_pattern, domain_failure_tracker = (
        _initialize_configuration(args)
    )

    with sync_playwright() as p:
        browser, context = _setup_browser_context(p, args)
        cache_dir, use_cache, progress_state = _setup_cache_and_progress(args, base_url_normalized)

        try:
            progress_state = _execute_crawling_workflow(
                context,
                args,
                base_url_normalized,
                url_pattern,
                url_blacklist_patterns,
                timeout_config,
                progress_state,
                domain_failure_tracker,
            )

            logger.info("çˆ¬å–å®Œæˆï¼Œå…³é—­æµè§ˆå™¨...")
            browser.close()

            # æ˜¾ç¤ºåŸŸåå¤±è´¥ç»Ÿè®¡
            failure_summary = domain_failure_tracker.get_failure_summary()
            if failure_summary != "æ— åŸŸåå¤±è´¥è®°å½•":
                logger.info(f"\nğŸ“Š {failure_summary}")

            # åˆå¹¶PDFæ–‡ä»¶
            _merge_pdfs(progress_state.pdf_files, progress_state.processed_urls, args)

            # æˆåŠŸå®Œæˆåè‡ªåŠ¨æ¸…ç†ç¼“å­˜ç›®å½•
            if use_cache:
                cleanup_cache_directory(cache_dir)

        except KeyboardInterrupt:
            logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            logger.info(f"è¿›åº¦å·²ä¿å­˜åˆ°: {progress_state.progress_file}")
            logger.info(f"ç¼“å­˜ç›®å½•: {cache_dir}")
            logger.info("ä¸‹æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ç»§ç»­ï¼ˆé™¤éä½¿ç”¨ --restart å‚æ•°é‡æ–°å¼€å§‹ï¼‰")
            browser.close()
            return
        except Exception:
            logger.exception("ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
            browser.close()
            raise


if __name__ == "__main__":
    main()
