"""
页面处理器实现

该模块包含具体的页面处理器实现，用于处理不同的页面任务。
"""

import logging
import time
import uuid
from collections import defaultdict
from urllib.parse import urlparse

from playwright.async_api import Request, Response
from prometheus_client import Counter, Gauge, Histogram

from .protocol import PageContext, PageProcessor, ProcessorState, URL, URLCollection, URLStatus

logger = logging.getLogger(__name__)

# Prometheus 指标
page_monitor_slow_requests = Counter(
    "page_monitor_slow_requests_total",
    "监控到的慢请求总数",
    ["domain", "path"],
)

page_monitor_failed_requests = Counter(
    "page_monitor_failed_requests_total",
    "监控到的失败请求总数",
    ["domain", "path", "failure_type"],
)

page_monitor_state_changes = Counter(
    "page_monitor_state_changes_total",
    "页面状态变化总数",
    ["state"],
)

page_monitor_processing_time = Histogram(
    "page_monitor_processing_seconds",
    "页面监控处理时间",
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
)

page_monitor_active_pages = Gauge(
    "page_monitor_active_pages",
    "当前活跃的页面监控数量",
)

# RequestMonitor Prometheus 指标
request_monitor_blocked_urls = Counter(
    "request_monitor_blocked_urls_total",
    "被屏蔽的URL总数",
    ["reason", "domain", "path"],
)

request_monitor_processing_time = Histogram(
    "request_monitor_processing_seconds",
    "请求监控处理时间",
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],
)

request_monitor_active_monitors = Gauge(
    "request_monitor_active_monitors",
    "当前活跃的请求监控数量",
)

# LinksFinder Prometheus 指标
links_finder_discovered_links = Counter(
    "links_finder_discovered_links_total",
    "发现的链接总数",
    ["domain", "source_domain"],
)

links_finder_valid_links = Counter(
    "links_finder_valid_links_total",
    "有效链接总数",
    ["domain", "source_domain"],
)

links_finder_processing_time = Histogram(
    "links_finder_processing_seconds",
    "链接发现处理时间",
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0],
)

links_found_total = Counter(
    'links_found_total', 
    'Total number of links found',
    ['css_selector']
)

# ElementCleaner 指标
elements_removed_total = Counter(
    'elements_removed_total',
    'Total number of elements removed',
    ['css_selector', 'success']
)


class PageMonitor(PageProcessor):
    """页面监控处理器，监控页面加载状态、慢请求和失败请求"""

    def __init__(self, name: str, page_timeout: float = 60.0, priority: int = 0):
        """
        初始化页面监控处理器

        Args:
            name: 处理器名称
            page_timeout: 页面加载超时时间（秒）
            priority: 优先级，固定为0（最高优先级）

        """
        super().__init__(name, priority)
        self.page_timeout = page_timeout
        self.slow_request_timeout = page_timeout / 10  # 慢请求超时为页面超时的1/10
        self._page_state = "loading"  # loading, ready, completed
        self._request_start_times = {}  # 请求开始时间
        self._monitoring_started = False
        self._start_time = None

    def _remove_query_string(self, url: str) -> str:
        """移除URL中的查询字符串"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def _get_domain_path(self, url: str) -> tuple[str, str]:
        """获取URL的域名和路径"""
        parsed = urlparse(url)
        domain = parsed.netloc or "unknown"
        path = parsed.path or "/"
        return domain, path

    async def _on_request(self, request: Request) -> None:
        """请求开始事件处理"""
        self._request_start_times[request.url] = time.time()
        logger.debug(f"请求开始: {request.url}")

    async def _on_response(self, response: Response) -> None:
        """响应事件处理"""
        request_url = response.request.url
        start_time = self._request_start_times.pop(request_url, None)

        if start_time:
            duration = time.time() - start_time

            # 检查慢请求
            if duration > self.slow_request_timeout:
                clean_url = self._remove_query_string(request_url)
                domain, path = self._get_domain_path(clean_url)

                logger.warning(f"慢请求检测: {clean_url}, 耗时: {duration:.2f}秒")

                # 更新上下文中的慢请求计数器
                if "slow_requests" not in self._context.data:
                    self._context.data["slow_requests"] = defaultdict(int)
                self._context.data["slow_requests"][clean_url] += 1

                # 更新 Prometheus 指标
                page_monitor_slow_requests.labels(domain=domain, path=path).inc()

        logger.debug(f"响应完成: {request_url}, 状态: {response.status}")

    async def _on_request_failed(self, request: Request) -> None:
        """请求失败事件处理"""
        request_url = request.url
        self._request_start_times.pop(request_url, None)

        clean_url = self._remove_query_string(request_url)
        domain, path = self._get_domain_path(clean_url)

        failure_reason = request.failure or "unknown"
        logger.warning(f"请求失败: {clean_url}, 原因: {failure_reason}")

        # 更新上下文中的失败请求计数器
        if "failed_requests" not in self._context.data:
            self._context.data["failed_requests"] = defaultdict(int)
        self._context.data["failed_requests"][clean_url] += 1

        # 更新 Prometheus 指标
        page_monitor_failed_requests.labels(
            domain=domain,
            path=path,
            failure_type=failure_reason,
        ).inc()

    async def _setup_page_listeners(self, page) -> None:
        """设置页面事件监听器"""
        page.on("request", self._on_request)
        page.on("response", self._on_response)
        page.on("requestfailed", self._on_request_failed)

        # 监听页面加载状态变化
        page.on("load", self._on_load)
        page.on("domcontentloaded", self._on_dom_content_loaded)

        logger.debug("页面事件监听器已设置")

    async def _on_load(self) -> None:
        """页面load事件处理"""
        if self._page_state == "loading":
            self._page_state = "ready"
            self._context.data["page_state"] = "ready"
            page_monitor_state_changes.labels(state="ready").inc()
            logger.info(f"页面进入load状态: {self._context.url.url}")

    async def _on_dom_content_loaded(self) -> None:
        """DOM内容加载完成事件处理"""
        logger.debug(f"DOM内容加载完成: {self._context.url.url}")

    async def _wait_for_network_idle(self, page, timeout: float = 5.0) -> bool:
        """等待网络空闲状态"""
        try:
            await page.wait_for_load_state("networkidle", timeout=timeout * 1000)
            return True
        except Exception as e:
            logger.warning(f"等待网络空闲超时: {e}")
            return False


class ElementCleaner(PageProcessor):
    """
    元素清理处理器
    
    用于删除页面中指定CSS选择器匹配的元素，常用于移除广告、弹窗等不需要的内容。
    """
    
    def __init__(self, name: str, css_selector: str = "*[id*='ad'], *[class*='popup']", priority: int = 20) -> None:
        """
        初始化元素清理处理器
        
        Args:
            name: 处理器名称
            css_selector: CSS选择器，用于定位要删除的元素
            priority: 处理器优先级，固定为20
        """
        super().__init__(name, priority)
        self.css_selector = css_selector
        self._elements_removed = 0
        
        logger.info(f"ElementCleaner初始化: CSS选择器='{css_selector}', 优先级={priority}")
    
    async def detect(self, context: PageContext) -> bool:
        """
        检测是否应该执行元素清理
        
        在页面进入就绪状态时启动
        
        Args:
            context: 页面上下文
            
        Returns:
            bool: 是否应该执行清理
        """
        current_url = context.get_current_url()
        if not current_url:
            return False
        
        # 检查URL状态
        if current_url.status != URLStatus.PROCESSING:
            return False
        
        # 检查页面状态
        page_state = context.data.get("page_state")
        if page_state not in ["ready", "completed"]:
            return False
        
        logger.info(f"ElementCleaner检测到页面就绪，准备清理元素: {current_url.url}")
        return True
    
    async def run(self, context: PageContext) -> None:
        """
        执行元素清理
        
        删除CSS选择器对应的节点，删除完标记成功，否则标记放弃
        
        Args:
            context: 页面上下文
        """
        current_url = context.get_current_url()
        if not current_url:
            self.state = ProcessorState.FAILED
            return
        
        logger.info(f"开始清理元素: {current_url.url}")
        self._elements_removed = 0
        
        try:
            page = context.page
            if not page:
                logger.error("页面对象不存在")
                self.state = ProcessorState.FAILED
                return
            
            # 查找匹配的元素
            elements = await page.query_selector_all(self.css_selector)
            if not elements:
                logger.info(f"未找到匹配CSS选择器 '{self.css_selector}' 的元素")
                self.state = ProcessorState.COMPLETED
                elements_removed_total.labels(
                    css_selector=self.css_selector, 
                    success="true"
                ).inc(0)
                return
            
            logger.info(f"找到 {len(elements)} 个匹配元素，开始删除")
            
            # 删除所有匹配的元素
            removed_count = 0
            for element in elements:
                try:
                    await element.evaluate("element => element.remove()")
                    removed_count += 1
                except Exception as e:
                    logger.warning(f"删除元素失败: {e}")
            
            self._elements_removed = removed_count
            
            if removed_count > 0:
                logger.info(f"成功删除 {removed_count} 个元素")
                self.state = ProcessorState.COMPLETED
                
                # 更新上下文数据
                context.data["elements_removed"] = removed_count
                context.data["css_selector_used"] = self.css_selector
                
                # 更新指标
                elements_removed_total.labels(
                    css_selector=self.css_selector,
                    success="true"
                ).inc(removed_count)
            else:
                logger.warning("没有成功删除任何元素")
                self.state = ProcessorState.FAILED
                elements_removed_total.labels(
                    css_selector=self.css_selector,
                    success="false"
                ).inc()
            
        except Exception as e:
            logger.error(f"元素清理过程中发生错误: {e}")
            self.state = ProcessorState.FAILED
            elements_removed_total.labels(
                css_selector=self.css_selector,
                success="false"
            ).inc()
    
    async def finish(self, context: PageContext) -> None:
        """
        完成元素清理处理
        
        Args:
            context: 页面上下文
        """
        current_url = context.get_current_url()
        url_str = current_url.url if current_url else "unknown"
        
        if self.state == ProcessorState.COMPLETED:
            logger.info(f"元素清理完成: {url_str}, 删除了 {self._elements_removed} 个元素")
        elif self.state == ProcessorState.FAILED:
            logger.warning(f"元素清理失败: {url_str}")
        else:
            logger.info(f"元素清理处理器状态: {self.state.value}")
        
        # 清理临时数据
        self._elements_removed = 0
        
        self.state = ProcessorState.FINISHED


class RequestMonitor(PageProcessor):
        self._context = context

        if not self._monitoring_started:
            # 初始化监控
            self._monitoring_started = True
            self._start_time = time.time()
            self._page_state = "loading"

            # 设置页面状态
            context.data["page_state"] = "loading"
            page_monitor_state_changes.labels(state="loading").inc()
            page_monitor_active_pages.inc()

            # 初始化请求计数器
            context.data["slow_requests"] = defaultdict(int)
            context.data["failed_requests"] = defaultdict(int)

            # 设置页面监听器
            await self._setup_page_listeners(context.page)

            logger.info(f"开始监控页面: {context.url.url}")

        # 检查页面load状态
        try:
            ready_state = await context.page.evaluate("document.readyState")
            if ready_state == "complete" and self._page_state == "loading":
                self._page_state = "ready"
                context.data["page_state"] = "ready"
                page_monitor_state_changes.labels(state="ready").inc()
                logger.info(f"页面进入load状态: {context.url.url}")
        except Exception as e:
            logger.warning(f"检查页面状态失败: {e}")

        # 检查网络空闲状态
        if self._page_state == "ready":
            network_idle = await self._wait_for_network_idle(context.page, timeout=2.0)
            if network_idle:
                self._page_state = "completed"
                context.data["page_state"] = "completed"
                page_monitor_state_changes.labels(state="completed").inc()
                logger.info(f"页面进入networkidle状态: {context.url.url}")

                # 检查是否有更高优先级的处理器在运行
                running_processors = [
                    p
                    for p in context.processors.values()
                    if p.state == ProcessorState.RUNNING and p.priority < self.priority
                ]

                if not running_processors:
                    logger.info(f"页面监控完成，无更高优先级处理器运行: {context.url.url}")
                else:
                    logger.debug(f"等待更高优先级处理器完成: {[p.name for p in running_processors]}")

        # 记录处理时间
        if self._start_time:
            processing_time = time.time() - self._start_time
            page_monitor_processing_time.observe(processing_time)

    async def finish(self, context: PageContext) -> None:
        """清理页面监控"""
        try:
            # 移除页面事件监听器
            if context.page:
                # Playwright 的事件监听器在页面关闭时会自动清理
                await context.page.close()
                logger.info(f"页面已关闭: {context.url.url}")

            # 更新指标
            page_monitor_active_pages.dec()

            # 清理请求时间记录
            self._request_start_times.clear()

            # 记录统计信息
            slow_count = sum(context.data.get("slow_requests", {}).values())
            failed_count = sum(context.data.get("failed_requests", {}).values())

            logger.info(
                f"页面监控完成: {context.url.url}, 慢请求: {slow_count}, 失败请求: {failed_count}",
            )

        except Exception as e:
            logger.error(f"页面监控清理失败: {e}")
        finally:
            self._set_state(ProcessorState.FINISHED)


class RequestMonitor(PageProcessor):
    """请求监控处理器，监控特殊请求并自动屏蔽问题URL"""

    def __init__(
        self,
        name: str,
        url_collection: URLCollection,
        slow_request_threshold: int = 100,
        failed_request_threshold: int = 10,
        priority: int = 1,
    ):
        """
        初始化请求监控处理器

        Args:
            name: 处理器名称
            url_collection: URL集合，用于添加屏蔽的URL
            slow_request_threshold: 慢请求数量阈值，默认100
            failed_request_threshold: 失败请求数量阈值，默认10
            priority: 优先级，固定为1

        """
        super().__init__(name, priority)
        self.url_collection = url_collection
        self.slow_request_threshold = slow_request_threshold
        self.failed_request_threshold = failed_request_threshold
        self._monitoring_started = False
        self._start_time = None

    def _remove_query_string(self, url: str) -> str:
        """移除URL中的查询字符串"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def _get_domain_path(self, url: str) -> tuple[str, str]:
        """获取URL的域名和路径"""
        parsed = urlparse(url)
        domain = parsed.netloc or "unknown"
        path = parsed.path or "/"
        return domain, path

    def _block_problematic_url(self, url: str, reason: str, context: PageContext) -> None:
        """将问题URL添加到集合中并标记为已屏蔽"""
        try:
            # 生成唯一ID
            url_id = f"blocked_{int(time.time())}_{hash(url) % 10000}"
            
            # 创建新的URL对象，状态为BLOCKED
            blocked_url = URL(
                id=url_id,
                url=url,
                category="blocked_by_request_monitor",
                status=URLStatus.BLOCKED,
            )
            
            # 添加到URL集合
            added = self.url_collection.add(blocked_url)
            
            if added:
                domain, path = self._get_domain_path(url)
                logger.warning(f"屏蔽问题URL: {url}, 原因: {reason}")
                
                # 更新 Prometheus 指标
                request_monitor_blocked_urls.labels(
                    reason=reason,
                    domain=domain,
                    path=path,
                ).inc()
                
                # 记录到上下文
                if "blocked_urls" not in context.data:
                    context.data["blocked_urls"] = []
                context.data["blocked_urls"].append({
                    "url": url,
                    "reason": reason,
                    "blocked_at": time.time(),
                })
            else:
                logger.debug(f"URL已存在于集合中: {url}")
                
        except Exception as e:
            logger.error(f"屏蔽URL失败 {url}: {e}")

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测是否开始监控"""
        # 在页面进入就绪状态时启动
        page_state = context.data.get("page_state", "loading")
        
        if not self._monitoring_started and page_state in ("ready", "completed"):
            return ProcessorState.READY
        
        # 如果已经开始监控但未完成，继续运行
        if self._monitoring_started and page_state != "completed":
            return ProcessorState.RUNNING
        
        # 页面进入完成状态，检查是否可以结束
        if page_state == "completed":
            # 检查是否有更高优先级的处理器在运行
            running_processors = [
                p
                for p in context.processors.values()
                if p.state == ProcessorState.RUNNING and p.priority < self.priority
            ]
            
            if not running_processors:
                return ProcessorState.COMPLETED
            else:
                return ProcessorState.RUNNING
        
        return ProcessorState.WAITING

    async def run(self, context: PageContext) -> None:
        """执行请求监控"""
        if not self._monitoring_started:
            # 初始化监控
            self._monitoring_started = True
            self._start_time = time.time()
            
            # 更新指标
            request_monitor_active_monitors.inc()
            
            # 初始化上下文数据
            context.data.setdefault("blocked_urls", [])
            
            logger.info(f"开始监控特殊请求: {context.url.url}")
        
        # 检查慢请求阈值
        slow_requests = context.data.get("slow_requests", {})
        for url, count in slow_requests.items():
            if count >= self.slow_request_threshold:
                self._block_problematic_url(
                    url,
                    f"慢请求次数过多({count}>={self.slow_request_threshold})",
                    context,
                )
        
        # 检查失败请求阈值
        failed_requests = context.data.get("failed_requests", {})
        for url, count in failed_requests.items():
            if count >= self.failed_request_threshold:
                self._block_problematic_url(
                    url,
                    f"失败请求次数过多({count}>={self.failed_request_threshold})",
                    context,
                )
        
        # 记录处理时间
        if self._start_time:
            processing_time = time.time() - self._start_time
            request_monitor_processing_time.observe(processing_time)

    async def finish(self, context: PageContext) -> None:
        """清理请求监控"""
        try:
            # 更新指标
            request_monitor_active_monitors.dec()
            
            # 记录统计信息
            blocked_count = len(context.data.get("blocked_urls", []))
            slow_count = sum(context.data.get("slow_requests", {}).values())
            failed_count = sum(context.data.get("failed_requests", {}).values())
            
            logger.info(
                f"请求监控完成: {context.url.url}, "
                f"屏蔽URL: {blocked_count}, 慢请求: {slow_count}, 失败请求: {failed_count}"
            )
            
        except Exception as e:
            logger.error(f"请求监控清理失败: {e}")
        finally:
            self._set_state(ProcessorState.FINISHED)


class LinksFinder(PageProcessor):
    """链接发现处理器，寻找更多的链接并添加到URL集合中"""

    def __init__(
        self,
        name: str,
        url_collection: URLCollection,
        css_selector: str = "body",
        priority: int = 10,
    ):
        """
        初始化链接发现处理器

        Args:
            name: 处理器名称
            url_collection: URL集合，用于添加发现的链接
            css_selector: CSS选择器，指定要搜索链接的容器
            priority: 优先级，固定为10

        """
        super().__init__(name, priority)
        self.url_collection = url_collection
        self.css_selector = css_selector
        self._ready_executed = False
        self._completed_executed = False
        self._start_time = None

    def _get_domain_path(self, url: str) -> tuple[str, str]:
        """获取URL的域名和路径"""
        parsed = urlparse(url)
        domain = parsed.netloc or "unknown"
        path = parsed.path or "/"
        return domain, path

    def _is_valid_url(self, url: str) -> bool:
        """检查URL是否有效"""
        if not url or not isinstance(url, str):
            return False
        
        url = url.strip()
        if not url:
            return False
        
        # 检查是否是HTTP/HTTPS协议
        if not url.startswith(("http://", "https://")):
            return False
        
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc)
        except Exception:
            return False

    def _generate_url_id(self, url: str) -> str:
        """生成URL的唯一ID"""
        import hashlib
        # 使用URL的hash值和时间戳生成唯一ID
        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
        timestamp = int(time.time() * 1000) % 100000
        return f"links_{timestamp}_{url_hash}"

    async def _extract_links_from_container(self, page, container_selector: str, source_domain: str) -> list[str]:
        """从指定容器中提取所有有效链接"""
        try:
            # 使用JavaScript在页面中提取链接
            links = await page.evaluate(f"""
                () => {{
                    const container = document.querySelector('{container_selector}');
                    if (!container) return [];
                    
                    const links = [];
                    
                    // 如果容器本身是a标签，添加它
                    if (container.tagName === 'A' && container.href) {{
                        links.push(container.href);
                    }}
                    
                    // 查找容器下的所有a标签
                    const aElements = container.querySelectorAll('a[href]');
                    for (const a of aElements) {{
                        if (a.href) {{
                            links.push(a.href);
                        }}
                    }}
                    
                    return links;
                }}
            """)
            
            # 过滤和验证链接
            valid_links = []
            for link in links:
                if self._is_valid_url(link):
                    valid_links.append(link)
                    
                    # 更新指标
                    link_domain, _ = self._get_domain_path(link)
                    links_finder_valid_links.labels(
                        domain=link_domain,
                        source_domain=source_domain,
                    ).inc()
            
            # 更新发现链接总数指标
            total_discovered = len(links)
            if total_discovered > 0:
                links_finder_discovered_links.labels(
                    domain="all",
                    source_domain=source_domain,
                ).inc(total_discovered)
            
            logger.info(f"从 {container_selector} 提取到 {len(valid_links)} 个有效链接（总共 {total_discovered} 个）")
            return valid_links
            
        except Exception as e:
            logger.error(f"提取链接失败: {e}")
            return []

    async def _add_links_to_collection(self, links: list[str], context: PageContext) -> int:
        """将链接添加到URL集合中"""
        added_count = 0
        
        for link in links:
            try:
                # 生成唯一ID
                url_id = self._generate_url_id(link)
                
                # 创建URL对象
                url_obj = URL(
                    id=url_id,
                    url=link,
                    category="discovered_by_links_finder",
                    status=URLStatus.PENDING,  # 标记为待访问
                )
                
                # 添加到URL集合
                added = self.url_collection.add(url_obj)
                if added:
                    added_count += 1
                    logger.debug(f"添加新链接: {link}")
                else:
                    logger.debug(f"链接已存在: {link}")
                    
            except Exception as e:
                logger.error(f"添加链接失败 {link}: {e}")
        
        # 记录到上下文
        if "discovered_links" not in context.data:
            context.data["discovered_links"] = []
        
        context.data["discovered_links"].extend([
            {
                "url": link,
                "discovered_at": time.time(),
                "selector": self.css_selector,
            } for link in links
        ])
        
        logger.info(f"成功添加 {added_count} 个新链接到URL集合")
        return added_count

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测是否开始链接发现"""
        # 在页面进入就绪状态时启动
        page_state = context.data.get("page_state", "loading")
        
        if page_state in ("ready", "completed"):
            return ProcessorState.READY
        
        return ProcessorState.WAITING

    async def run(self, context: PageContext) -> None:
        """执行链接发现"""
        if not self._start_time:
            self._start_time = time.time()
            links_finder_active_finders.inc()
            logger.info(f"开始链接发现: {context.url.url}")
        
        page_state = context.data.get("page_state", "loading")
        source_domain, _ = self._get_domain_path(context.url.url)
        
        # 在页面就绪状态执行一次
        if page_state == "ready" and not self._ready_executed:
            logger.info(f"页面就绪状态 - 执行链接发现: {context.url.url}")
            
            links = await self._extract_links_from_container(
                context.page, 
                self.css_selector, 
                source_domain
            )
            
            if links:
                await self._add_links_to_collection(links, context)
            
            self._ready_executed = True
        
        # 在页面完成状态执行一次
        if page_state == "completed" and not self._completed_executed:
            logger.info(f"页面完成状态 - 执行链接发现: {context.url.url}")
            
            links = await self._extract_links_from_container(
                context.page, 
                self.css_selector, 
                source_domain
            )
            
            if links:
                await self._add_links_to_collection(links, context)
            
            self._completed_executed = True
            
            # 标记执行完成
            logger.info(f"链接发现完成: {context.url.url}")
        
        # 记录处理时间
        if self._start_time:
            processing_time = time.time() - self._start_time
            links_finder_processing_time.observe(processing_time)

    async def finish(self, context: PageContext) -> None:
        """清理链接发现处理器"""
        try:
            # 更新指标
            links_finder_active_finders.dec()
            
            # 记录统计信息
            discovered_count = len(context.data.get("discovered_links", []))
            
            logger.info(f"链接发现完成: {context.url.url}, 发现链接: {discovered_count}")
            
        except Exception as e:
            logger.error(f"链接发现清理失败: {e}")
        finally:
            self._set_state(ProcessorState.FINISHED)


class PageLoadProcessor(PageProcessor):
    """页面加载处理器，确保页面完全加载"""

    def __init__(self, name: str, priority: int = 10):
        """
        初始化页面加载处理器

        Args:
            name: 处理器名称
            priority: 优先级，默认为10（最高优先级）

        """
        super().__init__(name, priority)
        self._load_completed = False

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测页面是否已加载完成"""
        if self._load_completed:
            return ProcessorState.COMPLETED

        try:
            # 检查页面是否已加载
            ready_state = await context.page.evaluate("document.readyState")
            if ready_state == "complete":
                return ProcessorState.READY
            return ProcessorState.WAITING

        except Exception as e:
            logger.error(f"页面加载检测失败: {e}")
            return ProcessorState.CANCELLED

    async def run(self, context: PageContext) -> None:
        """执行页面加载处理"""
        logger.info(f"页面加载完成: {context.url.url}")
        self._load_completed = True
        # 注意：不要在这里设置状态，Manager会负责状态管理

        # 保存页面基本信息到上下文
        context.data["title"] = await context.page.title()
        context.data["page_url"] = context.page.url
        context.data["load_time"] = await context.page.evaluate("performance.now()")

    async def finish(self, context: PageContext) -> None:
        """清理页面加载处理器"""
        self._set_state(ProcessorState.FINISHED)
        logger.debug(f"页面加载处理器清理完成: {context.url.url}")


class ContentExtractProcessor(PageProcessor):
    """内容提取处理器，提取页面主要内容"""

    def __init__(self, name: str, content_selector: str = "body", priority: int = 20):
        """
        初始化内容提取处理器

        Args:
            name: 处理器名称
            content_selector: 内容选择器
            priority: 优先级，默认为20（依赖页面加载）

        """
        super().__init__(name, priority)
        self.content_selector = content_selector
        self._content_extracted = False

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测是否可以提取内容"""
        if self._content_extracted:
            return ProcessorState.COMPLETED

        # 依赖页面加载完成（检查title存在）
        if "title" not in context.data:
            return ProcessorState.WAITING

        try:
            # 检查内容元素是否存在
            element = await context.page.query_selector(self.content_selector)
            if element:
                return ProcessorState.READY
            return ProcessorState.WAITING

        except Exception as e:
            logger.error(f"内容检测失败: {e}")
            return ProcessorState.CANCELLED

    async def run(self, context: PageContext) -> None:
        """执行内容提取"""
        try:
            # 提取文本内容
            text_content = await context.page.evaluate(f"""
                () => {{
                    const element = document.querySelector('{self.content_selector}');
                    return element ? element.innerText : '';
                }}
            """)

            # 提取HTML内容
            html_content = await context.page.evaluate(f"""
                () => {{
                    const element = document.querySelector('{self.content_selector}');
                    return element ? element.innerHTML : '';
                }}
            """)

            # 保存到上下文
            context.data["content"] = text_content
            context.data["html_content"] = html_content
            context.data["content_length"] = len(text_content)

            self._content_extracted = True
            # 注意：不要在这里设置状态，Manager会负责状态管理
            logger.info(f"内容提取完成: {context.url.url}, 长度: {len(text_content)}")

        except Exception as e:
            logger.error(f"内容提取失败 {context.url.url}: {e}")
            raise

    async def finish(self, context: PageContext) -> None:
        """清理内容提取处理器"""
        self._set_state(ProcessorState.FINISHED)
        logger.debug(f"内容提取处理器清理完成: {context.url.url}")


class PDFGenerateProcessor(PageProcessor):
    """PDF生成处理器，将页面转换为PDF"""

    def __init__(self, name: str, output_dir: str = "/tmp", priority: int = 40):
        """
        初始化PDF生成处理器

        Args:
            name: 处理器名称
            output_dir: 输出目录
            priority: 优先级，默认为40（依赖内容提取）

        """
        super().__init__(name, priority)
        self.output_dir = output_dir
        self._pdf_generated = False

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测是否可以生成PDF"""
        if self._pdf_generated:
            return ProcessorState.COMPLETED

        # 依赖内容提取完成
        if "content" not in context.data:
            return ProcessorState.WAITING

        # 检查是否有内容
        if context.data.get("content_length", 0) > 0:
            return ProcessorState.READY
        logger.warning(f"页面无内容，跳过PDF生成: {context.url.url}")
        return ProcessorState.CANCELLED

    async def run(self, context: PageContext) -> None:
        """执行PDF生成"""
        try:
            # 生成文件名
            safe_url = context.url.url.replace("://", "_").replace("/", "_").replace("?", "_")
            pdf_path = f"{self.output_dir}/{safe_url}_{context.url.id}.pdf"

            # 生成PDF
            await context.page.pdf(
                path=pdf_path,
                format="A4",
                print_background=True,
                margin={
                    "top": "1cm",
                    "right": "1cm",
                    "bottom": "1cm",
                    "left": "1cm",
                },
            )

            # 保存PDF信息到上下文
            context.data["pdf_path"] = pdf_path
            context.data["pdf_generated"] = True

            self._pdf_generated = True
            # 注意：不要在这里设置状态，Manager会负责状态管理
            logger.info(f"PDF生成完成: {context.url.url} -> {pdf_path}")

        except Exception as e:
            logger.error(f"PDF生成失败 {context.url.url}: {e}")
            raise

    async def finish(self, context: PageContext) -> None:
        """清理PDF生成处理器"""
        self._set_state(ProcessorState.FINISHED)
        logger.debug(f"PDF生成处理器清理完成: {context.url.url}")


class LinkExtractProcessor(PageProcessor):
    """链接提取处理器，提取页面中的链接"""

    def __init__(self, name: str, link_selector: str = "a[href]", priority: int = 30):
        """
        初始化链接提取处理器

        Args:
            name: 处理器名称
            link_selector: 链接选择器
            priority: 优先级，默认为30（依赖页面加载）

        """
        super().__init__(name, priority)
        self.link_selector = link_selector
        self._links_extracted = False

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测是否可以提取链接"""
        if self._links_extracted:
            return ProcessorState.COMPLETED

        # 依赖页面加载处理器
        page_loader = context.get_processor("page_loader")
        if not page_loader or page_loader.state != ProcessorState.COMPLETED:
            return ProcessorState.WAITING

        return ProcessorState.READY

    async def run(self, context: PageContext) -> None:
        """执行链接提取"""
        try:
            # 提取所有链接
            links = await context.page.evaluate(f"""
                () => {{
                    const links = Array.from(document.querySelectorAll('{self.link_selector}'));
                    return links.map(link => ({{
                        href: link.href,
                        text: link.innerText.trim(),
                        title: link.title || ''
                    }}));
                }}
            """)

            # 过滤和处理链接
            valid_links = []
            for link in links:
                href = link.get("href", "").strip()
                if href and href.startswith(("http://", "https://")):
                    valid_links.append(
                        {
                            "url": href,
                            "text": link.get("text", "")[:100],  # 限制文本长度
                            "title": link.get("title", "")[:100],
                        }
                    )

            # 保存到上下文
            context.data["extracted_links"] = valid_links
            context.data["links_count"] = len(valid_links)

            self._links_extracted = True
            logger.info(f"链接提取完成: {context.url.url}, 发现 {len(valid_links)} 个链接")

        except Exception as e:
            logger.error(f"链接提取失败 {context.url.url}: {e}")
            raise

    async def finish(self, context: PageContext) -> None:
        """清理链接提取处理器"""
        logger.debug(f"链接提取处理器清理完成: {context.url.url}")


class ScreenshotProcessor(PageProcessor):
    """截图处理器，为页面生成截图"""

    def __init__(self, name: str, output_dir: str = "/tmp", priority: int = 50):
        """
        初始化截图处理器

        Args:
            name: 处理器名称
            output_dir: 输出目录
            priority: 优先级，默认为50（依赖页面加载）

        """
        super().__init__(name, priority)
        self.output_dir = output_dir
        self._screenshot_taken = False

    async def detect(self, context: PageContext) -> ProcessorState:
        """检测是否可以截图"""
        if self._screenshot_taken:
            return ProcessorState.COMPLETED

        # 依赖页面加载处理器
        page_loader = context.get_processor("page_loader")
        if not page_loader or page_loader.state != ProcessorState.COMPLETED:
            return ProcessorState.WAITING

        return ProcessorState.READY

    async def run(self, context: PageContext) -> None:
        """执行截图"""
        try:
            # 生成文件名
            safe_url = context.url.url.replace("://", "_").replace("/", "_").replace("?", "_")
            screenshot_path = f"{self.output_dir}/{safe_url}_{context.url.id}.png"

            # 截图
            await context.page.screenshot(
                path=screenshot_path,
                full_page=True,
                type="png",
            )

            # 保存截图信息到上下文
            context.data["screenshot_path"] = screenshot_path
            context.data["screenshot_taken"] = True

            self._screenshot_taken = True
            logger.info(f"截图完成: {context.url.url} -> {screenshot_path}")

        except Exception as e:
            logger.error(f"截图失败 {context.url.url}: {e}")
            raise

    async def finish(self, context: PageContext) -> None:
        """清理截图处理器"""
        logger.debug(f"截图处理器清理完成: {context.url.url}")
